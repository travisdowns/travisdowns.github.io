<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>What Has Your Microcode Done for You Lately? | Performance Matters</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="What Has Your Microcode Done for You Lately?" />
<meta name="author" content="Travis Downs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CPU microcode updates can cause silent and dramatic performance changes." />
<meta property="og:description" content="CPU microcode updates can cause silent and dramatic performance changes." />
<link rel="canonical" href="https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html" />
<meta property="og:url" content="https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html" />
<meta property="og:site_name" content="Performance Matters" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-19T15:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="What Has Your Microcode Done for You Lately?" />
<meta name="twitter:site" content="@trav_downs" />
<meta name="twitter:creator" content="@trav_downs" />
<script type="application/ld+json">
{"headline":"What Has Your Microcode Done for You Lately?","dateModified":"2019-03-19T15:00:00+00:00","description":"CPU microcode updates can cause silent and dramatic performance changes.","datePublished":"2019-03-19T15:00:00+00:00","url":"https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html"},"author":{"@type":"Person","name":"Travis Downs"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://travisdowns.github.io/feed.xml" title="Performance Matters" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-136594956-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Performance Matters</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!-- BASE:  -->

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

<!-- INSIDE -->

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">What Has Your Microcode Done for You Lately?</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-03-19T15:00:00+00:00" itemprop="datePublished">Mar 19, 2019
      </time><!-- travis override --><span>
          •
          <span class="tag-link"><a href="/tags/Intel.html">Intel</a></span><span class="tag-link"><a href="/tags/memory.html">memory</a></span><span class="tag-link"><a href="/tags/performance.html">performance</a></span>
        </span><!-- end override -->
    </p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="microcode-mystery">Microcode Mystery</h2>

<p>Did you ever wonder what is <em>inside</em> those <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> updates that get silently applied to your CPU via Windows update, BIOS upgrades, and various <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> packages on Linux?</p>

<p>Well, you are in the wrong place, because this blog post won’t answer that question (you might like <a href="https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-koppe.pdf">this</a> though).</p>

<p>In fact, the overwhelming majority of this this post is about the performance of scattered writes, and not very much at all about the details of CPU <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>. Where the <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> comes in, and what might make this more interesting than usual, is that performance on a purely CPU-bound benchmark can vary dramatically <em>depending on <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version</em>. In particular, we will show that the most recent Intel <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version can significantly slow down a store heavy workload when some stores hit in the L1 data cache, and some miss.</p>

<p>My results are intended to be reproducible and the benchmarking and data collection code is available as described <a href="#the-source">at the bottom</a>.</p>

<h2 id="a-series-of-random-writes">A series of random writes</h2>

<p>How fast can you perform a series of random writes? Because of the importance of caching, you might reasonably expect that it depends heavily on how big of a region the writes are scattered across, and you’d be right. For example, if we test a series of random writes to a region that fits entirely in L1, we find that random writes take almost exactly 1 cycle on modern Intel chips, matching the published limit of one write per cycle <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup>.</p>

<p>If we use larger regions, we expect performance to slow down as many of the writes miss to outer cache levels. In fact, I measure roughly the following performance whether for linear (64 byte stride) or random writes to various sized regions:</p>

<table>
  <thead>
    <tr>
      <th>Region Size</th>
      <th>Cycles/Write</th>
      <th>Typical Read Latency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>L1</td>
      <td>1</td>
      <td>5</td>
    </tr>
    <tr>
      <td>L2</td>
      <td>3</td>
      <td>12</td>
    </tr>
    <tr>
      <td>L3</td>
      <td>5-6</td>
      <td>~35</td>
    </tr>
    <tr>
      <td>RAM</td>
      <td>15-20</td>
      <td>~200</td>
    </tr>
  </tbody>
</table>

<p>I’ve also included a third column in the table above which records typical read latency figures for each cache level. This gives an indication of roughly how <em>far away</em> a cache is from the core, based on the round-trip read time. Since all normal stores<sup id="fnref:normal-stores" role="doc-noteref"><a href="#fn:normal-stores" class="footnote" rel="footnote">2</a></sup> also involve a read (to get the cache line to write to into the L1 cache with its existing contents), the time to “complete” a single store should be at least that long<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup>. As the observed time per write is much less, these tests must exhibit significant <a href="https://en.wikipedia.org/wiki/Memory-level_parallelism">memory level parallelism</a> (<abbr title="Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.">MLP</abbr>), i.e., several store misses are in-progress in the memory subsystem at once and their latencies overlap.  We usually care about <abbr title="Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.">MLP</abbr> when it comes to loads, but it is important also for a long stream of stores such as these benchmarks. The last line in above table implies that we may have requests for 10 or more stores in flight in the memory subsystem at once, in order to achieve average store time of 15-20 cycles with a memory latency of 200 cycles.</p>

<p>You can reproduce this table yourself using the <code class="language-plaintext highlighter-rouge">wrandom1-unroll</code> and <code class="language-plaintext highlighter-rouge">wlinear1</code> tests.</p>

<h2 id="interleaved-writes">Interleaved writes</h2>

<p>Let’s move on to the case where we actually observe some interesting behavior. Here we tackle the same scenario that I asked about in a <a href="https://twitter.com/trav_downs/status/1103396480994422784">twitter poll</a>.</p>

<p>Consider the following loop, which writes randomly to <em>two</em> character arrays.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">writes_inter</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">iters</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">a1</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">size1</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">a2</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">size2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">rng_state</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">RAND_INIT</span><span class="p">;</span>
    <span class="k">do</span> <span class="p">{</span>
        <span class="kt">uint32_t</span> <span class="n">val</span> <span class="o">=</span> <span class="n">RAND_FUNC</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rng</span><span class="p">);</span>
        <span class="n">a1</span><span class="p">[(</span><span class="n">val</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">size1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">a2</span><span class="p">[(</span><span class="n">val</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">size2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="o">--</span><span class="n">iters</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Let’s say we fix the size of the first array, <code class="language-plaintext highlighter-rouge">size1</code>, to something like half the size of the L2 cache, and evaluate the performance for a range of sizes for the second array, <code class="language-plaintext highlighter-rouge">size2</code>. What type of performance do we expect? We already know the time it takes for a single write to regions of various size, so in principle one might expect the above loop to perform something like the sum of the time of one write to an L2-sized region (the write to <code class="language-plaintext highlighter-rouge">a1</code>) and one write to a <code class="language-plaintext highlighter-rouge">size2</code> sized region (the write to <code class="language-plaintext highlighter-rouge">a2</code>).</p>

<p>Let’s try it! Here’s a test of single stores vs interleaved stores (with one of the interleaved stores accessing a fixed 128 KiB region), varying the size of the other region, run on my Skylake i7-6700HQ.</p>

<p><img src="/assets/2019-03-19/skl/i-vs-s-old.svg" alt="Interleaved vs Single stores" /></p>

<p>Overall we see that behavior of the two benchmarks roughly track each other, with the interleaved version (twice as many stores) taking longer than the single store version, as expected.</p>

<p>Especially for large region sizes (the right side of the graph), the assumption that interleaved accesses are more or less additive with the same accesses by themselves mostly pans out: there is a gap of about 4 cycles between the single stream and the stream with interleaved accesses, which is just slightly more than the cost of an L2 access. For small region sizes, the correspondence is less exact. In particular, the single stream drops down to ~1 cycle accesses when the region fits in L1, but in the interleaved case this doesn’t occur.</p>

<p>At least part of this behavior makes sense: the two streams of stores will interact in the caches, and the L1 contained region isn’t really “L1 contained” in the interleaved case because the second stream of stores will be evicting lines from L1 constantly. So with a 16 KiB second region, the test really behaves as if a 16 + 128 = 144 KiB region was being accessed, i.e., L2 resident, but in a biased way (with the 16 KiB block being accessed much more frequently), so there is no sharp decrease in iteration time at the 32 KiB boundary<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>.</p>

<h2 id="the-weirdness-begins">The weirdness begins</h2>

<p>So far, so good and nothing too weird. However, starting now, it <em>is</em> about to get weird!</p>

<p>Everything above is a reduced version of a benchmark I was using to test some <em>real code</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">5</a></sup>, about a year ago. This code had a tight loop with a table lookup and then writes to two different arrays. When I benchmarked this code, performance was usually consistent with the performance of “interleaved” benchmark plotted above.</p>

<p>Recently, I returned to the benchmark to check the performance on newer CPU architectures. First, I went back to check the results on the original hardware (the <a href="https://ark.intel.com/content/www/us/en/ark/products/88967/intel-core-i7-6700hq-processor-6m-cache-up-to-3-50-ghz.html">Skylake i7-6700HQ</a> in my laptop). I failed to reproduce it – I wasn’t able to achieve the same performance, with the same test and on the same hardware as before: it was always running significantly slower (about half the original speed).</p>

<p>With some help from user Adrian on the <a href="https://www.realworldtech.com/forum/?roomid=1">RWT forums</a> I was able to bisect the difference down to a CPU <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> update. In particular, with newest <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">6</a></sup>, <code class="language-plaintext highlighter-rouge">0xc6</code> the interleaved stores scenario runs <em>much</em> slower. For example, the same benchmark as above now looks like this, every time you run it:</p>

<p><img src="/assets/2019-03-19/skl/i-vs-s-new.svg" alt="Interleaved vs Single Stores (New Microcode)" /></p>

<p>The behavior of interleaved for small regions (left hand side of chart) is drastically different - the throughput is less than half of the old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>. It is not obvious just by visual comparison it, but performance is actually reduced across the range of tested sizes for the interleaved case, albeit by only a few cycles as the region size becomes large. I tested various <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> versions and found that only the most recent <abbr title="Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7">SKL</abbr> <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>, revision <code class="language-plaintext highlighter-rouge">0xc6</code> and released in August 2018 exhibits the “always slow” behavior shown above. The preceding version <code class="language-plaintext highlighter-rouge">0xc2</code> usually results in the fast behavior.</p>

<p>What’s up with that?</p>

<h3 id="performance-counters">Performance Counters</h3>

<p>We can check the performance counters to see if they reveal anything. We’ll use the <code class="language-plaintext highlighter-rouge">l2_rqsts.references</code>, <code class="language-plaintext highlighter-rouge">l2_rqsts.all_rfo</code> and <code class="language-plaintext highlighter-rouge">l2_rqsts.rfo_miss</code> counters, which count the total number of accesses (<code class="language-plaintext highlighter-rouge">references</code>) and total accesses related to <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> requests (<code class="language-plaintext highlighter-rouge">all_rfo</code> aka <em>stores</em>) from the core as well as the number that miss (<code class="language-plaintext highlighter-rouge">rfo_miss</code>). Since we are only performing stores, we expect these counts to match and to correspond to the number of L1 store misses, since any store that misses in L1 ultimately contributes<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">7</a></sup> to an L2 access.</p>

<p>Here’s the old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>:</p>

<p><img src="/assets/2019-03-19/skl/i-plus-counters-old.svg" alt="Interleaved Stores w/ Perf Counters (old microcode)" /></p>

<p>… and with the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> (note the change in the y axis, it’s about 3x slower for the L1 hit region):</p>

<p><img src="/assets/2019-03-19/skl/i-plus-counters-new.svg" alt="Interleaved Stores w/ Perf Counters (new microcode)" /></p>

<p>Despite the large difference in performance, there is very little to no difference in the relevant performance counters. In both cases, the number of L1 misses (i.e., L2 references) approaches 0.75 as the second region size approaches zero as we’d expect (all L1 hits in the second region, and about 25% L1 hits in the 128 KiB fixed region as the L1D is 25% of the size of L2). On the right side, the number of L1 misses approaches something like 1.875, as the L1 hits in the 128 KiB region are cut in half by competition with with the other large region.</p>

<p>So despite the much slower performance, for L1-sized second regions, the difference doesn’t obviously originate in different cache hit behavior. Indeed, with the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>, performance goes <em>down</em> as the L1 hit rate goes <em>up</em>.</p>

<p>So it seems that the likeliest explanation is that <em>the presence of an L1 hit in the store buffer prevents overlapping of miss handling for stores on either side</em>, at least with the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>, on <abbr title="Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7">SKL</abbr> hardware. That is, a series of consecutive stores can be handled in parallel only if none of them is an L1 hit. In this way L1 store hits somehow act as a store fence with the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>. The performance is in line with each store going alone to the memory hierarchy: roughly the L2 latency plus a few cycles.</p>

<h3 id="will-the-real-sfence-please-stand-up">Will the real sfence please stand up</h3>

<p>Let’s test the “L1 hits act as a store fence” theory. In fact, there is already an instruction that acts as a store force in the x86 ISA: <a href="https://www.felixcloutier.com/x86/sfence"><code class="language-plaintext highlighter-rouge">sfence</code></a>. Repeatedly executed back-to-back this instruction only takes a <a href="https://uops.info/html-instr/SFENCE.html">few cycles</a> but its most interesting effect occurs when stores are in the pipeline: this instruction blocks dispatch of subsequent stores until all earlier stores have committed to the L1 cache, implying that stores on different sides of the fence cannot overlap<sup id="fnref:sfence-note" role="doc-noteref"><a href="#fn:sfence-note" class="footnote" rel="footnote">8</a></sup>.</p>

<p>We will look at two version of the interleaved loop with <code class="language-plaintext highlighter-rouge">sfence</code>: one with <code class="language-plaintext highlighter-rouge">sfence</code> inserted right after the store to the first region (fixed 128 KiB), and the other inserted after the store to the second region - let’s call them sfenceA and sfenceB respectively. Both have the same number of fences (one per iteration, i.e., per pair of stores) and only differ in what store happens to be last in the store buffer when the <code class="language-plaintext highlighter-rouge">sfence</code> executes. Here’s the result on the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> (the results on the old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> are <a href="/assets/2019-03-19/skl/i-sfence-old.svg">over here</a>):</p>

<p><img src="/assets/2019-03-19/skl/i-sfence-new.svg" alt="Interleaved Stores w/ SFENCE" /></p>

<p>The right side of the graph is fairly unremarkable: both versions with sfence perform roughly at the latency for the associated cache level because there is zero memory level parallelism (no, I don’t know why one performs better than other or why the performance crosses over near 64 KiB). The left part is pretty amazing though: one of the sfence configurations is <em>faster than the same code without sfence</em>. That’s right, adding a store serializing instruction like sfence, can speed up the code by several cycles. It doesn’t come close to the fast performance of the old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> versions, but the behavior is very surprising nonetheless.</p>

<p>The version that was faster, sfenceA, had the <code class="language-plaintext highlighter-rouge">sfence</code> between the 128 KiB store and the L1 store. So perhaps there is some kind of penalty when an L1 hit store arrives right after a L1-miss-L2-hit store, in addition to the “no <abbr title="Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.">MLP</abbr>” penalty we normally see.</p>

<h2 id="larger-fixed-regions">Larger fixed regions</h2>

<p>To this point we’ve been we’ve been looking at the scenario where a write to a 128 KiB region is interleaved with a write to a region of varying size. The fixed size of 128 KiB means that most<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">9</a></sup> of those writes will be L2 hits. What if we make the fixed size region larger? Let’s say 2 MiB, which is much larger than L2 (256 KiB) but still fits easily in L3 (6 MiB on my CPU). Now we expect most writes to the fixed region to be L2 misses but L3 hits.</p>

<p>What’s the behavior? Here’s the old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>:</p>

<p><img src="/assets/2019-03-19/skl/i-vs-s-2mib-old.svg" alt="Interleaved Stores w/ 2048 KiB Fixed Region" /></p>

<p>… and the new:</p>

<p><img src="/assets/2019-03-19/skl/i-vs-s-2mib-new.svg" alt="Interleaved Stores w/ 2048 KiB Fixed Region" /></p>

<p>Again we see a large performance impact with the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>, and the results are consistent with the theory that L1 hits in the store stream prevent overlapping of store misses on either side. In particular we see that the region with L1 hits takes about 37 cycles, almost exactly the L3 latency on this CPU. In this scenario, it is <em>slower to have L1 hits mixed in to the stream of accesses than to replace those L1 hits with misses to DRAM</em>. That’s a remarkable demonstration of the power of memory level parallelism and of the potential impact of this change.</p>

<h2 id="why">Why?</h2>

<p>I can’t tell you for certain why the store related machinery acts the way it does in this case. Speculating is fun though, so lets do that. Here are a couple possibilities for why the memory model acts the way it does.</p>

<h3 id="the-x86-memory-model">The x86 Memory Model</h3>

<p>First, let’s quickly review the x86 memory model.</p>

<p>The x86 has a relatively strong memory model. Intel doesn’t give it a handy name, but lets call it <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf">x86-TSO</a>. In x86-TSO, stores from all CPUs appear in a global total order with stores from each CPU consistent with program order. If a given CPU makes stores A and B in that order, all other CPUs will observe not only a consistent order of stores A and B, but the <em>same</em> A-before-B order as the program order. All this store ordering complicates the pipeline. In weaker memory models like ARM and POWER, in the absence of fences, you can simply commit senior stores<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">10</a></sup> in whatever order is convenient. If some store locations are already present in L1, you can commit those, while making <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> requests for other store locations which aren’t in L1.</p>

<p>An x86 CPU has a to take more conservative strategy. The basic idea is that stores are only made globally observable <em>in program order</em> as they reach the head of the store buffer. The CPU may still try to get parallelism by prefetching upcoming stores, as described for example in Intel’s <a href="https://patents.google.com/patent/US7130965/en">US patent 7130965</a><sup id="fnref:patent-note" role="doc-noteref"><a href="#fn:patent-note" class="footnote" rel="footnote">11</a></sup> - but care must be taken. For example, any snoop request that comes in for any of the lines in flight must get a consistent result: whether the lines are in a write-back buffer being evicted from L1, in a fill buffer making their way to L2, in a write-combining buffer<sup id="fnref:wc-note" role="doc-noteref"><a href="#fn:wc-note" class="footnote" rel="footnote">12</a></sup> waiting to commit to L1, and so on.</p>

<h3 id="write-combining-buffers">Write Combining Buffers</h3>

<p>With that out of the way, let’s talk about how the store pipeline might actually work.</p>

<p>Let’s assume that when a store misses in the L1 it allocates a <em>fill buffer</em> to fetch the associated line from the outer levels of the memory hierarchy (we can be pretty sure this is true). Lets further assume that if another stores in the store buffer reaches the head of the store buffer and is to the same line, we get effectively a “fill buffer hit”, and that in this case the store is <em>merged into the existing fill buffer and removed from the store buffer</em>*. That is, the fill buffer entry itself keeps track of the written bytes, and merges those bytes with any unwritten ones when the line returns from the memory hierarchy, before finally committing it to L1<sup id="fnref:wc-stores" role="doc-noteref"><a href="#fn:wc-stores" class="footnote" rel="footnote">13</a></sup>.</p>

<p>In the scenario where there are outstanding fill buffers containing store stater, committing stores that hit in L1 is tricky: if you have several outstanding fill buffers for outstanding stores, as well as several interleaved L1-hit stores, the strong memory model used by x86 and described above means that you have to ensure that any incoming snoop requests see all those stores in the same order. You can’t just snoop all the fill buffers and then the L1 or vice-versa since that might change the apparent order. Additionally, stores become globally visible if they are committed to the L1, but the global observability point for stores whose data is being collected in the fill buffers is less clear.</p>

<p>One simple approach for dealing with L1-hits stores when there are outstanding stores in the fill buffers is to delay the store until the outstanding stores complete and are committed to L1. This could prevent any parallelism between stores with an intervening L1 hit, unless <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> prefetching kicks in. So perhaps the difference is whether the <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> prefetch heuristic determines it is profitable to prefetch stores. Or perhaps the CPU is able to choose between two strategies in this scenario, one of which allows parallelism and one which doesn’t. For example, perhaps the L1 stores could themselves be buffered in fill buffers, which seems silly except that it may allow preserving the order among stores which both hit and miss in L1. For whatever reason the CPU choose the no-parallelism strategy more in the case of the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>.</p>

<p>Perhaps the overlapping behavior was completely disabled to support some recent type of Spectre mitigation (see for example <a href="https://en.wikipedia.org/wiki/Speculative_Store_Bypass">SSB disable</a> functionality which was probably added in this newest <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version).</p>

<p>Without more details on the mechanisms on modern Intel CPUs it is hard to say more, but there are certainly cases where extreme care has to be taken to preserve the order of writes. The <em>fill buffers</em> used for L1 misses, as well as associated components in the outer cache layers already need to be ordered to support the memory model (which also disallows load-load reordering), so in that sense all the stores that miss L1 are already in good hands. Stores that want to commit directly to L1 are more problematic since they are no longer tracked and have become globally observable (a snoop may arrive at any moment and see the newly written) value. I did take a good long look at the patents, but didn’t find any smoking gun to explain the current behavior.</p>

<h2 id="workarounds">Workarounds</h2>

<p>Now that we’re aware of the problem, is there anything we can do in the case we are bitten by it? Yes.</p>

<h3 id="avoid-or-reduce-fine-grained-interleaving">Avoid or reduce fine-grained interleaving</h3>

<p>The problem occurs when you have <em>fine-grained</em> interleaving between L1 hits and L1 misses. Sometimes you can avoid the interleaving entirely, but if not you can perhaps make it coarser grained. For example, the current interleaved test alternates between L1 misses and L1 misses, like <code class="language-plaintext highlighter-rouge">L1-hit, L1-miss, L1-hit, L1-miss</code>. If you unroll by a factor of two and then move the writes to the same region to be adjacent in the source (which doesn’t change the semantics since the regions are not overlapping), you’ll coarser grained interleaving, like: <code class="language-plaintext highlighter-rouge">L1-hit, L1-hit, L1-miss, L1-miss</code>. Based on our theory of reduced memory level parallelism, grouping the stores in this way will allow at least <em>some</em> overlapping (in this example, two stores can be overlapped).</p>

<p>Let’s try this, comparing unrolling by a factor of two and four versus the plain unrolled version. The main loop in the factor of two unrolled version (the factor of 4 is equivalent) looks like:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">do</span> <span class="p">{</span>
    <span class="kt">uint32_t</span> <span class="n">val1</span> <span class="o">=</span> <span class="n">RAND_FUNC</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rng</span><span class="p">);</span>
    <span class="kt">uint32_t</span> <span class="n">val2</span> <span class="o">=</span> <span class="n">RAND_FUNC</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rng</span><span class="p">);</span>
    <span class="n">a1</span><span class="p">[(</span><span class="n">val1</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">size1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">a1</span><span class="p">[(</span><span class="n">val2</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">size1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">a2</span><span class="p">[(</span><span class="n">val1</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">size2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="n">a2</span><span class="p">[(</span><span class="n">val2</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">size2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="o">--</span><span class="n">iters</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">);</span>
</code></pre></div></div>

<p>Here’s is the performance with a fixed array size of 2048 KiB (since the performance degradation is more dramatic with large fixed region sizes):</p>

<p><img src="/assets/2019-03-19/skl/i-unrolled-2mib-new.svg" alt="Interleaved Stores with Unrolling" /></p>

<p>For the region where L1 hits occur, the unroll by gives a 1.6x speedup, and the unroll by 4 a 2.5x speed. Even when unrolling by 4 we still see an impact from this issue (performance still improves once almost every store is an L1 miss) - but we are much closer to the expected the baseline performance before the <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> update.</p>

<p>This change doesn’t come for free: unrolling the loop by hand has a cost in development complexity as the unrolled loop is more complicated. Indeed, the implementation in the benchmark doesn’t handle values of <code class="language-plaintext highlighter-rouge">iters</code> which aren’t a multiple or 2 or 4. It also has a cost in code size as the unrolled functions are larger:</p>

<table>
  <thead>
    <tr>
      <th>Function</th>
      <th>Loop Size in Bytes</th>
      <th>Function Size in Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Original</td>
      <td>40</td>
      <td>74</td>
    </tr>
    <tr>
      <td>Unrolled 2x</td>
      <td>72</td>
      <td>108</td>
    </tr>
    <tr>
      <td>Unrolled 4x</td>
      <td>140</td>
      <td>191</td>
    </tr>
  </tbody>
</table>

<p>Finally, note that while more unrolling is faster in the region where L1 hists is faster, the situation reverses itself around 64 KiB, and after that point no unrolling is fastest.</p>

<p>All this means that in this particular example you would face some tough tradeoffs if you want to reduce the impact by unrolling.</p>

<h3 id="prefetching">Prefetching</h3>

<p>You can solve this particular problem using software prefetching instructions. If you prefetch the lines you are going to store to, a totally different path is invoked: the same one that handles loads, and here the memory level parallelism will be available regardless of the the limitations of the store path. One complication is that, except for <code class="language-plaintext highlighter-rouge">prefetchw</code><sup id="fnref:prefetchw" role="doc-noteref"><a href="#fn:prefetchw" class="footnote" rel="footnote">14</a></sup>, such prefetches will be “shared OK” requests for the line, rather than an <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> (request for ownership). This means that the core might receive the line in the S MESI state, and then when the store occurs, a <em>second</em> request may be incurred to change the line from S state to M state. In my testing this didn’t see to be a problem in practice, perhaps because the lines are not shared across cores so generally arrive in the E state, and the E-&gt;M transition is cheap.</p>

<p>We don’t even really have to <strong>pre</strong>-fetch: that is, we don’t need to issue the prefetch instructions early (which would be hard in this case since we’d need to run ahead of the RNG) - we just issue the PF at the same spot we would have otherwise done the store. This transforms the nature of the request from a store to a load, which is the goal here - even though it doesn’t make the request visible to the CPU any earlier than before.</p>

<p>One question is which of the two regions to prefetch? The fixed region, the variable region or both? It turns out that “both” is a fine strategy and is often the sole fastest approach and is generally tied in the remaining cases. Here’s a look at all three approaches against no prefetching at all on the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> (128 KiB fixed region size):</p>

<p><img src="/assets/2019-03-19/skl/i-prefetch-new.svg" alt="Interleaved Stores with Prefetching" /></p>

<p>A key observation is that if you had decided to only prefetch one <em>or</em> the other of the two stores, you’d be slower than no prefetching at all over most of the range. It isn’t exactly clear to me why this is the case: perhaps the prefetches compete for fill buffers or otherwise result in a worse allocation of fill buffers to requests.</p>

<h3 id="avoiding-microcode-updates">Avoiding Microcode Updates</h3>

<p>The simplest solution is to simply avoid the newest <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> updates. These updates seem drive by new spectre mitigations, so if you are not enabling that functionality (e.g., SSDB is disabled by default in Linux, so if you aren’t explicitly enabling it, you won’t get it), perhaps you can do without these updates.</p>

<p>This strategy is not feasible once the <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> update contains something you need.</p>

<p>Additionally, as noted above, even the old microcodes <em>sometimes</em> experience the same slower performance that new microcodes always exhibit. I cannot exactly characterize the conditions in which this occurs, but one should at least be aware that old microcodes aren’t <em>always</em> fast.</p>

<h2 id="other-findings">Other findings</h2>

<p>This post is already longer than I wanted it to be. The idea is for posts closer in length to <a href="https://shipilev.net/jvm/anatomy-quarks/">JVM Anatomy Park</a> than <a href="https://en.wikipedia.org/wiki/War_and_Peace">War and Peace</a>. Still, there is a bunch of stuff uncovered which I’ll summarize here:</p>

<ul>
  <li>The current test uses regions whose addresses whose bottom 12 bits are identically zero, but whose 13th bit varies. That is, the regions “4K alias” but do not “8K alias”. Since the main loop uses the same random address for both regions (wrapped to region size by masking) in each iteration, this means that the stores alias as describe above. However, this is not the cause of the main effects reported here: you can remove the aliasing completely and the behavior is largely the same<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">15</a></sup>.</li>
  <li>You can go the other way too: if you increase the aliasing (you can try this by setting environment variable <code class="language-plaintext highlighter-rouge">ALLOW_ALIAS=1</code>) up to 64 KiB (bottom 16 bits of the <em>physical</em> address), I found a strong effect where performance was slower with the <em>old</em> <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>. This effect seems to have disappeared with the new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>. Now 64 KiB aliasing (especially <em>physical</em> aliasing) is probably a lot more rare than mixed L1 hits and L1 misses in the stream of stores, so I’d rather the old behavior than the new - but this is probably interesting enough to write about separately.</li>
  <li>I do sometimes see the “slow mode” behavior with earlier <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> versions. Almost a year ago, when the last several version of the <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> didn’t even exist, I experienced periodic slow mode behavior while benchmarking - the same type of performance in the L1 region as the current <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> shows all the time. On older <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> I can still reproduce this consistently: <em>if all CPUs are loaded when I start the <code class="language-plaintext highlighter-rouge">bench</code> process</em>. For example <code class="language-plaintext highlighter-rouge">./bench interleaved</code> consistently gives fast mode, but <code class="language-plaintext highlighter-rouge">stress -c 4 &amp; ./bench interleaved</code> consistently gives slow timings … <em>even when I kill the CPU using processes before the results roll in</em>. In that case, the tests keep running in slow mode even though it’s the only thing running on the system.<br /><br />This seems to explain why I randomly got slow mode in the past. For example, I noticed that something like <code class="language-plaintext highlighter-rouge">./bench interleaved &gt; data; plot-csv.py data</code> would give fast mode results, but when I shortened it to <code class="language-plaintext highlighter-rouge">./bench interleaved | plot-csv.py</code> it would be in slow mode, because apparently launching the python interpreter in parallel on the RHS of the pipe used enough CPU to trigger the slow mode. I had a weird 10 minutes or so where I’d run <code class="language-plaintext highlighter-rouge">./bench</code> without piping it and look at the data, and then try to plot it and it would be totally different, back and forth.</li>
  <li>I considered the idea that this bad behavior only shows up when the store buffer is full, e.g., because of some interaction that occurs when renaming is stalled on store buffer entries, but versions of the test which periodically drain the store buffer with <code class="language-plaintext highlighter-rouge">sfence</code> so it never becomes very full showed the same result.</li>
  <li>I examined the values of a lot more performance counters than the few shown above, but none of them provided any smoking gun for the behavior: they were all consistent with L1 hits simply blocking overlap of L1 miss stores on either side.</li>
</ul>

<h3 id="other-platforms">Other platforms</h3>

<p>An obvious and <abbr title="When discussing assembly instructions an immediate is a value embedded in the instruction itself, e.g., the 1 in add eax, 1.">immediate</abbr> question is what happens on other micro-architectures, beyond my Skylake client core.</p>

<p>On Haswell, the behavior is <em>always slow</em>. That is, whether with old or new <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>, store misses mixed with L1 store hits were much slower than expected. So if you target Haswell or (perhaps) Broadwell era hardware, you might want to keep this in mind regardless of <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version.</p>

<p>On Skylake-X (Xeon W-2401), the behavior is <em>always fast</em>. That is, even with the newest <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version I did not see the slow behavior. I also was not able to trigger the behavior by starting the test with loaded CPUs as I was with Skylake client with old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr>.</p>

<p>On Cannonlake I did not observe the slow behavior. I don’t know if I was using an “old” or “new” <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> as Intel does not publish <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> guidance for Cannonlake (and it isn’t clear to me if any Cannonlake microcodes have been released at all as very few chips were ever shipped).</p>

<p>You can look at the results for all the platforms I tested in the <a href="/assets/2019-03-19">assets directory</a>. The plots are the same as described above for Skylake plus some variants not show but which should be obvious from the title or filename.</p>

<h2 id="the-source">The Source</h2>

<p>You can have fun reproducing all these results yourself as my code is available in the store-bench project <a href="https://github.com/travisdowns/store-bench">on GitHub</a>. Documentation is a bit lacking, but it shouldn’t be to hard to figure out. Open an issue if anything is unclear or you find a bug, and pull requests gladly accepted.</p>

<h2 id="thanks">Thanks</h2>

<p>Thanks to Nathan Kurz and Leonard Inkret who pointed out some errors in the text and to <a href="https://lemire.me/en/">Daniel Lemire</a> who kindly provided additional hardware on which I was able to test these results.</p>

<h2 id="comments">Comments</h2>

<p>I don’t have a comment system, but I’ll reply to stuff posted in the <a href="https://news.ycombinator.com/item?id=19445566">Hacker News discussion</a>, or <a href="https://twitter.com/trav_downs/status/1108415599267450882">on Twitter</a>.</p>

<p class="info">If you liked this post, check out the <a href="/">homepage</a> for others you might enjoy.</p>

<hr />
<hr />
<p><br /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>Of course, to achieve one-write per cycle, your benchmark has to be otherwise quite efficient: among other things the process by which you generate random addresses needs to have a throughput of one per cycle too, so usually you’ll want cheat a bit on the RNG side. I wrote such a test and you can run it with <code class="language-plaintext highlighter-rouge">./bench wrandom1-unroll</code>. For buffer sizes that fit within L1, it achieves very close to 1 cycle per write (roughly 1.01 cycles per write for most buffer sizes). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:normal-stores" role="doc-endnote">
      <p>Here “normal stores” basically means stores that are to write-back (WB) memory regions and which are not the special non-temporal stores that x86 offers. Almost every store you’ll do from a typical program falls into this category. <a href="#fnref:normal-stores" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>In fact, we can test this - the <code class="language-plaintext highlighter-rouge">wlinear1-sfence</code> test is a linear write test like <code class="language-plaintext highlighter-rouge">wlinear1</code> except with an <code class="language-plaintext highlighter-rouge">sfence</code> instruction between every store. This flushes the store buffer, preventing any overlap in the stores and the observed time per store is in all cases a couple of cycles above the corresponding read latency (probably corresponding to <code class="language-plaintext highlighter-rouge">sfence</code> overhead). <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>This isn’t really the whole story though. If the L1 cache misses explained it, we’d expect performance to approach ~4 cycles (1 cycle L1 + 3 cycle L2) as the size of the region approaches 0, since at some point the smaller region will stay in L2 regardless of interference from other stores. It doesn’t happen though: performance flatlines at 6 cycles, the cost of two stores to L2. Perhaps what happens is that the L1 stores in the store buffer reduce the <abbr title="Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.">MLP</abbr> of the interleaved L2 stores because the <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> prefetch mechanism only has a certain horizon it examines. For example, maybe it examines the 10 entries closest to the store buffer head for prefetch candidates, and with the L1-hitting stores in there, there are only half as many L2-hitting stores to fetch. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>By <em>real code</em> I simply mean something that is not a benchmark, not necessarily anything actually useful. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>See your <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> version on Linux using <code class="language-plaintext highlighter-rouge">cat /proc/cpuinfo | grep -m1 micro</code> or <code class="language-plaintext highlighter-rouge">dmesg | grep micro</code>. The latter option also helps you determine if the <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> was updated during boot by the Linux <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> driver. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>I used the weasel word “contributes” here rather than “ultimately results in an L1 miss” to cover the case where two stores occur to the same line in short succession and that line is not in L1. In this case, both stores will miss, but there will generally only be one reference to L2 since the fill buffers operate on whole cache lines, so both stores will be satisfied by the same miss. The same effect occurs for loads and can be measured explicitly by the <code class="language-plaintext highlighter-rouge">mem_load_retired.fb_hit</code> event: those are loads that missed in L1, but subsequently hit the <em>fill buffer</em> (aka miss status handling register) allocated for an earlier access to the same cache line that also missed. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:sfence-note" role="doc-endnote">
      <p>Actually, this doesn’t seem to be strictly true. The results on some CPUs are too good to represent zero overlapping between stores. E.g., the <a href="/assets/2019-03-19/skl/i-sfence-old.svg">old <abbr title="Internal instructions and other logic forming part of a CPU which may be used to implement user-visible instructions and control other aspects of CPU behavior and which may be modified dynamically by vendor-provided updates.">microcode</abbr> results</a>) show the sfenceB results staying under 30 cycles even for main-memory sized regions (and quite close to the no sfence results), which is only possible with a lot of store overlapping. So something remains to be discovered about sfence behavior. <a href="#fnref:sfence-note" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>In particular, when the variable sized region is small, we expect the fixed region write to always hit in L1 or L2 (since the total working set fits in L2), with a ratio approaching 1:3 as the variable region goes to zero. When the variable region is large, we expect many fixed region writes to hit in L2 and less frequently L1, but some will miss even in L2 as the working set is larger than L2 and with random writes some fixed region lines will be evicted before they are written again. The cache related performance counters agree with this hand-waving explanation. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p><em>Senior stores</em> are stores that have retired (the instruction has been completed in the <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> engine), but whose value hasn’t yet been committed to the memory subsystem and hence are not globally observable. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:patent-note" role="doc-endnote">
      <p>This patent is interesting not least because the title is “Apparatus and method for store address for store address prefetch <strong>and line locking</strong>”, but as far as I can tell the latter part about “line locking” is never mentioned again in the body of the patent. One might imagine that line locking involves something like delaying or nacking incoming snoops for a line that is about to be written. <a href="#fnref:patent-note" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:wc-note" role="doc-endnote">
      <p>It is an open question whether normal writes which miss in L1 simply wait in the store buffer until the <abbr title="Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.">RFO</abbr> request is complete, or whether they instead get stashed in a write combining buffer associated with the cache line, potentially collecting several store misses to the same line. The latter sounds more efficient but any combining of stores out of order with respect to the store buffer is problematic: committing multiple such WC buffers when the line is available in L1 could change the apparent order of stores unless all WC buffers are committed as a unit or some other approach is taken. <a href="#fnref:wc-note" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:wc-stores" role="doc-endnote">
      <p>I am not 100% sure this is the mechanism, versus an alternative of say stalling the store buffer with the missing store at the head, until the fill buffer returns, but we have evidence both in wording from the Intel manuals, and <a href="https://stackoverflow.com/a/53438221">this answer on StackOverflow</a>. The main argument in favor of this implementation would be performance: it prevents the store buffer from stalling, allowing more stores to commit or start additional requests to memory and keeping the store buffer smaller to avoid stalling the front-end. The main argument against is that it seems hard to maintain ordering in this scenario: if a stream of stores is coalesced into more than one fill buffer, the relative order between the stores is lost, and it is not in general possible to commit the store buffers to L1 “one at a time” while preserving the original store order, you’d basically have to commit all the fill buffers at once (atomically wrt the outside world), or put limits on what stores can be coalesced. <a href="#fnref:wc-stores" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:prefetchw" role="doc-endnote">
      <p>The <code class="language-plaintext highlighter-rouge">prefetchw</code> instruction has long been supported by AMD, but on Intel it is only supported since Broadwell. Earlier Intel chips didn’t implement this functionality, but the <code class="language-plaintext highlighter-rouge">prefetchw</code> opcode was still accepted and executed as a no-op. <a href="#fnref:prefetchw" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>I did notice <em>some</em> differences when removing the aliasing: for example, sfenceA and sfenceB converged and finally performed the same as the region size increased, rather than sfenceB crossing over and being several cycles faster than sfenceA. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- travis override -->
  
    <section class="comments" id="comment-section">
  <hr>
  
  <!-- Existing comments -->
  <div class="comments__existing">
    <h2>Comments</h2>
    
    
    <!-- List main comments in reverse date order, newest first. List replies in date order, oldest first. -->
    
    

<article id="comment-ce0e0ec0-5176-11ea-a5fe-fd19547f6821" class="js-comment comment" uid="ce0e0ec0-5176-11ea-a5fe-fd19547f6821">

  <div class="comment__author">
    Leonard Inkret
    <span class="comment__date">•
        <a href="#comment-ce0e0ec0-5176-11ea-a5fe-fd19547f6821" title="Permalink to this comment">February 17th, 2020 11:15</a></span>
  </div>

  <div class="comment__body">
    <p>“This means that the core might receive the line in the S MESI state, adn then When the store occurs, a second request may be incurred to change the line from S state to M state.”</p>

<p>Typo, change “adn” to “and”</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-ce0e0ec0-5176-11ea-a5fe-fd19547f6821', 'respond', 'random-writes-and-microcode-oh-my', 'ce0e0ec0-5176-11ea-a5fe-fd19547f6821')">↪&#xFE0E; Reply to Leonard Inkret</a>
    </div>
</article>
  

<article id="comment-bdd320d0-51cc-11ea-9b7c-a15aa610580d" class="js-comment comment admin child" uid="bdd320d0-51cc-11ea-9b7c-a15aa610580d">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-bdd320d0-51cc-11ea-9b7c-a15aa610580d" title="Permalink to this comment">February 17th, 2020 21:30</a></span>
  </div>

  <div class="comment__body">
    <p>Thanks, fixed and credited!</p>

<p>I also fixed that nearby capitalized-for-no-reason “When”.</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
  </div>
  

  <!-- New comment form -->
  <div id="respond" class="comment__new">
    <form class="js-form form" method="post" action="https://staticman-travisdownsio.herokuapp.com/v2/entry/travisdowns/travisdowns.github.io/master/comments">
  <input type="hidden" name="options[origin]" value="https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html">
  <input type="hidden" name="options[parent]" value="https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html">
  <input type="hidden" id="comment-replying-to-uid" name="fields[replying_to_uid]" value="">
  <input type="hidden" name="options[slug]" value="random-writes-and-microcode-oh-my">
  
  
  <div class="textfield">
    <label for="comment-form-message"><h2>Add Comment<small><a rel="nofollow" id="cancel-comment-reply-link" href="https://travisdowns.github.io/blog/2019/03/19/random-writes-and-microcode-oh-my.html#respond" style="display:none;">(cancel reply)</a></small></h2>
      <textarea class="textfield__input" name="fields[message]" type="text" id="comment-form-message" placeholder="Your comment (markdown accepted)" required rows="6"></textarea>
    </label>
  </div>

    <div class="textfield narrowfield">
      <label for="comment-form-name">Name
        <input class="textfield__input" name="fields[name]" type="text" id="comment-form-name" placeholder="Your name (required)" required/>
      </label>
    </div>

    <div class="textfield narrowfield">
      <label for="comment-form-email">E-mail
        <input class="textfield__input" name="fields[email]" type="email" id="comment-form-email" placeholder="Your email (optional)"/>
      </label>
    </div>

    <div class="textfield narrowfield hp">
      <label for="hp">
        <input class="textfield__input" name="fields[hp]" id="hp" type="text" placeholder="Leave blank">
      </label>
    </div>

    

    <button class="button" id="comment-form-submit">
      Submit
    </button>

</form>

<article class="modal mdl-card mdl-shadow--2dp">
  <div>
    <h3 class="modal-title js-modal-title"></h3>
  </div>
  <div class="mdl-card__supporting-text js-modal-text"></div>
  <div class="mdl-card__actions mdl-card--border">
    <button class="button mdl-button--colored mdl-js-button mdl-js-ripple-effect js-close-modal">Close</button>
  </div>
</article>

<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="display:none" >
  <symbol id="icon-loading" viewBox="149.8 37.8 499.818 525"><path d="M557.8 187.8c13.8 0 24.601-10.8 24.601-24.6S571.6 138.6 557.8 138.6s-24.6 10.8-24.6 24.6c0 13.2 10.8 24.6 24.6 24.6zm61.2 90.6c-16.8 0-30.6 13.8-30.6 30.6s13.8 30.6 30.6 30.6 30.6-13.8 30.6-30.6c.6-16.8-13.2-30.6-30.6-30.6zm-61.2 145.2c-20.399 0-36.6 16.2-36.6 36.601 0 20.399 16.2 36.6 36.6 36.6 20.4 0 36.601-16.2 36.601-36.6C595 439.8 578.2 423.6 557.8 423.6zM409 476.4c-24 0-43.2 19.199-43.2 43.199s19.2 43.2 43.2 43.2 43.2-19.2 43.2-43.2S433 476.4 409 476.4zM260.8 411c-27 0-49.2 22.2-49.2 49.2s22.2 49.2 49.2 49.2 49.2-22.2 49.2-49.2-22.2-49.2-49.2-49.2zm-10.2-102c0-27.6-22.8-50.4-50.4-50.4-27.6 0-50.4 22.8-50.4 50.4 0 27.6 22.8 50.4 50.4 50.4 27.6 0 50.4-22.2 50.4-50.4zm10.2-199.8c-30 0-54 24-54 54s24 54 54 54 54-24 54-54-24.6-54-54-54zM409 37.8c-35.4 0-63.6 28.8-63.6 63.6S374.2 165 409 165s63.6-28.8 63.6-63.6-28.2-63.6-63.6-63.6z"/>
  </symbol>
</svg>



  </div>
</section>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="/assets/main.js"></script>


  
  <!-- end override -->

  <a class="u-url" href="/blog/2019/03/19/random-writes-and-microcode-oh-my.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Performance Matters</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Travis Downs</li><li><a class="u-email" href="mailto:travis.downs@gmail.com">travis.downs@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/travisdowns"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">travisdowns</span></a></li><li><a href="https://www.twitter.com/trav_downs"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">trav_downs</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about low-level software and hardware performance.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
