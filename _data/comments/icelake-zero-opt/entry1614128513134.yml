_id: e1ddce50-763b-11eb-9ce9-c7a446ce00c0
_parent: 'https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html'
replying_to_uid: f26ed410-75fc-11eb-b1b1-b1e90cd6ebd3
message: "Hi Noah,\r\n\r\nThanks for your (comprehensive!) comment.\r\n\r\nI think RFO-ND might be a bit of a red herring here: none of the vanilla store types use this protocol, so we can mostly ignore it and treat `rep stosb` as its own special thing which doesn't get 0-over-0 optimization, and which can affect prefetchers in a totally differenet way, etc. The fact that aligned 64-byte `zmm` writes don't use the ND protocol is perhaps a bit surprising, but maybe not _that_ surprising in that back-to-back contiguous, aligned 32-byte writes also did not use it on previous architectures and given the intermediate buffering provided by the LFB it seems like even the narrower stores could have done this, if it were easy enough. It seems like it is not easy enough.\r\n\r\nI am suprised by the `perf` results for case 3 vs the other two cases. There are more than 2x the number of RFO requests. I would have expected about the same numbers: yes, there is a lack of coalescing in the LFB (so I think performance would suffer) but I would expect ultimately that the second writes to each line would \"hit\" in the existing LFB or L1 (when they are allowed to proceed) and not generate any additional RFO requests. \r\n\r\nWhat do those numbers look like when normalized against the number of cache lines written? I.e., how many cache lines are written by the test? This helps understand which if any of the tests is close to 1:1 with lines written/RFO.\r\n\r\nI am not sure the optimization is related to how the data is returned by the RFO. I believe it may happen much later, when the relevant lines are _evicted_. For instances, there is the test that initially writes non-zero values to a line, then writes zero values: in this case the optimization would not apply at the time of the RFO (non-zero values), but it would apply at the time of eviction (0s were written after, but w/o any new RFO since the lines are already locally cached in M state): in this case, I still saw the 0-over-0 optimization happen. Also, I saw it happen when the entire line is not overwritten (from my notes, although they aren't 100% clear on this). \r\n\r\nI believe the optimization may be load/occupancy related: e.g., only happen sometimes, when the load between some cache levels is above/below some level, or some other condition is met. In this case, small changes to the test might change things enough to affect that threshold.\r\n\r\nAnother possibility is more along the lines you mentioned: as part of the coherence flows, the \"R\" (data) and \"O\" (grant of exclusive ownership) might arrive at different times, in two different messages. Also, sometimes the outer cache might send a \"pull\" request to the inner cache for the data (after the inner cache indicates that it wants to write it), while other times the inner cache might send the data without a pull. Perhaps whether the optimization applies depends on the order of the messages or another detail like this and timing changes it.\r\n\r\nYour guess that"
name: Travis Downs
email: c6937532928911c0dae3c9c89b658c09
hp: ''
date: 1614128513
