<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">  
  <meta name="color-scheme" content="light dark">
  <link rel="stylesheet" href="/assets/css/light.css">
  <link rel="stylesheet" href="/assets/css/dark.css" media="(prefers-color-scheme: dark)">
<!-- Begin Jekyll SEO tag v2.7.1p -->
<title>Performance Matters | A blog about low-level software and hardware performance.</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Performance Matters" />
<meta name="author" content="Travis Downs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A blog about low-level software and hardware performance." />
<meta property="og:description" content="A blog about low-level software and hardware performance." />
<link rel="canonical" href="https://travisdowns.github.io/misc/avxfreq-orig.html" />
<meta property="og:url" content="https://travisdowns.github.io/misc/avxfreq-orig.html" />
<meta property="og:site_name" content="Performance Matters" />
<meta property="og:image" content="https://travisdowns.github.io/assets/avxfreq1/twitter-card.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://travisdowns.github.io/assets/avxfreq1/twitter-card.png" />
<meta property="twitter:title" content="Performance Matters" />
<meta name="twitter:site" content="@trav_downs" />
<meta name="twitter:creator" content="@trav_downs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Travis Downs"},"description":"A blog about low-level software and hardware performance.","headline":"Performance Matters","image":"https://travisdowns.github.io/assets/avxfreq1/twitter-card.png","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://travisdowns.github.io/assets/rabbit3.png"},"name":"Travis Downs"},"url":"https://travisdowns.github.io/misc/avxfreq-orig.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://travisdowns.github.io/feed.xml" title="Performance Matters" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-136594956-1"></script>
<script>
  window['ga-disable-UA-136594956-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-136594956-1');
</script>

</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Performance Matters</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline"></h1>
    <p class="post-meta"><time class="dt-published" datetime="" itemprop="datePublished">
        
      </time><span>
          •
          <span class="tag-link"><a href="/tags/performance.html">performance</a></span><span class="tag-link"><a href="/tags/c++.html">c++</a></span><span class="tag-link"><a href="/tags/Intel.html">Intel</a></span><span class="tag-link"><a href="/tags/uarch.html">uarch</a></span>
        </span></p>
    <!-- end override -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p class="info"><strong>Note:</strong> For the really short version, you can <a href="#summary">skip to the summary</a>, but then what will do you for the rest of the day?</p>

<h2 id="introduction">Introduction</h2>

<p>This is a post about AVX and AVX-512 related frequency scaling<sup id="fnref:first" role="doc-noteref"><a href="#fn:first" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Now, something more than nothing has been written about this already, including <a href="https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/">cautionary tales</a> of performance loss and some <a href="https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/">broad guidelines</a><sup id="fnref:dmore" role="doc-noteref"><a href="#fn:dmore" class="footnote" rel="footnote">2</a></sup>, so do we really need to add to the pile?</p>

<p>Perhaps not, but I’m doing it anyway. My angle is a lower level look, almost microscopic really, at the specific transition behaviors. One would hope that this will lead to specific, <em>quantitative</em> advice about exactly when various instruction types are likely to pay off, but (spoiler) I didn’t make it there in this post.</p>

<p>Now I wasn’t really planning on writing about this just now, but I got off on a (nested) tangent<sup id="fnref:intro" role="doc-noteref"><a href="#fn:intro" class="footnote" rel="footnote">3</a></sup>, so let’s examine the AVX-512 downclocking behavior using target tests. At a minimum, this is necessary background for the next post, but I hope that it is also standalone interesting.</p>

<p class="info"><strong>Note:</strong> If you are here because of your footnote fetish, skip straight to the <a href="#footnotes">good 🦶 stuff</a>.</p>

<h3 id="table-of-contents">Table of Contents</h3>

<p>You could perhaps trying skipping ahead to a section that interests you using this obligatory table of contents, but sections are not self contained, so you’ll be better off reading the whole thing linearly.</p>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="#table-of-contents" id="markdown-toc-table-of-contents">Table of Contents</a></li>
    </ul>
  </li>
  <li><a href="#the-source" id="markdown-toc-the-source">The Source</a></li>
  <li><a href="#test-structure" id="markdown-toc-test-structure">Test Structure</a>    <ul>
      <li><a href="#hardware" id="markdown-toc-hardware">Hardware</a></li>
    </ul>
  </li>
  <li><a href="#tests" id="markdown-toc-tests">Tests</a>    <ul>
      <li><a href="#256-bit-integer-simd-avx" id="markdown-toc-256-bit-integer-simd-avx">256-bit Integer <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> (AVX)</a></li>
      <li><a href="#512-bit-integer-simd-avx-512" id="markdown-toc-512-bit-integer-simd-avx-512">512-bit Integer <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> (AVX-512)</a>        <ul>
          <li><a href="#enter-ipc" id="markdown-toc-enter-ipc">Enter <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr></a></li>
          <li><a href="#voltage-only-transitions" id="markdown-toc-voltage-only-transitions">Voltage Only Transitions</a></li>
          <li><a href="#attenuation" id="markdown-toc-attenuation">Attenuation</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#what-was-left-out" id="markdown-toc-what-was-left-out">What Was Left Out</a></li>
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#thanks" id="markdown-toc-thanks">Thanks</a></li>
  <li><a href="#discuss" id="markdown-toc-discuss">Discuss</a></li>
</ul>

<h2 id="the-source">The Source</h2>

<p>All of the code underlying this post is available in the <a href="https://github.com/travisdowns/freq-bench/tree/post1">post1 branch of freq-bench</a>, so you can follow along at home, check my work, and check out the behavior on your own hardware. It requires Linux and the <a href="https://github.com/travisdowns/freq-bench/blob/post1/README.md">README</a> gives basic clues on getting started.</p>

<p>The source includes the <a href="https://github.com/travisdowns/freq-bench/blob/post1/scripts/data.sh">data generation scripts</a> as well as those to <a href="https://github.com/travisdowns/freq-bench/blob/post1/scripts/plots.sh">generate the plots</a>. Neither shell scripting nor Python are my forte, so be gentle.</p>

<h2 id="test-structure">Test Structure</h2>

<p>We want to investigate what happens when instruction stream related performance transitions occur. The most famous example is what happens when you execute an AVX-512 instruction<sup id="fnref:widthmatters" role="doc-noteref"><a href="#fn:widthmatters" class="footnote" rel="footnote">4</a></sup> for the first time in a while, but as we will see there are other cases.</p>

<p>The basic idea is that the test has a <em>duty period</em> and every time this period elapses, we run a test-specific payload for the duration of the <em>payload period</em> which consists of one or more “interesting” instructions (which depend on the test). During the entire test we sample various metrics at a best-effort fixed frequency. This repeats for the entire test period. The sample period will generally be much smaller than the duty period<sup id="fnref:speriod" role="doc-noteref"><a href="#fn:speriod" class="footnote" rel="footnote">5</a></sup>: in our tests we use a 5,000 μs duty period and a sample period of 1 μs, mostly.</p>

<p>Visually, it is something like this (showing a single duty period: one benchmark is composed of multiple duty cycles back to back):</p>

<p><img src="/assets/avxfreq1/test-structure.png" alt="Test Structure" /></p>

<p>This diagram shows the payload period as occupying a non-negligible amount of time. However, in the first few first tests, the payload period is essentially zero: we run the payload function (which consists of only a couple instructions) only once, so it is really a payload <em>moment</em> rather than period.</p>

<h3 id="hardware">Hardware</h3>

<p>We are running these tests on a <abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr> architecture W-series CPU: a W-2104<sup id="fnref:f2104" role="doc-noteref"><a href="#fn:f2104" class="footnote" rel="footnote">6</a></sup> with the following <a href="https://stackoverflow.com/a/56861355/149138">license-based</a> frequencies<sup id="fnref:avxt" role="doc-noteref"><a href="#fn:avxt" class="footnote" rel="footnote">7</a></sup>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>License</th>
      <th>Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-AVX Turbo</td>
      <td>L0</td>
      <td>3.2 GHz</td>
    </tr>
    <tr>
      <td>AVX Turbo</td>
      <td>L1</td>
      <td>2.8 GHz</td>
    </tr>
    <tr>
      <td>AVX-512 Turbo</td>
      <td>L2</td>
      <td>2.4 GHz</td>
    </tr>
  </tbody>
</table>

<p>For one (voltage) test I also use my Skylake (mobile) <a href="https://ark.intel.com/content/www/us/en/ark/products/88967/intel-core-i7-6700hq-processor-6m-cache-up-to-3-50-ghz.html">i7-6700HQ</a>, running at either it’s nominal frequency of 2.6 GHz, or the turbo frequency of 3.5 GHz.</p>

<h2 id="tests">Tests</h2>

<p>The basic approach this post will take is examining the CPU behavior using the test framework above, primarily varying what the payload is, and what metrics we look at. Let’s get the ball rolling with 256-bit instructions.</p>

<h3 id="256-bit-integer-simd-avx">256-bit Integer <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> (AVX)</h3>

<p>For the first test will use as payload the <code class="language-plaintext highlighter-rouge">vporymm_vz</code> function, which is just a single 256-bit <code class="language-plaintext highlighter-rouge">vpor</code> instruction, followed by a <code class="language-plaintext highlighter-rouge">vzeroupper</code><sup id="fnref:vz" role="doc-noteref"><a href="#fn:vz" class="footnote" rel="footnote">8</a></sup>:</p>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">vporymm_vz:</span>
  <span class="nf">vpor</span>   <span class="nv">ymm0</span><span class="p">,</span><span class="nv">ymm0</span><span class="p">,</span><span class="nv">ymm0</span>
  <span class="nf">vzeroupper</span> 
  <span class="nf">ret</span>    
</code></pre></div></div>

<p>We call the payload function only once at the start of each duty period<sup id="fnref:pperiod" role="doc-noteref"><a href="#fn:pperiod" class="footnote" rel="footnote">9</a></sup>. The duty period is set to 5000 μs and the sample period to 1 μs, and the total test time is set to 31,000 μs (so the payload will execute 7 times).</p>

<p>Here’s the result (plot notes<sup id="fnref:plotnotes" role="doc-noteref"><a href="#fn:plotnotes" class="footnote" rel="footnote">10</a></sup>), with time along the x axis<sup id="fnref:falk" role="doc-noteref"><a href="#fn:falk" class="footnote" rel="footnote">11</a></sup>, showing the measured frequency at each sample (there are three separate test runs shown<sup id="fnref:threerun" role="doc-noteref"><a href="#fn:threerun" class="footnote" rel="footnote">12</a></sup>):</p>

<p><img src="/assets/avxfreq1/fig-vporvz256.svg" alt="256-bit vpor transitions" /></p>

<p>Well that’s really boring. The entire test runs consistently at 3.2 GHz, the nominal (L0 license) frequency, if we ignore the a few uninteresting outliers<sup id="fnref:outlie" role="doc-noteref"><a href="#fn:outlie" class="footnote" rel="footnote">13</a></sup>.</p>

<h3 id="512-bit-integer-simd-avx-512">512-bit Integer <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> (AVX-512)</h3>

<p>Before the crowd gets too rowdy, let’s quickly move on to the next test, which is identical except that it uses 512-bit <code class="language-plaintext highlighter-rouge">zmm</code> registers:</p>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">vporzmm_vz:</span>
  <span class="nf">vpor</span>   <span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span>
  <span class="nf">vzeroupper</span> 
  <span class="nf">ret</span>    
</code></pre></div></div>

<p>Here is the result:</p>

<p><img src="/assets/avxfreq1/fig-vporvz512.svg" alt="512-bit vpor transitions" /></p>

<p>We’ve got something to sink our teeth into!</p>

<p>Remember that the duty cycle is 5000 μs, so at each x-axis tick we execute the payload. Now the behavior is clear: every time the payload instruction executes (at multiples of 5000 μs), the frequency drops from the 3.2 GHz L0 license down to 2.8 GHz L1 license frequency. So far this is all pretty much as expected.</p>

<p>Let’s zoom in on one of the transition points at 15,000 μs:</p>

<p><img src="/assets/avxfreq1/fig-vpor-zoomed.svg" alt="512-bit vpor transitions (zoomed)" /></p>

<p>We can make the following observations:</p>

<ol>
  <li>There is a transition period (the rightmost of the two shaded regions, in orange<sup id="fnref:peachpuff" role="doc-noteref"><a href="#fn:peachpuff" class="footnote" rel="footnote">14</a></sup>) of ~11 μs<sup id="fnref:resnote" role="doc-noteref"><a href="#fn:resnote" class="footnote" rel="footnote">15</a></sup> where the CPU is halted: no samples occur during this period<sup id="fnref:halted" role="doc-noteref"><a href="#fn:halted" class="footnote" rel="footnote">16</a></sup>. For fun, I’ll call this a <em>frequency transition</em>.</li>
  <li>The leftmost shaded region, shown in purple<sup id="fnref:thistle" role="doc-noteref"><a href="#fn:thistle" class="footnote" rel="footnote">17</a></sup>, immediately following the payload execution at 15,000 μs and prior to the halted region, is ~9 μs long and the frequency remains unchanged. This is not just a test issue or measurement error: this period occurs after the payload and is consistently reproducible<sup id="fnref:confirm" role="doc-noteref"><a href="#fn:confirm" class="footnote" rel="footnote">18</a></sup>. Although it looks like nothing interesting is going on in this region, we’ll soon see it is indeed special and will call this region a <em>voltage-only</em> transition.</li>
  <li>Although not fully shown in the zoomed plot, the lower 2.8 GHz frequency period lasts for ~650 μs.</li>
  <li>Not shown in the zoomed plot (but seen as a second downwards spike on the full plot, after the ~650 μs period of low frequency), there is another fully halted period of ~11 us, after which the CPU returns to it’s maximum speed of 3.2 GHz (L0 license).</li>
  <li>These attributes are mostly consistent across the three runs (so much that the series, in green, mostly overlaps and obscures the others) – but there are a few outliers in where the return to 3.2 GHz takes somewhat longer. This is consistent across runs: recovery is never <em>faster</em> than ~650 μs, but sometimes longer. I believe it occurs when an interrupt during the L1 region “resets the timer”.</li>
</ol>

<h4 id="enter-ipc">Enter <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr></h4>

<p>Although it is not visible in this plot, there is something special about the behavior of 512-bit instructions in the first shaded (purple) region – that is, in the 9 microseconds between the execution of the payload instruction and before the subsequent halted period: <em>they execute much slower than usual</em>.</p>

<p>This is easiest to see if we extend the payload period: instead executing the payload function once every 5000 μs, then looping on <code class="language-plaintext highlighter-rouge">rdtsc</code>, waiting for the next sample, we will continue to execute the payload function for 100 μs after a new duty period starts (that is, the <em>payload period</em> is set to 100 μs). During this time we still take samples as usual, every 1 μs – but in between samples we are executing the payload instruction(s)<sup id="fnref:whynot" role="doc-noteref"><a href="#fn:whynot" class="footnote" rel="footnote">19</a></sup>. So one duty period now looks like 100 μs of payload followed by 4850 μs of normal payload-free hot spinning.</p>

<p>We lengthen the payload period in order to examine the performance of the payload instructions. There are several metrics we could look at, but a simple one is to look at <em>instructions per second</em>. As long as we make sure the large majority of the executed instructions are payload instructions, the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> will largely reflect the execution of the payload<sup id="fnref:ideal" role="doc-noteref"><a href="#fn:ideal" class="footnote" rel="footnote">20</a></sup>.</p>

<p>As payload, we will use a function composed simply of 1,000 dependent 512-bit <code class="language-plaintext highlighter-rouge">vpord</code> instructions:</p>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">vpord</span>  <span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span>
<span class="nf">vpord</span>  <span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span>
<span class="c1">; ... 997 more like this</span>
<span class="nf">vpord</span>  <span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span><span class="p">,</span><span class="nv">zmm0</span>
<span class="nf">vzeroupper</span> 
<span class="nf">ret</span>    
</code></pre></div></div>

<p>We <a href="https://uops.info/table.html?search=vpord%20(zmm%2C%20zmm%2C%20zmm)&amp;cb_lat=on&amp;cb_tp=on&amp;cb_uops=on&amp;cb_ports=on&amp;cb_SKX=on&amp;cb_measurements=on&amp;cb_iaca30=on&amp;cb_avx512=on">know</a> these <code class="language-plaintext highlighter-rouge">vpord</code> instructions have a latency of 1 cycle and here they are serially dependent so we expect this function to take 1,000 cycles, give or take<sup id="fnref:give" role="doc-noteref"><a href="#fn:give" class="footnote" rel="footnote">21</a></sup>, for an <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> of 1.0.</p>

<p>Here’s what a the same zoomed transition point for this looks like, with <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> plotted on the secondary axis:</p>

<p><img src="/assets/avxfreq1/fig-ipc-zoomed-zmm.svg" alt="512-bit vpor transitions (with IPC)" /></p>

<p>First, note that in the unshaded regions on the left (before 15,000 μs) and right (after 15,100 μs), the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> is basically irrelevant: no payload instructions are being executed, so the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> there is just the whatever the measurement code happens to net out to. We only care about the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> in the shaded regions, where the payload is executing.</p>

<p>Let’s tackle the regions from right to left, which happens to correspond to obvious to less obvious.</p>

<p>We have the blue region, running from ~15020 μs to 15100 μs (where the extra payload period ends). Here the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> is right at 1 instruction per cycle. So the payload is executing right at the expected rate, i.e., <em>full speed</em>. Keeners may point out that the very beginning of the blue period, the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> (and the measured frequency) is a bit noisier and slightly above 1. This is not a CPU effect, but rather a measurement one: during this phase the benchmark is <em>catching up</em> on samples missed during the previous halted period, which changes the payload to overhead ratio and bumps up the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> (details<sup id="fnref:catchup" role="doc-noteref"><a href="#fn:catchup" class="footnote" rel="footnote">22</a></sup>).</p>

<p>The middle, orange, region shows us what we’ve already seen: the CPU is halted, so no samples occur. <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> doesn’t tell us much here.</p>

<h4 id="voltage-only-transitions">Voltage Only Transitions</h4>

<p>The most interesting part is the first shaded region (purple): after the payload starts running but before the halt which I call a <em>voltage only</em> transition for reasons that will soon become clear.</p>

<p>Here, we see that the payload executes <em>much</em> more slowly, with an <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> of ~0.25. So in this region, the <code class="language-plaintext highlighter-rouge">vpord</code> instructions are apparently executing at <em>four times</em> their normal latency. I also observe an identical 4x slowdown for <code class="language-plaintext highlighter-rouge">vpord</code> throughput, using an identical <a href="https://github.com/travisdowns/freq-bench/blob/434c7cf5db73e2d48061e78525c7bbf7eb7757a3/basic-impls.cpp#L64">test</a> except with independent <code class="language-plaintext highlighter-rouge">vpord</code> instructions<sup id="fnref:tput" role="doc-noteref"><a href="#fn:tput" class="footnote" rel="footnote">23</a></sup>.</p>

<p>Perhaps surprisingly, this same slowdown occurs for 256-bit <code class="language-plaintext highlighter-rouge">ymm</code> instructions as well. This contradicts the conventional wisdom that on AVX-512 chips there is no penalty to using light 256-bit instructions:</p>

<p><img src="/assets/avxfreq1/fig-ipc-zoomed-ymm.svg" alt="256-bit vpor transitions (with IPC)" /></p>

<p>The results shown above are for a test identical to the 512-version except that it uses 256-bit <code class="language-plaintext highlighter-rouge">vpor ymm0, ymm0, ymm0</code> as the payload. It shows the same slowdown for ~9 μs after the payload starts executing, but no subsequent halt and no frequency transition. That is, it shows a voltage-only transition (lack of frequency transition is expected because we don’t expect a turbo license change for light 256-bit instructions).</p>

<p><a name="xmmeffect"></a>By now, you are probably wondering about 128-bit <code class="language-plaintext highlighter-rouge">xmm</code> registers. The good news is that these show no effect at all:</p>

<p><img src="/assets/avxfreq1/fig-ipc-zoomed-xmm.svg" alt="128-bit vpor transitions (with IPC)" /></p>

<p>Here, the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> jumps immediately to the expected value. So it appears that the CPU runs in a state where the 128-bit lanes are ready to go at all times<sup id="fnref:orisit" role="doc-noteref"><a href="#fn:orisit" class="footnote" rel="footnote">24</a></sup>.</p>

<p>The conventional wisdom regarding this “warmup” period is that the upper part<sup id="fnref:upper" role="doc-noteref"><a href="#fn:upper" class="footnote" rel="footnote">25</a></sup> of the vector units is shut down when not in use, and takes time to power up. The story goes that during this power-up period the CPU does not need to halt but it runs <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instructions at a reduced throughput <em>by splitting up the input</em> into 128-bit chunks and passing the data two or more times through the powered-on 128-bit lanes<sup id="fnref:amd" role="doc-noteref"><a href="#fn:amd" class="footnote" rel="footnote">26</a></sup>.</p>

<p>However, there are some observations that seem to contradict this hypothesis (in rough order from least to most convincing):</p>

<ol>
  <li>The observed impact to latency and throughput is ~4x, whereas I would expect 2x for simple instructions such as <code class="language-plaintext highlighter-rouge">vpor</code>.</li>
  <li>The timing is the same for 256-bit and 512-bit instructions: despite that 512-bit instructions take at least 2x the work, i.e., need to be passed through the 128-bit unit at least 4 times.</li>
  <li>Some instructions are more difficult to implement using this type of splitting, e.g., instructions where both high and low output lanes depend on all of the input lanes<sup id="fnref:ewise" role="doc-noteref"><a href="#fn:ewise" class="footnote" rel="footnote">27</a></sup> (see how slow they are on Zen). I expected that maybe these instructions would be slower when running in split mode, but I tested <code class="language-plaintext highlighter-rouge">vpermd</code> and found that it runs at 4L4T<sup id="fnref:lt" role="doc-noteref"><a href="#fn:lt" class="footnote" rel="footnote">28</a></sup>, compared to 3L1T normally. So <code class="language-plaintext highlighter-rouge">vpermd</code> (including the 512-bit version) didn’t slow more than <code class="language-plaintext highlighter-rouge">vpor</code>, and in fact in a relative sense it slowed down <em>less</em> (e.g., the latency only changed from 3 to 4). The fact that the latency and throughput reacted differently for this instruction seems odd, and that it has now the exact same 4L4T timing as <code class="language-plaintext highlighter-rouge">vpor</code> seems like a strange coincidence.</li>
  <li>Oddly, when I tried to time the slowdown more precisely, I kept coming with fractional value around 4.2x, not 4.0x, kind of contradicting the idea that the instruction is simply operating in a different mode, which should still have an integral latency.</li>
  <li>As it turns out, <em>all ALU<sup id="fnref:alu" role="doc-noteref"><a href="#fn:alu" class="footnote" rel="footnote">29</a></sup> instructions</em> are slower in this mode, not just wide <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> ones.</li>
</ol>

<p>It was 5 that sealed the deal on this not being a slowdown related to split execution. I believe what is actually happening is the CPU is doing very fine-grained throttling when wider instructions are executing in the core. That is, the upper lanes <em>are</em> being used in this mode (they are either not gated at at all, or are gated but enabling them is very quick, less than 1 μs) but execution frequency is reduced by 4x because CPU power delivery is not a state that can handle full-speed execution of these wider instructions, yet. While the CPU waits (e.g., for voltage to rise, fattening the guardband) for higher power execution to be allowed, this fine-grained throttling occurs.</p>

<p>This throttling affects non-<abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instructions too, causing them to execute at 4x their normal latency and inverse throughput. We can show with the following test, which combines a single <code class="language-plaintext highlighter-rouge">vpor ymm0, ymm0, ymm0</code> with N chained <code class="language-plaintext highlighter-rouge">add eax, 0</code> instructions, shown here for N = 3:</p>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">vpor</span>   <span class="nv">ymm0</span><span class="p">,</span><span class="nv">ymm0</span><span class="p">,</span><span class="nv">ymm0</span>
<span class="nf">add</span>    <span class="nb">eax</span><span class="p">,</span><span class="mh">0x0</span>
<span class="nf">add</span>    <span class="nb">eax</span><span class="p">,</span><span class="mh">0x0</span>
<span class="nf">add</span>    <span class="nb">eax</span><span class="p">,</span><span class="mh">0x0</span>
<span class="c1">; repeated 9 more times</span>
</code></pre></div></div>

<p>If only <code class="language-plaintext highlighter-rouge">vpor</code> is slowed down, each block of 4 instructions will take 4 cycles, limited by the <code class="language-plaintext highlighter-rouge">vpor</code> chain (the <code class="language-plaintext highlighter-rouge">add</code> chain is 3 cycles long). However, I actually measure ~12 cycles, indicating that we are instead limited by the <code class="language-plaintext highlighter-rouge">add</code> chain, each of which takes 4 cycles for a total of 12.</p>

<p><a name="throttle-anchor"></a>We can vary the number of <code class="language-plaintext highlighter-rouge">add</code> instructions (N) to see how long this effect persists. This table is the result:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">ADD instructions (N)</th>
      <th style="text-align: right">Cycles/ADD</th>
      <th style="text-align: right">Delta Cycles (slow)</th>
      <th style="text-align: right">Delta Cycles (fast)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">2.3</td>
      <td style="text-align: right">-0.2</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">0.8</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">1.1</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">3.9</td>
      <td style="text-align: right">1.1</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">4.3</td>
      <td style="text-align: right">0.7</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">3.4</td>
      <td style="text-align: right">1.1</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">4.2</td>
      <td style="text-align: right">0.9</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">3.9</td>
      <td style="text-align: right">0.8</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">4.3</td>
      <td style="text-align: right">1.1</td>
    </tr>
    <tr>
      <td style="text-align: right">20</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">30</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">4.0</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">40</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">4.3</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">50</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">3.9</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">60</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">4.4</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">70</td>
      <td style="text-align: right">4.2</td>
      <td style="text-align: right">4.5</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">80</td>
      <td style="text-align: right">4.1</td>
      <td style="text-align: right">3.4</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">90</td>
      <td style="text-align: right">3.6</td>
      <td style="text-align: right">-0.2</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">100</td>
      <td style="text-align: right">3.3</td>
      <td style="text-align: right">1.1</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">120</td>
      <td style="text-align: right">2.9</td>
      <td style="text-align: right">0.9</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">140</td>
      <td style="text-align: right">2.7</td>
      <td style="text-align: right">1.1</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">160</td>
      <td style="text-align: right">2.5</td>
      <td style="text-align: right">1.2</td>
      <td style="text-align: right">1.0</td>
    </tr>
    <tr>
      <td style="text-align: right">180</td>
      <td style="text-align: right">2.3</td>
      <td style="text-align: right">0.7</td>
      <td style="text-align: right">0.9</td>
    </tr>
    <tr>
      <td style="text-align: right">200</td>
      <td style="text-align: right">2.2</td>
      <td style="text-align: right">0.8</td>
      <td style="text-align: right">1.0</td>
    </tr>
  </tbody>
</table>

<p>The <strong>Cycles/ADD</strong> column shows the number of cycles taken per add instruction over the entire slow region (roughly the first 8-10 μs after the payload starts executing). The <strong>Delta Cycles (slow)</strong> shows how many cycles each additional <code class="language-plaintext highlighter-rouge">add</code> instruction took compared to the previous row: i.e., for row N = 30, it determines how much longer the 10 additional <code class="language-plaintext highlighter-rouge">add</code> instructions took compared to the row N = 20. The <strong>Delta Cycles (fast)</strong> column is the same thing, but applies to the samples after ~10 μs when the CPU is back up to full speed (that column shows the expected 1.0 cycles per additional add).</p>

<p>Here we clearly see that up to roughly 70 <code class="language-plaintext highlighter-rouge">add</code> instructions, interleaved with a single <code class="language-plaintext highlighter-rouge">vpor</code>, all the <code class="language-plaintext highlighter-rouge">add</code> instructions are taking 4 cycles, i.e., the CPU is throttled. Somewhere between 80 and 90 a transition happens: <em>additional</em> <code class="language-plaintext highlighter-rouge">add</code> instructions now take 1 cycle, but the overall time per <code class="language-plaintext highlighter-rouge">add</code> is (initially) close to 4. This shows that when <code class="language-plaintext highlighter-rouge">add</code> (and presumably any non-wide instruction) is far enough away from the closest wide <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instruction, they start executing at full speed. So the timings for the larger N values can be understood as a blend of a slow section of ~70-80 <code class="language-plaintext highlighter-rouge">add</code> instructions near the <code class="language-plaintext highlighter-rouge">vpor</code> which run at 1 per 4 cycles, and the remaining section where they run at full speed: 1 per cycle.</p>

<p>We can probably conclude the CPU is not just throttling frequency or “duty cycling”: in that case every instruction would be slowed down by the same factor, but instead the rule is more like “latency extended to the next multiple of 4 cycles”, e.g., a latency 3 instruction like <code class="language-plaintext highlighter-rouge">imul eax, eax, 0</code> ends up taking 4 cycles when the CPU is throttling. It is likely that the throttling happens at some part of the pipeline before execution, e.g., at issue or dispatch.</p>

<p>The transition to fast mode when the <code class="language-plaintext highlighter-rouge">vpor</code> instructions are spread sufficiently apart probably reflects the size of some structure such as the <abbr title="Queue that collects incoming instructions from the decoder, uop cache or microcode engine and delivers them to the renamer (RAT).">IDQ</abbr> (64 entries in Skylake) or scheduler (97 entries claimed<sup id="fnref:ratentries" role="doc-noteref"><a href="#fn:ratentries" class="footnote" rel="footnote">30</a></sup>). The core could track whether <em>any</em> wide instruction currently in that structure, and enforce the slow mode if so. The <code class="language-plaintext highlighter-rouge">vpor</code> instructions are close enough together, there is <em>always</em> at least one present, but once they are spaced out enough, you get periods of fast mode.</p>

<p><strong>Voltage Effects</strong></p>

<p>We can actually test the theory that this transition is associated with waiting for a change in power delivery configuration. Specifically, we can observe the CPU core voltage, using bits 47:32 of the <code class="language-plaintext highlighter-rouge">MSR_PERF_STATUS</code> MSR. Volume 4 of the Intel Software Development Manual let’s us on a secret: these bits expose the <em>core voltage<sup id="fnref:vcc" role="doc-noteref"><a href="#fn:vcc" class="footnote" rel="footnote">31</a></sup></em>:</p>

<p><img src="/assets/avxfreq1/msr_198h.png" alt="Intel SDM Volume 4: Table 2-20" /></p>

<p>Let’s zoom as usual on a transition point, in this case using a 256-bit (ymm) payload of 1000 dependent <code class="language-plaintext highlighter-rouge">vpor</code> instructions. This 256-bit payload means no frequency transition, only a dispatch throttling period associated with running 256-bit instructions for the first time in a while. We plot the time it takes to run an iteration of the payload<sup id="fnref:whyp" role="doc-noteref"><a href="#fn:whyp" class="footnote" rel="footnote">32</a></sup>, along with the measured voltage:</p>

<p><img src="/assets/avxfreq1/fig-volts256-1.svg" alt="Voltage Changes" /></p>

<p>The length of the throttling period is around 10 μs as usual, as shown by the period where the payload takes ~4,000 cycles (the usual 4x throttling), and the voltage is unchanged from the pre-transition period (at about 0.951 V) during the throttling period. At the moment the throttling stops, the voltage jumps to about 0.957, a change of about 6 mV. This happens at 2.6 GHz, the nominal non-turbo speed of my i7-6700HQ. At 3.5 GHz, the transition is from 1.167 to 1.182, so both the absolute voltages and the difference (about 15 mV) are larger, consistent the basic idea that higher frequencies need higher voltages.</p>

<p>So one theory is that this type of transition represents the period between when the CPU has requested a higher voltage (because wider 256-bit instructions imply a larger worst-case current delta event, hence a worst-case voltage drop) and when the higher voltage is delivered. While the core waits for the change to take effect, throttling is in effect in order to reduce the worst-case drop: without throttling<sup id="fnref:cthrottle" role="doc-noteref"><a href="#fn:cthrottle" class="footnote" rel="footnote">33</a></sup> there is no guarantee that a burst of wide <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instructions won’t drop the voltage below the minimum voltage required for safe operation at this frequency.</p>

<h4 id="attenuation">Attenuation</h4>

<p>We might check if there is any <em>attenuation</em> of either type of transition. By <em>attenuation</em> I mean that if a core is transitioning between frequencies too frequently, the power management algorithm may decide to simply keep the core at the lower frequency, which can provide more overall performance when considering the halted periods needed in each transition. This is exactly what happens for active core count<sup id="fnref:acc" role="doc-noteref"><a href="#fn:acc" class="footnote" rel="footnote">34</a></sup> transitions: too many transitions in a short period of time and the CPU will just decide to run at the lower frequency rather than incurring the halts need to transition between e.g. the 1-core and 2-core turbos<sup id="fnref:hiddenbo" role="doc-noteref"><a href="#fn:hiddenbo" class="footnote" rel="footnote">35</a></sup>.</p>

<p>We check this by setting a duty period which is just above the observed recovery time from 2.8 to 3.2 GHz, to see if we still see transitions. Here’s a duty cycle of 760 μs, about 10 μs more than the observed recovery period for this test<sup id="fnref:recovery" role="doc-noteref"><a href="#fn:recovery" class="footnote" rel="footnote">36</a></sup>:</p>

<p><img src="/assets/avxfreq1/fig-vporvz512-ipc-p760.svg" alt="760 μs period closeup" /></p>

<p>I’m not going to color the regions here, as by now I think you are probably (over?) familiar with them. The key points are:</p>

<ul>
  <li>The payload starts executing at 7600 μs, which is <em>before</em> the upwards frequency transition, we are still executing at 2.8 GHz - so initially the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> is high, 1 per cycle.</li>
  <li>Despite the fact that we are already executing again 512-bit instructions, the frequency adjusts upwards a few μs later. Most likely what happened is that the power logic already evaluated earlier (say at ~7558 μs, just before the payload started) that an upwards transition should occur, but as we’ve seen the response is generally delayed by 8 to 10 μs so it occurs after the payload has already started executing.</li>
  <li>Of course, as soon as the transition occurs, the core is no longer in a suitable state for full-speed wide <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> execution, so <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> drops to ~0.25.</li>
  <li>Another transition back to low frequency occurs ~10 μs later and then full speed execution can resume.</li>
</ul>

<p>So there is no attenuation, but attenuation isn’t really needed: the long (~650 μs) cooldown period between the last wide instruction and subsequent frequency boost means that the damage from halt periods are fairly limited: this is unlike the active core count scenario where the CPU has no control over the transition frequency (rather it is driven by interrupts and changes in runnable processes and threads). Here, we have the worst case scenario of transitions packed as closely as possible, but we lose only ~20 μs (for 2 transitions) out of 760 μs, less than a 3% impact. The impact of running at the lower frequency is much higher: 2.8 vs 3.2 GHz: a 12.5% impact in the case that the lowered frequency was not useful (i.e., because the wide <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> payload represents a vanishingly small part of the total work).</p>

<h2 id="what-was-left-out">What Was Left Out</h2>

<p>There’s lots we’ve left out. We haven’t even touched:</p>

<ul>
  <li>Checking whether xmm registers also cause a voltage-only transition, if they haven’t been used for a while. We didn’t find <a href="#xmmeffect">any effect</a>, but it also certain that some 128-bit instructions appear in the measurement loop which would hide the effect.</li>
  <li>Checking whether the voltage-only transition implied by 256-bit instructions are disjoint from those for 512-bit. That is, if you execute a 256-bit instruction after a while without any, you get a voltage-only transition (confirmed above). If you then execute a 512-bit instruction, before the relaxation period expires, do you get a second throttling period prior to the frequency transition? I believe so but I haven’t checked it.</li>
  <li>Any type of investigation of “heavy” 256-bit or 512-bit instructions. These require a license one level (numerically) higher than light instructions, and knowing if any of the key timings change would be interesting<sup id="fnref:heavy" role="doc-noteref"><a href="#fn:heavy" class="footnote" rel="footnote">37</a></sup>.</li>
  <li>Almost no investigation was made how any of these timings (and the magnitude of voltage changes) vary with frequency. For example, if we are already running at a lower frequency, frequency transitions are presumably not needed, and voltage-only transitions may be shorter.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>For the benefit of anyone who just skipped to the bottom, or whose eyes glazed over at some point, here’s a summary of the key findings:</p>

<ul>
  <li>After a period of about 680 μs not using the <em>AVX upper bits</em>  (255:128) or <em>AVX-512 upper bits</em> (511:256) the processor enters a mode where using those bits again requires at least a voltage transition, and sometimes a frequency transition.</li>
  <li>The processor continues executing instructions during a voltage transition, but at a greatly reduced speed: 1/4th the usual instruction dispatch rate. However, this throttling is fine-grained: it only applies when wide instructions are <em>in flight</em> (<a href="#throttle-anchor">details</a>).</li>
  <li>Voltage transitions end when the voltage reaches the desired level, this depends on the magnitude of the transition but 8 to 20 μs is common on the hardware I tested.</li>
  <li>In some cases a frequency transitions is also required, e.g., because the involved instruction requires a higher power license. These transitions seem to <em>first</em> incur a throttling period similar to a voltage-only transition, and then a halted period of 8 to 10 μs while the frequency changes.</li>
  <li>A key motivator for this post was to give concrete, qualitative guidance on how to write code that is as fast as possible given this behavior. It got bumped to part 2.</li>
</ul>

<p>We also summarize the key timings in this beautifully rendered table:</p>

<table>
  <thead>
    <tr>
      <th>What</th>
      <th>Time</th>
      <th>Description</th>
      <th>Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Voltage Transition</td>
      <td>~8 to 20 μs</td>
      <td>Time required for a voltage transition, depends on frequency</td>
      <td><sup id="fnref:t1deets" role="doc-noteref"><a href="#fn:t1deets" class="footnote" rel="footnote">38</a></sup></td>
    </tr>
    <tr>
      <td>Frequency Transition</td>
      <td>~10 μs</td>
      <td>Time required for the halted part of a frequency transition</td>
      <td><sup id="fnref:fdeets" role="doc-noteref"><a href="#fn:fdeets" class="footnote" rel="footnote">39</a></sup></td>
    </tr>
    <tr>
      <td>Relaxation Period</td>
      <td>~680 μs</td>
      <td>Time required to go back to a lower power license, measured from the last instruction requiring the higher license</td>
      <td><sup id="fnref:lldeets" role="doc-noteref"><a href="#fn:lldeets" class="footnote" rel="footnote">40</a></sup></td>
    </tr>
  </tbody>
</table>

<h2 id="thanks">Thanks</h2>

<p><a href="https://lemire.me">Daniel Lemire</a> who provided access to the AVX-512 system I used for testing.</p>

<p><a href="https://twitter.com/thekanter">David Kanter</a> of <a href="http://www.realworldtech.com">RWT</a> for a fruitful discussion on power and voltage management in modern chips.</p>

<p>RWT forum members anon³, Ray, Etienne, Ricardo B, Foyle and Tim McCaffrey who provided feedback on this post and helped me understand the VR landscape for recent Intel chips.</p>

<p>Alexander Monakov, Kharzette and Justin Lebar for finding typos.</p>

<p><a href="https://twitter.com/JeffSmith888">Jeff Smith</a> for teaching me about spread spectrum clocking.</p>

<h2 id="discuss">Discuss</h2>

<p>Discussion on <a href="https://news.ycombinator.com/item?id=22077974">Hacker News</a>, <a href="https://twitter.com/trav_downs/status/1218238653354344449">Twitter</a> and <a href="https://lobste.rs/s/qaqmyo/gathering_intel_on_intel_avx_512">lobste.rs</a>.</p>

<p>Direct feedback also welcomed by <a href="mailto:travis.downs@gmail.com">email</a> or as <a href="https://github.com/travisdowns/travisdowns.github.io/issues">a GitHub issue</a>.</p>

<p class="info">If you liked this post, check out the <a href="/">homepage</a> for others you might enjoy.</p>

<hr />
<hr />
<p><br /></p>

<p><a name="footnotes"></a></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:first" role="doc-endnote">
      <p>… and also <em>non frequency</em> related performance events, which I mention only in a footnote not to spoil the fun for non-footnote type people and also to pad by footnote count. That’s why I call this <em>Performance</em> Transitions, instead of Frequency Transitions. <a href="#fnref:first" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dmore" role="doc-endnote">
      <p>Note that Daniel has <a href="https://lemire.me/blog/2018/08/25/avx-512-throttling-heavy-instructions-are-maybe-not-so-dangerous/">written</a> <a href="https://lemire.me/blog/2018/08/15/the-dangers-of-avx-512-throttling-a-3-impact/">much more</a> <a href="https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/">than</a> <a href="https://lemire.me/blog/2018/09/04/per-core-frequency-scaling-and-avx-512-an-experiment/">just that</a> one. <a href="#fnref:dmore" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:intro" role="doc-endnote">
      <p>This was going to be the actual post I was trying to write when I went off on <a href="/blog/2019/11/19/toupper.html">this tangent about clang-format</a>. In fact I <em>was</em> writing that post, when I went off this current tangent, but then a footnote turned into several paragraphs, then got its own section and ultimately graduated into a whole post: the one you are reading. So consider this background reading for the “interesting” post still to come, although honestly the stuff here is probably more generally useful than the next part. <a href="#fnref:intro" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:widthmatters" role="doc-endnote">
      <p>We should be clear here: when I say AVX-512 instruction in this context, I mean specifically a <em>512-bit wide instruction</em> (which currently only exist in AVX-512). The distinction is that AVX-512 includes 128-bit and 256-bit versions of almost every instruction it introduces, yet these narrower instructions behave just like 128-bit SSE* and 256-bit AVX* instructions in terms of performance transitions. So, for example, <code class="language-plaintext highlighter-rouge">vpermw</code> is unabiguously <em>AVX-512</em> instruction: it only exists in AVX-512BW, but only the version that takes <code class="language-plaintext highlighter-rouge">zmm</code> registers causes “AVX-512 like” performance transitions: the versions that take <code class="language-plaintext highlighter-rouge">ymm</code> or <code class="language-plaintext highlighter-rouge">xmm</code> registers act as any other integer 256-bit AVX2 or 128-bit SSE instruction. <a href="#fnref:widthmatters" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:speriod" role="doc-endnote">
      <p>In fact, we generally want the sample period to be as small as possible, to give the best resolution and insight into short-lived events. We can’t make it <em>too</em> short though as the samples themselves have a minimum time to capture, and very short samples tend to be noisy due to non-atomicity, quantization effects, etc. <a href="#fnref:speriod" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:f2104" role="doc-endnote">
      <p>The CPU is an <a href="https://en.wikichip.org/wiki/intel/xeon_w/w-2104">Intel W-2104</a>, a Xeon-W chip based on the <abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr> <abbr title="Microarchitecture: a specific implementation of an ISA, e.g., &quot;Haswell microarchitecture&quot;.">uarch</abbr>. It has no turbo and an on-the-box speed of 3200 MHz, but accurate tools will probably report it running at 3192 MHz due to <em><a href="https://twitter.com/JeffSmith888/status/1211821823035351043">spread spectrum clocking</a></em> (SSC). In fact, we can see the typical 0.5% spread spectrum triangle wave on almost any of the plots in this post, at the right zoom level, <a href="/assets/avxfreq1/fig-ssc.svg">like this one</a>. This occurs because we measure time (the x-axis) based on <code class="language-plaintext highlighter-rouge">rdtsc</code> which is based off of a different clock not subject to SSC, while the unhalted cycles counter counts CPU cycles, which are based off of the 100 MHz BLCK which is subject to SSC. <a href="#fnref:f2104" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:avxt" role="doc-endnote">
      <p><a href="https://www.realworldtech.com/forum/?threadid=179358&amp;curpostid=179652">Measured</a> with <a href="https://github.com/travisdowns/avx-turbo">avx-turbo</a>. <a href="#fnref:avxt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:vz" role="doc-endnote">
      <p>This is the part where I just gloss over what that <code class="language-plaintext highlighter-rouge">vzeroupper</code> is doing there. It’s there due to <em>implicit widening</em>. That’s a new term I just invented and it’s the first and last time I’m mentioning it this post, because it really deserves an entire post of its own. The very short version is that any time an <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instruction writes to an N-bit register (N in {256, 512}), all subsequent <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instructions are <em>implicitly</em> N bits wide, regardless of their actual width, for the purposes of determining turbo licenses and other transitions discussed here. This sounds a bit like the <a href="https://stackoverflow.com/q/41303780/149138">mixed-VEX penalties thing</a>, but it is very different. This a mini-bombshell hidden in a footnote, so if you want to scoop me you can, but I’m not coming to your birthday party. <a href="#fnref:vz" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:pperiod" role="doc-endnote">
      <p>I.e., the <em>payload period</em> is zero, but the structure of the test ensures the function is called once, at the start of the payload period, no matter how small the payload period is. <a href="#fnref:pperiod" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:plotnotes" role="doc-endnote">
      <p>All of the plots in this post are SVG images, meaning they can be zoomed arbitrarily: so if you want to zoom in on any region feel free (the browser limits the zoom amount, but just save as a file and open it with any SVG viewer). <a href="#fnref:plotnotes" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:falk" role="doc-endnote">
      <p>These are basically a poor man’s version of the <em>Falk Diagrams</em> that Brandon Falk describes in <a href="https://gamozolabs.github.io/metrology/2019/08/19/sushi_roll.html">this post</a> among others. Poor in the sense that they have ~3000 cycle resolution instead of 1 cycle resolution, and because the measurement code has to be integrated directly into the system under test. Basically they are nothing like <em>Falk Diagrams</em> except that they have time on the x-axis and some performance counter event on the y-axis, but they are good enough for our purposes. <a href="#fnref:falk" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:threerun" role="doc-endnote">
      <p>Specifically, all three runs are identical and I show them just to give a rough impression of which effects are reproducible and which are outliers. The second and third runs have the suffixes <code class="language-plaintext highlighter-rouge">_1</code> and <code class="language-plaintext highlighter-rouge">_2</code>, respectively, in the legends. <a href="#fnref:threerun" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:outlie" role="doc-endnote">
      <p>These outliers usually occur when an interrupt occurs during the measurement. Originally I used a 3 μs sample time, and there were almost no visible outliers with that period, but the 1 μs value I settled on is much better in most respects other than outliers. The main problem is when an interrupt takes more than the sample time of ~1 μs: this causes one or more samples to be very short, because a long sample will cause subsequent ones to be short (maybe <em>very</em> short) until we catch up to the fixed sample schedule, and very short samples are subject to much more noise because the absolute metric values are much smaller but the error sources usually have fixed absolute error. Another source of outliers is when an interrupt <em>splits the stamp</em>: the <em>stamp</em> is the series of metrics we calculate at the sample point. These various metrics aren’t sampled atomically: if an interrupt occurs in the middle of the sampling, some metrics will reflect a much shorter time period (before the interrupt) and some a longer one (after). This effect tends to cause bidirectional spike outliers: where an upspike and downspike occur in consecutive samples. I try to avoid this by retrying the stamp if I think I’ve detected an interrupt during measurement (up to a retry limit). We could avoid all this nonsense by running the benchmark itself in the kernel, where we can disable interrupts (although some SMIs might still sneak in). Maybe next time! <a href="#fnref:outlie" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:peachpuff" role="doc-endnote">
      <p>More precisely, the color is <a href="https://encycolorpedia.com/ffdab9">peachpuff</a>. <a href="#fnref:peachpuff" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:resnote" role="doc-endnote">
      <p>Note that the resolution of the sampling is 1 μs, so when we say things like <em>9 μs</em> and <em>11 μs</em> it could be off by up to 1 μs. This 1 μs “error” isn’t exactly randomly distributed, because the sampling interval is “exact” and in phase with the payload execution. So the samples are always at 1.0, 2.0, 3.0, … μs after the payload executes (plus or minus small variation on the order of 10 nanoseconds), so if the true time until halt is anywhere between 9.00 and 9.99 μs, we will always read 9 μs (because time shown for the sample is the <em>end</em> of the sample and covers the preceding 1 us). For the halt time, the scenario is reversed: the start of the interval is uncertain, but the end should be exact modulo the delay in taking the sample. <a href="#fnref:resnote" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:halted" role="doc-endnote">
      <p>Generally you can detect halted periods when <code class="language-plaintext highlighter-rouge">rdtsc</code> jumps forward, but performance counters like “non-halted cycles” do not, and there are not indications of a larger interruption such as a context switch. You can find a similar case of halted periods <a href="https://stackoverflow.com/q/45472147/149138">in this question</a> which was also caused by frequency transitions (in this case, to obey the different active core count turbo ratios). <a href="#fnref:halted" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:thistle" role="doc-endnote">
      <p>More precisely, the color is <a href="https://encycolorpedia.com/d8bfd8">thistle</a>. <a href="#fnref:thistle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:confirm" role="doc-endnote">
      <p>In particular, I have confirmed that these three samples all occur after the payload has executed and retired using the <em>period</em> column in the output, which indicates clearly which sames are before or after a given execution of the payload. The payload itself is followed by an <code class="language-plaintext highlighter-rouge">lfence</code> to ensure it has retired before taking further samples (and in any case the number of instructions per sample is too large be accommodated by the <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">OoO</abbr> window). <a href="#fnref:confirm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whynot" role="doc-endnote">
      <p>Those who are still awake at this point might be wondering why introduce this new variant of the test now: why not just execute the payload instructions during the wait period in the original tests too? One reason is that by hot spinning on <code class="language-plaintext highlighter-rouge">rdtsc</code> we get somewhat more consistent results when we care only about measuring the frequency in that we almost always sample at exactly the specified period (plus or minus the <code class="language-plaintext highlighter-rouge">rdtsc</code> latency, more or less). When we execute the longish payload function during the wait, the sample point diverges a bit more from the ideal, and the number of spins per sample suffers more quantization effects (i.e., the pattern of spin counts might be 3,3,3,2,3,3,3,2… rather than 450,450,451,450…), which can sometimes lead to a slight sawtooth effect in the samples. <a href="#fnref:whynot" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ideal" role="doc-endnote">
      <p>In practice, we don’t exactly reach this ideal: we execute the payload function 2 or 3 times, for 2,000 or 3,000 <code class="language-plaintext highlighter-rouge">vpor</code> instructions, but there are about 600 additional instructions of overhead associated with taking a sample, so the overhead instructions are a significant portion. Probably 600 instructions is too many, I haven’t optimized that and it could likely be lowered significantly. However, we can also improve the ratio simply by decreasing the sampling resolution (i.e., increasing the sampling time). I selected 1 μs as a tradeoff between these competing factors. Note: Since I wrote this footnote I optimized several things in the sampling loop, so measured IPCs are now very close to their theoretical values, but I guess this footnote still has value?. <a href="#fnref:ideal" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:give" role="doc-endnote">
      <p>The main uncertainty in the timing of the function itself concerns the boundary conditions: if we run this function 10 times <em>without</em> touching <code class="language-plaintext highlighter-rouge">zmm0</code> in between, the dependency on <code class="language-plaintext highlighter-rouge">zmm0</code> will be carried between functions and the total time will be very close to 10 x 1000 = 10,000 cycles. However, if some compiler generated code in between calls to the payload function happens to write to <code class="language-plaintext highlighter-rouge">zmm0</code>, breaking the dependency, the individual chains for each function no longer depend on each other, so some overlap is possible. The amount of this overlap is limited by the size of the RS, so the effect won’t be <em>huge</em> but it could be noticeable (with say 100 payload instructions rather than 1,000 it could be very significant). We basically sidestep this whole issue by putting an <code class="language-plaintext highlighter-rouge">lfence</code> between each call to the payload function, which forms an <em>execution barrier</em> between calls. <a href="#fnref:give" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:catchup" role="doc-endnote">

      <p>The way the sampling works in this test could be described <em>locked interval without skipping</em>. Here, <em>locked interval</em> means we calculate the next target sample time based on the previous <em>target</em> sample time (rather than the <em>actual</em> sample time), plus the period. So if we are sampling at 10 μs, the target sample times will always be 10 μs, 20 μs, etc. In particular, the series of target sample time doesn’t depend on what happens during the test, e.g., it doesn’t depend on the actual sample time: even if we actually end up sampling at time = 12 μs rather than 10, we target time = 20 μs as the next sample, not 12 + 10 = 22 μs. This raises the question about what happens when some delay (e.g., an interrupt, a frequency transition) causes us to miss more than 1 entire sample period.</p>

      <p>E.g., with 10 μs resolution we just sampled at 90 μs, so the next sample target is 100 μs, but a delay causes the next sample to be taken at 125 μs. We are now behind! The next sample should occur at 110 μs, but of course that is in the past. The current test design still takes all the required samples, as quickly as possible (but with a minimum of one payload call if we are in the <em>extra payload period</em>) – that’s the <em>no skipping</em> part. In the current example, it means we would take subsequent samples quickly until we are caught up, at say 125 μs (target 110 μs), 126 μs (target 120 μs), 130 μs (target 130 μs), with that last sample being “on target”.</p>

      <p>These samples occur quickly: very quickly in the case of normal samples which take less than 0.2 μs each, or more slowly in the case of the <em>extra payload</em> region, where the payload call bumps that to about 0.5 μs. So that’s what’s happening in the green region: we just had a frequency transition halt of ~10 μs, so we are ~10 samples beyind and so the following samples are taken more quickly (as you can see because the data point markers are spaced more closely), with only a single payload call each (versus 2 or 3 usually). This changes the ratio of payload instructions to overhead instructions, which tends to bump the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> a bit (for the same reason the caught-up blue region almost shows <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> &gt; 1). This effect can be reduced or eliminated by increasing the sampling period, since that reduces the number of catchup samples.</p>

      <p>Incidentally, this also explains the oscillating pattern you see in the blue region: the ideal number of payload calls to get a 1 μs sample rate is ~2.5, so the sampling strategy tends to alternative between two and three calls: more calls means <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> closer to 1: so the peaks of the oscillating are two calls and the valleys, three. <a href="#fnref:catchup" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tput" role="doc-endnote">
      <p>I’m not going to fully analyze the throughput case, but the thorough among us can find the chart is <a href="/assets/avxfreq1/fig-ipc-zoomed-zmm-tput.svg">here</a>. Note that the overhead here cuts the other way: pushing the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> below the expected value of 2.0 (there are 2 512-bit vector units capable of executing <code class="language-plaintext highlighter-rouge">vpord</code>) because here the throughput limited instructions compete for ports with the overhead instructions. <a href="#fnref:tput" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:orisit" role="doc-endnote">
      <p>Of course, another possibility is that something in the rest of the test loop uses <code class="language-plaintext highlighter-rouge">xmm</code> registers so they remain “hot”: they are “baseline” for x86-64 after all, so the compiler is free to use them without any special flags. We could test this theory with a more compact test loop audited to be free of any <code class="language-plaintext highlighter-rouge">xmm</code> use… but I’m not going to bother. I’m pretty sure these guys are powered up all the time as their use is pervasive in most x86-64 code. <a href="#fnref:orisit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:upper" role="doc-endnote">
      <p>Specifically, the the part handing the second (bits 128:255) for the <code class="language-plaintext highlighter-rouge">ymm</code> case, and the upper 3 lanes (bits 128:511) in the <code class="language-plaintext highlighter-rouge">zmm</code> case. <a href="#fnref:upper" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:amd" role="doc-endnote">
      <p>This isn’t far fetched - that’s exactly how AMD Zen always executes 256-bit AVX instructions with its 128 bit units, and similar to how SVB and <abbr title="Intel's Ivy Bridge architecture, aka 3rd Generation Intel Core i3,i5,i7">IVB</abbr> did the same for 256-bit load and store instructions. So using narrower vector units to implement wider instructions is definitely a thing. <a href="#fnref:amd" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ewise" role="doc-endnote">
      <p>These are usually called <em>cross lane</em> instructions (where a lane is 128 bits on Intel). For example, <code class="language-plaintext highlighter-rouge">vpor</code> is <em>not</em> like this: it is an <em>element-wise</em> operation where each output element (down to each bit, in this case) depends only on the element in the same position in the input vectors. On other the hand, <code class="language-plaintext highlighter-rouge">vpermd</code> is: each 32-bit element in the output can come from <em>any</em> position in the input vector (but it still behaves as element-wise wrt the mask register<sup id="fnref:bonus" role="doc-noteref"><a href="#fn:bonus" class="footnote" rel="footnote">41</a></sup>). <a href="#fnref:ewise" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lt" role="doc-endnote">
      <p>The notation 4L4T means: “4 cycles of latency and 4 cycles of inverse throughput”. That is, an given instance of this instruction takes 4 cycles to finish (latency), and a new instruction can start every 4 cycles (inverse throughput, hence the throughput is 0.25). When the CPU is running normally, most single-<abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> instructions have latency of 1 (in lane), 3 (most cross lane integer or shuffle ops) or 4 (most FP arithmetic). <a href="#fnref:lt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:alu" role="doc-endnote">
      <p>I say <em>ALU instructions</em> here, but I strongly suspect it might be all instructions: you can certainly test that at home using the same test (the <code class="language-plaintext highlighter-rouge">vporxymm250_*</code> group of tests) but with other types of instructions such as loads replacing the <code class="language-plaintext highlighter-rouge">add</code>. I didn’t really test <em>all</em> ALU instructions either: just a few - but it is fair to assume that if <code class="language-plaintext highlighter-rouge">add</code> is slowed down, it is something generic, probably affecting at least all ALU stuff, not something instruction specific. <a href="#fnref:alu" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ratentries" role="doc-endnote">
      <p>Documentation claims 97 entries, but my testing seems to indicate they are not unified in Skylake: apparently only 64 can be used for ALU ops, and 33 for memory ops. <a href="#fnref:ratentries" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:vcc" role="doc-endnote">
      <p>Try as I might, I can’t determine if this refers to <em>measured</em> core voltage, i.e., true voltage as determine at some sensor within the core, or <em>demanded</em> voltage, i.e., the voltage the processor wants right now based on the various power-relevant parameters, sometimes called the VID. In any case, we expect those values to track each other fairly closely, perhaps with some offset and since we are looking for voltage <em>changes</em> either one works. That said, I am very interested if you know the answer to this question. <a href="#fnref:vcc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whyp" role="doc-endnote">
      <p>This payload time series is meant to show the exact same thing as the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> series in earlier charts: we just want an indication of when the Type 1 throttling starts and stops. I used <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> initially because it was easy (I didn’t have to instrument the payload section specially) - but it doesn’t work when reading volts because that measurement involves a ton of additional instructions and a user-kernel transition, so it throws the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> off completely. So I went ahead and instrumented the payload section directly, so we can still see the throttling, but no way I want to go back and change the other plots that use <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr>. <a href="#fnref:whyp" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:cthrottle" role="doc-endnote">
      <p>The throttling here is quite conservative I think: this is only a very small voltage change (less than 1%), so it is hard to believe that 4x throttling is <em>necessary.</em> It seems likely, for example, that cutting the dispatch rate in half would be enough to compensate for the missing 6 mV – but it is easy to imagine that just having a big conservative throttling number for all these voltage-too-low throttling scenarios is easy and safe, and these periods don’t occur often enough for it to really matter. <a href="#fnref:cthrottle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:acc" role="doc-endnote">
      <p>Essentially all modern Intel CPUs have varying maximum turbo ratios depending on the number of active (not halted or in a sleep state). E.g., my Skylake CPU can run at at a max speed of 3.5, 3.3, 3.2 or 3.1 GHz if 1, 2, 3 or 4 cores are active, respectively. If only one core is currently running, at 3.5 GHz, and another core becomes active (e.g., because the scheduler found something to run) – the first core has to immediately transition down to 3.3 GHz and as we’ve seen above, it takes an ~8-10 μs halt to do so. When the other core stops running, it can return to 3.3 GHz. If cores are flipping between inactive and active quickly enough, those halts add up and cut into your effective frequency. At some point, you might get less work done by trying to run at max turbo, versus just running at 3.3 GHz all the time since in this case no halts need to be taken when the second core starts. Further explored <a href="https://stackoverflow.com/a/45592838/149138">over here</a>. <a href="#fnref:acc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hiddenbo" role="doc-endnote">
      <p>Avoiding these transitions are a hidden bonus of making the 1 and 2-core turbos the same, and more generally “grouping” the turbo ratios in a more coarse grained way across core counts: you don’t need any transition when the “from” and “to” core counts have the same max turbo ratio. <a href="#fnref:hiddenbo" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:recovery" role="doc-endnote">
      <p>Earlier we mentioned a low frequency duration of 650 μs, but that was the test that ran only a single payload instruction. The recovery period is measured from the <em>last</em> wide instruction and in this test (that shows the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr>) we execute 100 μs of payload, so the recovery time will be 100 μs + 650 μs = ~750 μs. <a href="#fnref:recovery" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:heavy" role="doc-endnote">
      <p>Unlike the transitions discussed here, transitions related to heavy instructions are <em>soft</em> transitions: they do not occur unconditionally after a single instruction of the relevant type is executed, but rather only after some <em>density</em> threshold is reached for those instructions. Exploring this threshold would be interesting. There is another effect mentioned in the Intel optimization manual: heavy instructions may cause a license transition even in cases where it wouldn’t normally occur, when light instructions of one license level are mixed with <em>half-width</em> heavy instructions. That is, 128-bit heavy instructions can use the fastest L0 license, as can 256-bit light instructions. However, apparently, mixing these can cause a request for the L1 license. Similarly for 256-bit heavy instructions and 512-bit light instructions, where a downgrade from L1 to L2 could occur. <a href="#fnref:heavy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:t1deets" role="doc-endnote">
      <p>I give a range of 8 to 20 μs because that’s what I measured in my testing, but the highest frequency I measured for a voltage-only transition at was 3.5 GHz, with a 15 mV delta. It is entirely possible that at higher frequencies and voltages, times are much longer. It could also depend on the hardware, e.g., the presence or absence of a <abbr title="Fully Integrated Voltage Regulator">FIVR</abbr>. <a href="#fnref:t1deets" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fdeets" role="doc-endnote">
      <p>This transition time seems to be required by <em>any</em> frequency transition, whether up or down in frequency and regardless of the cause. This includes transition causes not tested here: for example, when the max turbo ratio changes because the active core count changes, or when the ideal frequency changes for any other reason. <a href="#fnref:fdeets" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lldeets" role="doc-endnote">
      <p>This same relaxation period appears to apply to both of the transitions types discussed in this post, e.g., both voltage and frequency. The relaxation timer is reset any type an instruction that needs the current license is executed. In this case, the 680 μs period is measured from the instruction that causes the transition (which is also the last relevant instruction since only a single payload instruction is used), until the time that the CPU resumes executing again at the higher frequency. This period includes one dispatch throttling period and two frequency transitions, so only about 650 μs of the 680 μs is spent executing instructions at full speed. <a href="#fnref:lldeets" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bonus" role="doc-endnote">
      <p>Bonus question: are there any single-<abbr title="Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.">uop</abbr> AVX/AVX-512 instructions which are <abbr title="A SIMD operation whose lane-wise output depends on elements from lanes other than the same lane in the inputs (lanes are 128 bits in x86).">cross-lane</abbr> in both of their inputs? There are 3-input shuffles, like <code class="language-plaintext highlighter-rouge">VPERMI2B</code> which have 2 of their inputs 3 <abbr title="A SIMD operation whose lane-wise output depends on elements from lanes other than the same lane in the inputs (lanes are 128 bits in x86).">cross-lane</abbr> (the two input tables), but they need 2 uops. <a href="#fnref:bonus" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- travis override -->
  
  <!-- end override -->

  <a class="u-url" href="/misc/avxfreq-orig.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Travis Downs</li>
          <li><a class="u-email" href="mailto:travis.downs@gmail.com">travis.downs@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>A blog about low-level software and hardware performance.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/travisdowns" title="travisdowns"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/trav_downs" title="trav_downs"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
