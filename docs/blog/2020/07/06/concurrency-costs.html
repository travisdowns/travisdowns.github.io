<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="color-scheme" content="light dark">

<link rel="stylesheet" href="/assets/css/light.css">
<link rel="stylesheet" href="/assets/css/dark.css" media="(prefers-color-scheme: dark)">
<script>
  var DARKMODE = (function() {
    const i = {
    PROP: 'force-color',
    OID: 'override-style',
    BANNER: 'true',
    getOverride: function () {
      try {
        return localStorage.getItem(i.PROP);
      } catch (e) {
        return null;
      }
    },
    get: function () {
      try {
        var o = i.getOverride();
        if (o === 'dark' || (!o && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
          return 'dark';
        }
      } catch (e) {}
      return 'light';
    },
    gaTheme: function () {
      var o = i.getOverride();
      return o ? o + ' force' : i.get() + ' default';
    },
    makeLink: function (c) {
      e = document.createElement('link');
      e.id = i.OID;
      e.rel = 'stylesheet';
      e.href = (c === 'dark' ? '/assets/css/dark.css' : '/assets/css/light.css');
      return e;
    }
  }
  return i;
}());

if (DARKMODE.getOverride()) {
  var dm_color = DARKMODE.get();
  document.write(DARKMODE.makeLink(dm_color).outerHTML +
    '\n<meta name="theme-color" content="#' + (dm_color === 'dark' ? '181818' : 'fdfdfd') + '">');
}
</script>
<meta name="theme-color" content="#fdfdfd" media="not all and (prefers-color-scheme: dark)">
<meta name="theme-color" content="#181818" media="(prefers-color-scheme: dark)">
<script defer src="/assets/dark-mode.js"></script>

<!-- Begin Jekyll SEO tag v2.7.1p -->
<title>A Concurrency Cost Hierarchy | Performance Matters</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="A Concurrency Cost Hierarchy" />
<meta name="author" content="Travis Downs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Concurrent operations can be grouped relatively neatly into categories based on their cost" />
<meta property="og:description" content="Concurrent operations can be grouped relatively neatly into categories based on their cost" />
<link rel="canonical" href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html" />
<meta property="og:url" content="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html" />
<meta property="og:site_name" content="Performance Matters" />
<meta property="og:image" content="https://travisdowns.github.io/assets/concurrency-costs/avatar.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-06T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://travisdowns.github.io/assets/concurrency-costs/avatar.jpg" />
<meta property="twitter:title" content="A Concurrency Cost Hierarchy" />
<meta name="twitter:site" content="@trav_downs" />
<meta name="twitter:creator" content="@trav_downs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Travis Downs"},"dateModified":"2020-07-06T00:00:00+00:00","datePublished":"2020-07-06T00:00:00+00:00","description":"Concurrent operations can be grouped relatively neatly into categories based on their cost","headline":"A Concurrency Cost Hierarchy","image":"https://travisdowns.github.io/assets/concurrency-costs/avatar.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://travisdowns.github.io/assets/rabbit3.png"},"name":"Travis Downs"},"url":"https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://travisdowns.github.io/feed.xml" title="Performance Matters" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-136594956-1"></script>
<script>
  window['ga-disable-UA-136594956-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('set', { 'custom_map': { 'dimension1': 'theme' } });
  gtag('config', 'UA-136594956-1', { 'theme' : DARKMODE.gaTheme(), 'cookie_flags': 'SameSite=None;Secure' });
</script>

</head>
<body>
  <header id="dm-header" class="dm-header hidden">
    <div class="dm-bar">
      <div class="wrapper">
        <label class="dm-checkbox"><span>Enable Dark Mode: </span><input type="checkbox" id="dm-select"/></label>
        <span onclick="DARKMODE.closeBar()" class='dm-close'>&times;</span>
      </div>
    </div>
    <div class="dm-spacer"></div>
  </header>
<header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Performance Matters</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/settings">Settings</a></div>
      </nav></div>
</header>
<main class="page-content invert-rotate-img" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A Concurrency Cost Hierarchy</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-07-06T00:00:00+00:00" itemprop="datePublished">
        Jul 6, 2020
      </time><span>
          •
          <span class="tag-link"><a href="/tags/performance.html">performance</a></span><span class="tag-link"><a href="/tags/c++.html">c++</a></span><span class="tag-link"><a href="/tags/concurrency.html">concurrency</a></span>
        </span></p>
    <!-- end override -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    
<!-- boilerplate 
page.assets: /assets/concurrency-costs
assetpath: /assets/concurrency-costs
tablepath: /misc/tables/concurrency-costs
-->

<h2 id="introduction">Introduction</h2>

<p>Concurrency is hard to get <em>correct</em>, at least for those of us unlucky enough to be writing in languages which expose directly the guts of concurrent hardware: threads and shared memory. Getting concurrency correct <em>and</em> fast is hard, too. Your knowledge about single-threaded optimization often won’t help you: at a micro (instruction) level we can’t simply apply the usual rules of μops, dependency chains, throughput limits, and so on. The rules are different.</p>

<p>If that first paragraph got your hopes up, this second one is here to dash them: I’m not actually going to do a deep dive into the very low level aspects of concurrent performance. There are a lot of things we just don’t know about how atomic instructions and fences execute, and we’ll save that for another day.</p>

<p>Instead, I’m going to describe a higher level taxonomy that I use to think about concurrent performance. We’ll group the performance of concurrent operations into six broad <em>levels</em> running from fast to slow, with each level differing from its neighbors by roughly an order of magnitude in performance.</p>

<p>I often find myself thinking in terms of these categories when I need high performance concurrency: what is the best level I can practically achieve for the given problem? Keeping the levels in mind is useful both during initial design (sometimes a small change in requirements or high level design can allow you to achieve a better level), and also while evaluating existing systems (to better understand existing performance and evaluate the path of least resistance to improvements).</p>

<h3 id="a-real-world-example">A “Real World” Example</h3>

<p>I don’t want this to be totally abstract, so we will use a real-world-if-you-squint<sup id="fnref:realworld" role="doc-noteref"><a href="#fn:realworld" class="footnote" rel="footnote">1</a></sup> running example throughout: safely incrementing an integer counter across threads. By <em>safely</em> I mean without losing increments, producing out-of-thin air values, frying your RAM or making more than a minor rip in space-time.</p>

<h3 id="source-and-results">Source and Results</h3>

<p>The source for every benchmark here is <a href="https://github.com/travisdowns/concurrency-hierarchy-bench">available</a>, so you can follow along and even reproduce the results or run the benchmarks on your own hardware. All of the results discussed here (and more) are available in the same repository, and each plot includes a <code class="language-plaintext highlighter-rouge">[data table]</code> link to the specific subset used to generate the plot.</p>

<h3 id="hardware">Hardware</h3>

<p>All of the performance results are provided for several different hardware platforms: Intel Skylake, Ice Lake, Amazon Graviton and Graviton 2. However except when I explicitly mention other hardware, the prose refers to the results on Skylake. Although the specific numbers vary, most of the qualitative relationships hold for the hardware too, but <em>not always</em>. Not only does the hardware vary, but the OS and library implementations will vary as well.</p>

<p>It’s almost inevitable that this will be used to compare across hardware (“wow, Graviton 2 sure kicks Graviton 1’s ass”), but that’s not my goal here. The benchmarks are written primarily to tease apart the characteristics of the different levels, and <em>not</em> as a hardware shootout.</p>

<p>Find below the details of the hardware used:</p>

<table>
  <thead>
    <tr>
      <th>Micro-architecture</th>
      <th>ISA</th>
      <th>Model</th>
      <th>Tested Frequency</th>
      <th>Cores</th>
      <th>OS</th>
      <th>Instance Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Skylake</td>
      <td>x86</td>
      <td>i7-6700HQ</td>
      <td>2.6 GHz</td>
      <td>4</td>
      <td>Ubuntu 20.04</td>
      <td> </td>
    </tr>
    <tr>
      <td>Ice Lake</td>
      <td>x86</td>
      <td>i5-1035G4</td>
      <td>3.3 GHz</td>
      <td>4</td>
      <td>Ubuntu 19.10</td>
      <td> </td>
    </tr>
    <tr>
      <td>Graviton</td>
      <td>AArch64</td>
      <td>Cortex-A72</td>
      <td>2.3 GHz</td>
      <td>16</td>
      <td>Ubuntu 20.04</td>
      <td>a1.4xlarge</td>
    </tr>
    <tr>
      <td>Graviton 2</td>
      <td>AArch64</td>
      <td>Neoverse N1</td>
      <td>2.5 GHz</td>
      <td>16<sup id="fnref:g2cores" role="doc-noteref"><a href="#fn:g2cores" class="footnote" rel="footnote">2</a></sup></td>
      <td>Ubuntu 20.04</td>
      <td>c6g.4xlarge</td>
    </tr>
  </tbody>
</table>

<h2 id="level-2-contended-atomics">Level 2: Contended Atomics</h2>

<p>You’d probably expect this hierarchy to be introduced from fast to slow, or vice-versa, but we’re all about defying expectations here and we are going to start in the <em>middle</em> and work our way outwards. The middle (rounding down) turns out to be <em>level 2</em> and that’s where we will jump in.</p>

<p>The most elementary way to safely modify any shared object is to use a lock. It mostly <em>just works</em> for any type of object, no matter its structure or the nature of the modifications. Almost any mainstream CPU from the last thirty years has some type of locking<sup id="fnref:parisc" role="doc-noteref"><a href="#fn:parisc" class="footnote" rel="footnote">3</a></sup> instruction accessible to userspace.</p>

<p>So our baseline increment implementation will use a simple mutex of type <code class="language-plaintext highlighter-rouge">T</code> to protect a plain integer variable:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="n">lock</span><span class="p">;</span>
<span class="kt">uint64_t</span> <span class="n">counter</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">bench</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">iters</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">iters</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="n">holder</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
        <span class="n">counter</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We’ll call this implementation <em><abbr title="Uses a std::mutex and std::lock_guard to protect a plain integer counter.">mutex add</abbr></em>, and on my 4 CPU Skylake-S i7-6700HQ machine, when I use the vanilla <code class="language-plaintext highlighter-rouge">std::mutex</code> I get the following results for 2 to 4 threads:</p>

<div class="tabs" id="tabs-mutex">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-mutex-1" name="tab-group-mutex" checked="" />
      <label class="tab-label" for="tab-mutex-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/mutex.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/mutex.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/mutex.svg" alt="Mutex" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-mutex-2" name="tab-group-mutex" />
      <label class="tab-label" for="tab-mutex-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/mutex.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/mutex.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/mutex.svg" alt="Mutex" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-mutex-3" name="tab-group-mutex" />
      <label class="tab-label" for="tab-mutex-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/mutex.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/mutex.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/mutex.svg" alt="Mutex" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-mutex-4" name="tab-group-mutex" />
      <label class="tab-label" for="tab-mutex-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/mutex.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/mutex.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/mutex.svg" alt="Mutex" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p class="info">The reported value is the median of all trials, and the vertical black error lines at the top of each bar indicate the <em>interdecile range</em>, i.e., the values at the 10th and 90th percentile. Where the error bars don’t show up, it means there is no difference between the p10 and p90 values at all, at least within the limits of the reporting resolution (100 picoseconds).</p>

<p>This shows that the baseline contended cost to modify an integer protected by a lock starts at about 125 nanoseconds for two threads, and grows somewhat with increasing thread count.</p>

<p>I can already hear someone saying: <em>If you are just modifying a single 64-bit integer, skip the lock and just directly use the atomic operations that most ISAs support!</em></p>

<p>Sure, let’s add a couple of variants that do that. The <code class="language-plaintext highlighter-rouge">std::atomic&lt;T&gt;</code> template makes this easy: we can wrap any type meeting some basic requirements and then manipulate it atomically. The easiest of all is to use <code class="language-plaintext highlighter-rouge">std::atomic&lt;uint64&gt;::operator++()</code><sup id="fnref:post" role="doc-noteref"><a href="#fn:post" class="footnote" rel="footnote">4</a></sup> and this gives us <em><abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr></em>:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span> <span class="n">atomic_counter</span><span class="p">{};</span>

<span class="kt">void</span> <span class="nf">atomic_add</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">iters</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">iters</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">atomic_counter</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The other common approach would be to use <a href="https://en.wikipedia.org/wiki/Compare-and-swap">compare and swap (<abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>)</a> to load the existing value, add one and then <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> it back if it hasn’t changed. If it <em>has</em> changed, the increment raced with another thread and we try again.</p>

<p>Note that even if you use increment at the source level, the assembly might actually end up using <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> if your hardware doesn’t support atomic increment<sup id="fnref:atomicsup" role="doc-noteref"><a href="#fn:atomicsup" class="footnote" rel="footnote">5</a></sup>, or if your compiler or runtime just don’t take advantage of atomic operations even though they are available (e.g., see what even the newest version of <a href="https://godbolt.org/z/5h4K7y">icc does</a> for atomic increment, and what Java did for years<sup id="fnref:java" role="doc-noteref"><a href="#fn:java" class="footnote" rel="footnote">6</a></sup>). This caveat doesn’t apply to any of our tested platforms, however.</p>

<p>Let’s add a counter implementation that uses <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> as described above, and we’ll call it <em><abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr></em>:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span> <span class="n">cas_counter</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">cas_add</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">iters</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">iters</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">uint64_t</span> <span class="n">v</span> <span class="o">=</span> <span class="n">cas_counter</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
        <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">cas_counter</span><span class="p">.</span><span class="n">compare_exchange_weak</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">v</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Here’s what these look like alongside our existing <code class="language-plaintext highlighter-rouge">std::mutex</code> benchmark:</p>

<div class="tabs" id="tabs-atomic-inc">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-atomic-inc-1" name="tab-group-atomic-inc" checked="" />
      <label class="tab-label" for="tab-atomic-inc-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/atomic-inc.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/atomic-inc.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/atomic-inc.svg" alt="Atomic increment" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-atomic-inc-2" name="tab-group-atomic-inc" />
      <label class="tab-label" for="tab-atomic-inc-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/atomic-inc.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/atomic-inc.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/atomic-inc.svg" alt="Atomic increment" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-atomic-inc-3" name="tab-group-atomic-inc" />
      <label class="tab-label" for="tab-atomic-inc-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/atomic-inc.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/atomic-inc.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/atomic-inc.svg" alt="Atomic increment" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-atomic-inc-4" name="tab-group-atomic-inc" />
      <label class="tab-label" for="tab-atomic-inc-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/atomic-inc.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/atomic-inc.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/atomic-inc.svg" alt="Atomic increment" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>The first takeaway is that, at least in this <em>unrealistic maximum contention</em> benchmark, using <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> (<a href="https://www.felixcloutier.com/x86/xadd"><code class="language-plaintext highlighter-rouge">lock xadd</code></a> at the hardware level) is significantly better than <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>. The second would be that <code class="language-plaintext highlighter-rouge">std::mutex</code> doesn’t come out looking all that bad on Skylake. It is only slightly worse than the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> approach at 2 cores and beats it at 3 and 4 cores. It is slower than the atomic increment approach, but less than three times as slow and seems to be scaling in a reasonable way.</p>

<p>All of these operations are belong to <em>level 2</em> in the hierarchy. The primary characteristic of level 2 is that they make a <em>contended access</em> to a shared variable. This means that at a minimum, the line containing the data needs to move out to the caching agent that manages coherency<sup id="fnref:l3" role="doc-noteref"><a href="#fn:l3" class="footnote" rel="footnote">7</a></sup>, and then back up to the core that will receive ownership next. That’s about 70 cycles minimum just for that operation<sup id="fnref:inter" role="doc-noteref"><a href="#fn:inter" class="footnote" rel="footnote">8</a></sup>.</p>

<p>Can it get slower? You bet it can. <em>Way</em> slower.</p>

<h3 id="level-3-system-calls">Level 3: System Calls</h3>

<p>The next level up (“up” is not good here…) is level 3. The key characteristic of implementations at this level is that they make a <em>system call on almost every operation</em>.</p>

<p>It is easy to write concurrency primitives that make a system call <em>unconditionally</em> (e.g., a lock which always tries to wake waiters via a <code class="language-plaintext highlighter-rouge">futex(2)</code> call, even if there aren’t any), but we won’t look at those here. Rather we’ll take a look at a case where the fast path is written to avoid a system call, but the design or way it is used implies that such a call usually happens anyway.</p>

<p>Specifically, we are going to look at some <em>fair locks</em>. Fair locks allow threads into the critical section in the same order they began waiting. That is, when the critical section becomes available, the thread that has been waiting the longest is given the chance to take it.</p>

<p>Sounds like a good idea, right? Sometimes yes, but as we will see it can have significant performance implications.</p>

<p>On the menu are three different fair locks.</p>

<p>The first is a <a href="https://en.wikipedia.org/wiki/Ticket_lock">ticket lock</a> with a <code class="language-plaintext highlighter-rouge">sched_yield</code> in the spin loop. The idea of the yield is to give other threads which may hold the lock time to run. This <code class="language-plaintext highlighter-rouge">yield()</code> approach is publicly frowned upon by concurrency experts<sup id="fnref:notwhat" role="doc-noteref"><a href="#fn:notwhat" class="footnote" rel="footnote">9</a></sup>, who then sometimes go right ahead and use it anyway.</p>

<p id="ys-lock">We will call it <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr> and it looks like this:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
 * A ticket lock which uses sched_yield() while waiting
 * for the ticket to be served.
 */</span>
<span class="k">class</span> <span class="nc">ticket_yield</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="o">&gt;</span> <span class="n">dispenser</span><span class="p">{},</span> <span class="n">serving</span><span class="p">{};</span>

<span class="nl">public:</span>
    <span class="kt">void</span> <span class="n">lock</span><span class="p">()</span> <span class="p">{</span>
        <span class="k">auto</span> <span class="n">ticket</span> <span class="o">=</span> <span class="n">dispenser</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>

        <span class="k">while</span> <span class="p">(</span><span class="n">ticket</span> <span class="o">!=</span> <span class="n">serving</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_acquire</span><span class="p">))</span>
            <span class="n">sched_yield</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="kt">void</span> <span class="n">unlock</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">serving</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">serving</span><span class="p">.</span><span class="n">load</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">memory_order_release</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">};</span>
</code></pre></div></div>

<p>Let’s plot the performance results for this lock alongside the existing approaches:</p>

<div class="tabs" id="tabs-fair-yield">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-fair-yield-1" name="tab-group-fair-yield" checked="" />
      <label class="tab-label" for="tab-fair-yield-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/fair-yield.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/fair-yield.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/fair-yield.svg" alt="Increment Cost: Fair Yield" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-fair-yield-2" name="tab-group-fair-yield" />
      <label class="tab-label" for="tab-fair-yield-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/fair-yield.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/fair-yield.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/fair-yield.svg" alt="Increment Cost: Fair Yield" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-fair-yield-3" name="tab-group-fair-yield" />
      <label class="tab-label" for="tab-fair-yield-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/fair-yield.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/fair-yield.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/fair-yield.svg" alt="Increment Cost: Fair Yield" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-fair-yield-4" name="tab-group-fair-yield" />
      <label class="tab-label" for="tab-fair-yield-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/fair-yield.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/fair-yield.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/fair-yield.svg" alt="Increment Cost: Fair Yield" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>This is level 3 visualized: it is an order of magnitude slower than the level 2 approaches. The slowdown comes from the <code class="language-plaintext highlighter-rouge">sched_yield</code> call: this is a system call and these are generally on the order of 100s of nanoseconds<sup id="fnref:spectre" role="doc-noteref"><a href="#fn:spectre" class="footnote" rel="footnote">10</a></sup>, and it shows in the results.</p>

<p>This lock <em>does</em> have a fast path where <code class="language-plaintext highlighter-rouge">sched_yield</code> isn’t called: if the lock is available, no spinning occurs and <code class="language-plaintext highlighter-rouge">sched_yield</code> is never called. However, the combination of being a <em>fair</em> lock and the high contention in this test means that a lock convoy quickly forms (we’ll describe this in more detail later) and so the spin loop is entered basically every time <code class="language-plaintext highlighter-rouge">lock()</code> is called.</p>

<p>So have we <em>now</em> fully plumbed the depths of slow concurrency constructs? Not even close. We are only now just about to cross the River Styx.</p>

<h4 id="revisiting-stdmutex">Revisiting std::mutex</h4>

<p>Before we proceed, let’s quickly revisit the <code class="language-plaintext highlighter-rouge">std::mutex</code> implementation discussed in level 2 in light of our definition of level 3 as requiring a system call. Doesn’t <code class="language-plaintext highlighter-rouge">std::mutex</code> <em>also</em> make system calls? If a thread tries to lock a <code class="language-plaintext highlighter-rouge">std::mutex</code> object which is already locked, we expect that thread to block using OS-provided primitives. So why isn’t it level 3 and slow like <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr>?</p>

<p>The primary reason is that it makes <em>few</em> system calls in practice. Through a combination of spinning and unfairness I measure only about 0.18 system calls per increment, with three threads on my Skylake box. So <em>most</em> increments happen without a system call. On the other hand, <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr> makes about 2.4 system calls per increment, more than an order of magnitude more, and so it suffers a corresponding decrease in performance.</p>

<p>That out of way, let’s get even slower.</p>

<h3 id="level-4-implied-context-switch">Level 4: Implied Context Switch</h3>

<p>The next level is when the implementation forces a significant number of concurrent operations to cause a <em>context switch</em>.</p>

<p>The yielding lock wasn’t resulting in many context switches, since we are not running more threads than there are cores, and so there usually is no other runnable process (except for the occasional background process). Therefore, the current thread stays on the CPU when we call <code class="language-plaintext highlighter-rouge">sched_yield</code>. Of course, this burns a lot of CPU.</p>

<p>As the experts recommend whenever one suggests <em>yielding</em> in a spin loop, let us try a <em>blocking lock</em> instead.</p>

<p class="info"><strong>Blocking Locks</strong><br />
<br />
A more resource friendly design, and one that will often perform better is a <em>blocking</em> lock.<br /><br />Rather than busy waiting, these locks ask the OS to put the current thread to sleep until the lock becomes available. On Linux, the <a href="http://man7.org/linux/man-pages/man2/futex.2.html"><code class="language-plaintext highlighter-rouge">futex(3)</code></a> system call is the preferred way to accomplish this, while on Windows you have the <a href="https://docs.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-waitforsingleobject"><code class="language-plaintext highlighter-rouge">WaitFor*Object</code></a> API family. Above the OS interfaces, things like C++’s <code class="language-plaintext highlighter-rouge">std::condition_variable</code> provide a general purpose mechanism to wait until an arbitrary condition is true.</p>

<p>Our first blocking lock is again a ticket-based design, except this time it uses a condition variable to block when it detects that it isn’t first in line to be served (i.e., that the lock was held by another thread). We’ll name it <abbr title="A ticket lock which blocks if it cannot immediately acquire the lock.">ticket blocking</abbr> and it looks like this:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">blocking_ticket</span><span class="o">::</span><span class="n">lock</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">ticket</span> <span class="o">=</span> <span class="n">dispenser</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">ticket</span> <span class="o">==</span> <span class="n">serving</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_acquire</span><span class="p">))</span>
        <span class="k">return</span><span class="p">;</span> <span class="c1">// uncontended case</span>

    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex</span><span class="p">);</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">ticket</span> <span class="o">!=</span> <span class="n">serving</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_acquire</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">cvar</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">blocking_ticket</span><span class="o">::</span><span class="n">unlock</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">s</span> <span class="o">=</span> <span class="n">serving</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">serving</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">memory_order_release</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">d</span> <span class="o">=</span> <span class="n">dispenser</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
    <span class="n">assert</span><span class="p">(</span><span class="n">s</span> <span class="o">&lt;=</span> <span class="n">d</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">s</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// wake all waiters</span>
        <span class="n">cvar</span><span class="p">.</span><span class="n">notify_all</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The main difference with the earlier implementation occurs in the case where we don’t acquire the lock immediately (we don’t return at the location marked <code class="language-plaintext highlighter-rouge">// uncontended case</code>). Instead of yielding in a loop, we take the mutex associated with the condition variable and wait until notified. Every time we are notified we check if it is our turn.</p>

<p>Even without <abbr title="When a waiter on a condition variable is woken up even though no other thread notified it.">spurious wakeups</abbr> we might get woken many times, because this lock suffers from the <em>thundering herd</em> problem where every waiter is woken on <code class="language-plaintext highlighter-rouge">unlock()</code> even though only one will ultimately be able to get the lock.</p>

<p>We’ll try a second design too, that doesn’t suffer from thundering herd. This is a queued lock, where each lock waits on its own private node in a queue of waiters, so only a single waiter (the new lock owner) is woken up on unlock. We will call it <abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr> and if you’re interested in the implementation you <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/9b8e0e0dfec7d38036d114038c6a9ed020b5b775/fairlocks.cpp#L61">can find it here</a>.</p>

<p>Here’s how our new locks perform against the existing crowd:</p>

<div class="tabs" id="tabs-more-fair">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-more-fair-1" name="tab-group-more-fair" checked="" />
      <label class="tab-label" for="tab-more-fair-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/more-fair.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/more-fair.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/more-fair.svg" alt="Increment Cost: Fair Blocking" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-more-fair-2" name="tab-group-more-fair" />
      <label class="tab-label" for="tab-more-fair-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/more-fair.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/more-fair.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/more-fair.svg" alt="Increment Cost: Fair Blocking" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-more-fair-3" name="tab-group-more-fair" />
      <label class="tab-label" for="tab-more-fair-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/more-fair.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/more-fair.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/more-fair.svg" alt="Increment Cost: Fair Blocking" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-more-fair-4" name="tab-group-more-fair" />
      <label class="tab-label" for="tab-more-fair-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/more-fair.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/more-fair.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/more-fair.svg" alt="Increment Cost: Fair Blocking" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>You’re probably seeing the pattern now: performance is again a new level of terrible compared to the previous contenders. About an order of magnitude slower than the yielding approach, which was already slower than the earlier approaches, which are now just slivers a few pixels high on the plots. The queued version of the lock does slightly better at increasing thread counts (<em>especially</em> on Graviton 2), as might be expected from the lack of the thundering herd effect, but is still very slow because the primary problem isn’t thundering herd, but rather a <a href="https://en.wikipedia.org/wiki/Lock_convoy"><em>lock convoy</em></a>.</p>

<p class="info"><strong>Lock Convoy</strong><br />
<br />
Unlike unfair locks, fair locks can result in sustained convoys involving only a single lock, once the contention reaches a certain point<sup id="fnref:hyst" role="doc-noteref"><a href="#fn:hyst" class="footnote" rel="footnote">11</a></sup>.<br />
<br />
Consider what happens when two threads, <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>, try to acquire the lock repeatedly. Let’s say <code class="language-plaintext highlighter-rouge">A</code> gets ticket 1 and <code class="language-plaintext highlighter-rouge">B</code> ticket 2. So <code class="language-plaintext highlighter-rouge">A</code> gets to go first and <code class="language-plaintext highlighter-rouge">B</code> has to wait, and for these implementations that means blocking (we can say the thread is <em>parked</em> by the OS). Now, <code class="language-plaintext highlighter-rouge">A</code> unlocks the lock and sees <code class="language-plaintext highlighter-rouge">B</code> waiting and wakes it. <code class="language-plaintext highlighter-rouge">A</code> is still running and soon tries to get the lock again, receiving ticket 3, but it cannot acquire the lock immediately because the lock is <em>fair</em>: <code class="language-plaintext highlighter-rouge">A</code> can’t jump the queue and acquire the lock with ticket 3 before <code class="language-plaintext highlighter-rouge">B</code>, holding ticket 2, gets its chance to enter the lock.<br />
<br />
Of course, <code class="language-plaintext highlighter-rouge">B</code> is going to be a while: it needs to be woken by the scheduler and this takes a microsecond or two, at least. Now <code class="language-plaintext highlighter-rouge">B</code> wakes and gets the lock, and the same scenario repeats itself with the roles reversed. The upshot is that there is a full context switch for each acquisition of the lock.<br />
<br />
Unfair locks avoid this problem because they allow queue jumping: in the scenario above, <code class="language-plaintext highlighter-rouge">A</code> (or any other thread) could re-acquire the lock after unlocking it, before <code class="language-plaintext highlighter-rouge">B</code> got its chance. So the use of the shared resource doesn’t grind to a halt while <code class="language-plaintext highlighter-rouge">B</code> wakes up.</p>

<p>So, are you tired of seeing mostly-white plots where the newly introduced algorithm relegates the rest of the pack to little chunks of color near the x-axis, yet?</p>

<p>I’ve just got one more left on the slow end of the scale. Unlike the other examples, I haven’t actually diagnosed something <em>this</em> bad in real life, but examples are out there.</p>

<h3 id="level-5-catastrophe">Level 5: Catastrophe</h3>

<p>Here’s a ticket lock which is identical to the <a href="#ys-lock">first ticket lock we saw</a>, except that the <code class="language-plaintext highlighter-rouge">sched_yield();</code> is replaced by <code class="language-plaintext highlighter-rouge">;</code>. That is, it busy waits instead of yielding (<a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/9b8e0e0dfec7d38036d114038c6a9ed020b5b775/fairlocks.cpp#L31">and here are the spin flavors which specialize on a shared ticket lock template</a>). You could also replace this by a CPU-specific “relax” instruction like <a href="https://www.felixcloutier.com/x86/pause"><code class="language-plaintext highlighter-rouge">pause</code></a>, but it won’t <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/9b8e0e0dfec7d38036d114038c6a9ed020b5b775/fairlocks.hpp#L26">change the outcome</a>. We call it <abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr>, and here’s how it performs compared to the existing candidates:</p>

<div class="tabs" id="tabs-ts-4">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-4-1" name="tab-group-ts-4" checked="" />
      <label class="tab-label" for="tab-ts-4-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/ts-4.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/ts-4.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/ts-4.svg" alt="Increment Cost: Ticket Spin" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-4-2" name="tab-group-ts-4" />
      <label class="tab-label" for="tab-ts-4-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/ts-4.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/ts-4.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/ts-4.svg" alt="Increment Cost: Ticket Spin" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-4-3" name="tab-group-ts-4" />
      <label class="tab-label" for="tab-ts-4-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/ts-4.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/ts-4.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/ts-4.svg" alt="Increment Cost: Ticket Spin" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-4-4" name="tab-group-ts-4" />
      <label class="tab-label" for="tab-ts-4-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/ts-4.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/ts-4.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/ts-4.svg" alt="Increment Cost: Ticket Spin" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>What? That doesn’t look too bad at all. In fact, it is only slightly worse than the level 2 crew, the fastest we’ve seen so far<sup id="fnref:huh" role="doc-noteref"><a href="#fn:huh" class="footnote" rel="footnote">12</a></sup>.</p>

<p>The picture changes if we show the results for up to 6 threads, rather than just 4. Since I have 4 available cores<sup id="fnref:noht" role="doc-noteref"><a href="#fn:noht" class="footnote" rel="footnote">13</a></sup>, this means that not all the test threads will be able to run at once:</p>

<div class="tabs" id="tabs-ts-6">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-6-1" name="tab-group-ts-6" checked="" />
      <label class="tab-label" for="tab-ts-6-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/ts-6.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/ts-6.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/ts-6.svg" alt="Increment Cost: Ticket Spin (Oversubscribed)" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-6-2" name="tab-group-ts-6" />
      <label class="tab-label" for="tab-ts-6-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/ts-6.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/ts-6.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/ts-6.svg" alt="Increment Cost: Ticket Spin (Oversubscribed)" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-6-3" name="tab-group-ts-6" />
      <label class="tab-label" for="tab-ts-6-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/ts-6.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/ts-6.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/ts-6.svg" alt="Increment Cost: Ticket Spin (Oversubscribed)" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-ts-6-4" name="tab-group-ts-6" />
      <label class="tab-label" for="tab-ts-6-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/ts-6.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/ts-6.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/ts-6.svg" alt="Increment Cost: Ticket Spin (Oversubscribed)" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>Now it becomes clear why this level is called <em>catastrophic</em>. As soon as we oversubscribe the number of available cores, performance gets about <em>five hundred times worse</em>. We go from 100s of nanoseconds to 100s of microseconds. I don’t show more threads, but it only gets worse as you add more.</p>

<p>We are also about an order of magnitude slower than the best solution (<abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr>) of the previous level, although it varies a lot by hardware: on Ice Lake the difference is more like <em>forty</em> times, while on Graviton this solution is actually slightly faster than <abbr title="A ticket lock which blocks if it cannot immediately acquire the lock.">ticket blocking</abbr> (also level 4) at 17 threads. Note also the huge error bars. This is the least consistent benchmark of the bunch and exhibits a lot of variance and the slowest and fastest runs might vary by a factor of 100.</p>

<h4 id="lock-convoy-on-steroids">Lock Convoy on Steroids</h4>

<p>So what happens here?</p>

<p>It’s similar to the lock convoy described above: all the threads queue on the lock and acquire it in a round-robin order due to the fair design. The difference is that threads don’t block when they can’t acquire the lock. This works out great when the cores are not oversubscribed, but falls off a cliff otherwise.</p>

<p>Imagine 5 threads, <code class="language-plaintext highlighter-rouge">T1</code>, <code class="language-plaintext highlighter-rouge">T2</code>, …, <code class="language-plaintext highlighter-rouge">T5</code>, where <code class="language-plaintext highlighter-rouge">T5</code> is the one not currently running. As soon as <code class="language-plaintext highlighter-rouge">T5</code> is the thread that needs the acquire the lock next (i.e., <code class="language-plaintext highlighter-rouge">T5</code>’s saved ticket value is equal to <code class="language-plaintext highlighter-rouge">dispensing</code>), nothing will happen because <code class="language-plaintext highlighter-rouge">T1</code> through <code class="language-plaintext highlighter-rouge">T4</code> are busily spinning away waiting for their turn. The OS scheduler sees no reason to interrupt them until their time slice expires. Time slices are usually measured in milliseconds. Once one thread is preempted, say <code class="language-plaintext highlighter-rouge">T1</code>, <code class="language-plaintext highlighter-rouge">T5</code> will get the chance to run, but at most 4 total acquisitions can happen (<code class="language-plaintext highlighter-rouge">T5</code>, plus any of <code class="language-plaintext highlighter-rouge">T2</code>, <code class="language-plaintext highlighter-rouge">T3</code>, <code class="language-plaintext highlighter-rouge">T4</code>), before it’s <code class="language-plaintext highlighter-rouge">T1</code>’s turn. <code class="language-plaintext highlighter-rouge">T1</code> is waiting for their chance to run again, but since everyone is spinning this won’t occur until another time slice expires.</p>

<p>So the lock can only be acquired a few times (at most <code class="language-plaintext highlighter-rouge">$(nproc)</code> times), or as little as once<sup id="fnref:once" role="doc-noteref"><a href="#fn:once" class="footnote" rel="footnote">14</a></sup>, every time slice. Modern Linux using <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS</a> doesn’t have a fixed timeslice, but on my system, <code class="language-plaintext highlighter-rouge">sched_latency_ns</code> is 18,000,000 which means that we expect two threads competing for one core to get a typical timeslice of 9 ms. The measured numbers are roughly consistent with a timeslice of single-digit milliseconds.</p>

<p>If I was good at diagrams, there would be a diagram here.</p>

<p>Another way of thinking about this is that in this over-subscription scenario, the <abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr> lock implies roughly the same number of context switches as the blocking ticket lock<sup id="fnref:perf" role="doc-noteref"><a href="#fn:perf" class="footnote" rel="footnote">15</a></sup>, but in the former case each context switch comes with a giant delay caused by the need to exhaust the timeslice, while in the blocking case we are only limited by how fast a context switch can occur.</p>

<p>Interestingly, although this benchmark uses 100% CPU on every core, the performance of the benchmark in the oversubscribed case almost doesn’t depend on your CPU speed! Performance is approximately the same if I throttle my CPU to 1 GHz, or enable turbo up to 3.5 GHz. All of other implementations scale almost proportionally with CPU frequency. The benchmark does scale strongly with adjustment to <code class="language-plaintext highlighter-rouge">sched_latency_ns</code> (and <code class="language-plaintext highlighter-rouge">sched_min_granularity_ns</code> if the former is set low enough): lower scheduling latency values gives proportionally better performance as the time slices shrink, helping to confirm our theory of how this works.</p>

<p>This behavior also explains the large amount of variance once the available cores are oversubscribed: by definition, not all threads will be running at once, so the test becomes very sensitive to exactly where the not-running threads took their context switch. At the beginning of the test, only 4 of 6 threads will be running, and the two will be switched out, still waiting on the the <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/master/cyclic-barrier.hpp">barrier</a> that synchronizes the test start. Since the two switched out threads haven’t tried to get the lock yet, the four running threads will be able to quickly share the lock between themselves, since the six-thread convoy hasn’t been set up.</p>

<p>This runs up the “iteration count” (work done) during an initial period which varies randomly, until the first context switch lets the fifth thread join the competition and then the convoy gets set up<sup id="fnref:csdepend" role="doc-noteref"><a href="#fn:csdepend" class="footnote" rel="footnote">16</a></sup>. That’s when the catastrophe starts. This makes the results very noisy: for example, if you set a too-short time period for a trial, the <em>entire test</em> is composed of this initial phase and the results are artificially “good”.</p>

<p>We can probably invent something even worse, but that’s enough for now. Let’s move on to scenarios that are <em>faster</em> than the use of vanilla <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>.</p>

<h3 id="level-1-uncontended-atomics">Level 1: Uncontended Atomics</h3>

<p>Recall that we started at level 2: contended atomics. The name gives it away: the next faster level is when atomic operations are used but there is no contention, either by design or by luck. You might have noticed that so far we’ve only shown results for at least two threads. That’s because the single threaded case involves no contention, and so every implementation so far is level 1 if run on a single thread<sup id="fnref:notexx" role="doc-noteref"><a href="#fn:notexx" class="footnote" rel="footnote">17</a></sup>.</p>

<p>Here are the results for all the implementations we’ve looked at so far, for a single thread:</p>

<div class="tabs" id="tabs-single">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-single-1" name="tab-group-single" checked="" />
      <label class="tab-label" for="tab-single-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/single.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/single.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/single.svg" alt="Increment Cost: Single Threaded" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-single-2" name="tab-group-single" />
      <label class="tab-label" for="tab-single-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/single.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/single.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/single.svg" alt="Increment Cost: Single Threaded" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-single-3" name="tab-group-single" />
      <label class="tab-label" for="tab-single-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/single.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/single.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/single.svg" alt="Increment Cost: Single Threaded" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-single-4" name="tab-group-single" />
      <label class="tab-label" for="tab-single-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/single.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/single.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/single.svg" alt="Increment Cost: Single Threaded" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>The fastest implementations run in about 10 nanoseconds, which is 5x faster than the fastest solution for 2 or more threads. The <em>slowest</em> implementation (<abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr>) for one thread ties the <em>fastest</em> implementation (<abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>) at two threads, and beats it handily at three or four.</p>

<p>The number overlaid on each bar is the number of atomic operations<sup id="fnref:atomhow" role="doc-noteref"><a href="#fn:atomhow" class="footnote" rel="footnote">18</a></sup> each implementation makes per increment. It is obvious that the performance is almost directly proportional to the number of atomic instructions. On the other hand, performance does <em>not</em> have much of a relationship with the total number of instructions of any type, which vary a lot even between algorithms with the same performance as the following table shows:</p>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th style="text-align: right">Atomics</th>
      <th style="text-align: right">Instructions</th>
      <th style="text-align: right">Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><abbr title="Uses a std::mutex and std::lock_guard to protect a plain integer counter.">mutex add</abbr></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">64</td>
      <td style="text-align: right">~21 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">~7 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">~12 ns</td>
    </tr>
    <tr>
      <td><abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">13</td>
      <td style="text-align: right">~10 ns</td>
    </tr>
    <tr>
      <td><abbr title="A ticket lock which blocks if it cannot immediately acquire the lock.">ticket blocking</abbr></td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">107</td>
      <td style="text-align: right">~32 ns</td>
    </tr>
    <tr>
      <td><abbr title="A blocking ticket lock where each waiter waits on a unique condition variable.">queued fifo</abbr></td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">167</td>
      <td style="text-align: right">~45 ns</td>
    </tr>
    <tr>
      <td><abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">13</td>
      <td style="text-align: right">~10 ns</td>
    </tr>
    <tr>
      <td><abbr title="A simple mutex from &quot;Futexes Are Tricky&quot;.">mutex3</abbr></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">17</td>
      <td style="text-align: right">~20 ns</td>
    </tr>
  </tbody>
</table>

<p>In particular, note that <abbr title="Uses a std::mutex and std::lock_guard to protect a plain integer counter.">mutex add</abbr> has more than 9x the number of instructions compared to <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> yet still runs at half the speed, in line with the 2:1 ratio of atomics. Similarly, <abbr title="A ticket lock that calls sched_yield in a spin loop while waiting for its turn.">ticket yield</abbr> and <abbr title="A traditional spin-based ticket lock that does a hot spin while waiting for its ticket to be next.">ticket spin</abbr> have slightly <em>better</em> performance than <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> despite having about twice the number of instructions, in line with them all having a single atomic operation<sup id="fnref:casworse" role="doc-noteref"><a href="#fn:casworse" class="footnote" rel="footnote">19</a></sup>.</p>

<p>The last row in the table shows the performance of <abbr title="A simple mutex from &quot;Futexes Are Tricky&quot;.">mutex3</abbr>, an implementation we haven’t discussed. It is a basic mutex offering similar functionality to <code class="language-plaintext highlighter-rouge">std::mutex</code> and whose implementation is described in <a href="https://akkadia.org/drepper/futex.pdf">Futexes Are Tricky</a>. Because it doesn’t need to pass through two layers of abstraction<sup id="fnref:twolayer" role="doc-noteref"><a href="#fn:twolayer" class="footnote" rel="footnote">20</a></sup>, it has only about one third the instruction count of <code class="language-plaintext highlighter-rouge">std::mutex</code>, yet performance is almost exactly the same, differing by less than 10%.</p>

<p>So the idea that you can almost ignore things that are in a lower cost tier seems to hold here. Don’t take this too far: if you design a lock with a single atomic operation but 1,000 other instructions, it is not going to be fast. There are also reasons to keep your instruction count low other than microbenchmark performance: smaller instruction cache footprint, less space occupied in various <abbr title="Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.">out-of-order</abbr> execution buffers, more favorable inlining tradeoffs, etc.</p>

<p>Here it is important to note that the change in level of our various functions didn’t require a change in implementation. These are exactly the same few implementations we discussed in the slower levels. Instead, we simply changed (by fiat, i.e., adjusting the benchmark parameters) the contention level from “very high” to “zero”. So in this case the  level doesn’t depend only on the code, but also this external factor. Of course, just saying that we are going to get to level 1 by only running one thread is not very useful in real life: we often can’t simply ban multi-threaded operation.</p>

<p>So can we get to level 1 even under concurrent calls from multiple threads? For this particular problem, we can.</p>

<h4 id="adaptive-multi-counter">Adaptive Multi-Counter</h4>

<p>One option is to use multiple counters to represent the counter value. We try to organize it so that that threads running concurrently on different CPUs will increment different counters. Thus the <em>logical</em> counter value is split across all of these internal <em>physical</em> counters, and so a read of the logical counter value now needs to add together all the physical counter values.</p>

<p>Here’s an implementation:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">cas_multi_counter</span> <span class="p">{</span>
    <span class="k">static</span> <span class="k">constexpr</span> <span class="kt">size_t</span> <span class="n">NUM_COUNTERS</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>

    <span class="k">static</span> <span class="k">thread_local</span> <span class="kt">size_t</span> <span class="n">idx</span><span class="p">;</span>
    <span class="n">multi_holder</span> <span class="n">array</span><span class="p">[</span><span class="n">NUM_COUNTERS</span><span class="p">];</span>

<span class="nl">public:</span>

    <span class="cm">/** increment the logical counter value */</span>
    <span class="kt">uint64_t</span> <span class="k">operator</span><span class="o">++</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">auto</span><span class="o">&amp;</span> <span class="n">counter</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">counter</span><span class="p">;</span>

            <span class="k">auto</span> <span class="n">cur</span> <span class="o">=</span> <span class="n">counter</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">counter</span><span class="p">.</span><span class="n">compare_exchange_strong</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="n">cur</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
                <span class="k">return</span> <span class="n">cur</span><span class="p">;</span>
            <span class="p">}</span>

            <span class="c1">// CAS failure indicates contention,</span>
            <span class="c1">// so try again at a different index</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_COUNTERS</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="kt">uint64_t</span> <span class="n">read</span><span class="p">()</span> <span class="p">{</span>
        <span class="kt">uint64_t</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="n">h</span> <span class="o">:</span> <span class="n">array</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">h</span><span class="p">.</span><span class="n">counter</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">};</span>
</code></pre></div></div>

<p>We’ll call this <abbr title="Uses a CAS on an adatively per-CPU counter.">cas multi</abbr>, and the approach is relatively straightforward.</p>

<p>There are 64 padded<sup id="fnref:padded" role="doc-noteref"><a href="#fn:padded" class="footnote" rel="footnote">21</a></sup> physical counters whose sum makes up the logical counter value. There is a thread-local <code class="language-plaintext highlighter-rouge">idx</code> value, initially zero for every thread, that points to the physical counter that each thread should increment. When <code class="language-plaintext highlighter-rouge">operator++</code> is called, we attempt to increment the counter pointed to by <code class="language-plaintext highlighter-rouge">idx</code> using <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>.</p>

<p>If this fails, however, we don’t simply retry. Failure indicates contention<sup id="fnref:notallfailure" role="doc-noteref"><a href="#fn:notallfailure" class="footnote" rel="footnote">22</a></sup> (this is the only way the <em>strong</em> variant of <code class="language-plaintext highlighter-rouge">compare_exchange</code> can fail), so we add one to <code class="language-plaintext highlighter-rouge">idx</code> to try another counter on the next attempt.</p>

<p>In a high-contention scenario like our benchmark, every CPU quickly ends up pointing to a different index value. If there is low contention, it is possible that only the first physical counter will be used.</p>

<p>Let’s compare this to the <code class="language-plaintext highlighter-rouge">atomic add</code> version we looked at above, which was the fastest of the level 2 approaches. Recall that it uses an <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> on a single counter.</p>

<div class="tabs" id="tabs-cas-multi">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-cas-multi-1" name="tab-group-cas-multi" checked="" />
      <label class="tab-label" for="tab-cas-multi-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/cas-multi.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/cas-multi.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/cas-multi.svg" alt="Increment Cost: Contention Adaptive Multi-Counter" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-cas-multi-2" name="tab-group-cas-multi" />
      <label class="tab-label" for="tab-cas-multi-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/cas-multi.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/cas-multi.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/cas-multi.svg" alt="Increment Cost: Contention Adaptive Multi-Counter" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-cas-multi-3" name="tab-group-cas-multi" />
      <label class="tab-label" for="tab-cas-multi-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/cas-multi.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/cas-multi.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/cas-multi.svg" alt="Increment Cost: Contention Adaptive Multi-Counter" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-cas-multi-4" name="tab-group-cas-multi" />
      <label class="tab-label" for="tab-cas-multi-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/cas-multi.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/cas-multi.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/cas-multi.svg" alt="Increment Cost: Contention Adaptive Multi-Counter" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>For 1 active core, the results are the same as we saw earlier: the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> approach performs the same as the <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> algorithm<sup id="fnref:perfsame" role="doc-noteref"><a href="#fn:perfsame" class="footnote" rel="footnote">23</a></sup>, which is somewhat slower than <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>, due to the need for an additional load (i.e., the line with <code class="language-plaintext highlighter-rouge">counter.load()</code>) to set up the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>.</p>

<p>For 2 to 4 cores, the situation changes dramatically. The multiple counter approach performs the <em>same</em> regardless of the number of active cores. That is, it exhibits perfect scaling with multiple cores – in contrast to the single-counter approach which scales poorly. At four cores, the relative speedup of the multi-counter approach is about 9x. On Amazon’s Graviton ARM processor the speedup approaches <em>eighty</em> times at 16 threads.</p>

<p>This improvement in increment performance comes at a cost, however:</p>

<ul>
  <li>64 counters ought to be enough for anyone, but they take 4096 (!!) bytes of memory to store what takes only 8 bytes in the <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> approach<sup id="fnref:eightbyte" role="doc-noteref"><a href="#fn:eightbyte" class="footnote" rel="footnote">24</a></sup>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">read()</code> method is much slower: it needs to iterate over and add all 64 values, versus a single load for the earlier approaches.</li>
  <li>The implementation compiles to much larger code: 113 bytes versus 15 bytes for the single counter <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> approach or 7 bytes for the <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr> approach.</li>
  <li>The concurrent behavior is considerably harder to reason about and document. For example, it is harder to explain the consistency condition provided by <code class="language-plaintext highlighter-rouge">read()</code> since it is no longer a single atomic read<sup id="fnref:read" role="doc-noteref"><a href="#fn:read" class="footnote" rel="footnote">25</a></sup>.</li>
  <li>There is a single thread-local <code class="language-plaintext highlighter-rouge">idx</code> variable. So while different <code class="language-plaintext highlighter-rouge">cas_multi_counter</code> instances are logically independent, the shared <code class="language-plaintext highlighter-rouge">idx</code> variable means that things that happen in one counter can affect the non-functional behavior of the others<sup id="fnref:sharedidx" role="doc-noteref"><a href="#fn:sharedidx" class="footnote" rel="footnote">26</a></sup>.</li>
</ul>

<p>Some of these downsides can be partly mitigated:</p>

<ul>
  <li>A much smaller number of counters would probably be better for most practical uses. We could also set the array size dynamically based on the detected number of logical CPUs since a larger array should not provide much of a performance increase. Better yet, we might make the size even more dynamic, based on contention: start with a single element and grow it only when contention is detected. This means that even on systems with many CPUs, the size will remain small if contention is never seen in practice. This has a runtime cost<sup id="fnref:rtcost" role="doc-noteref"><a href="#fn:rtcost" class="footnote" rel="footnote">27</a></sup>, however.</li>
  <li>We could optimize the <code class="language-plaintext highlighter-rouge">read()</code> method by stopping when we see a zero counter. I believe a careful analysis shows that the non-zero counter values for any instance of this class are all in a contiguous region starting from the beginning of the counter array<sup id="fnref:subtle" role="doc-noteref"><a href="#fn:subtle" class="footnote" rel="footnote">28</a></sup>.</li>
  <li>We could mitigate some of the code footprint by carefully carving the “less hot”<sup id="fnref:lesshot" role="doc-noteref"><a href="#fn:lesshot" class="footnote" rel="footnote">29</a></sup> slow path out into a another function, and use our <a href="https://xania.org/201209/forcing-code-out-of-line-in-gcc">magic powers</a> to encourage the small fast path (the first <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>) to be inlined while the fallback remains not inlined.</li>
  <li>We could make the thread-local <code class="language-plaintext highlighter-rouge">idx</code> per instance specific to solve the “shared <code class="language-plaintext highlighter-rouge">idx</code> across all instances” problem. This does require some non-negligible amount of work to implement a dynamic TLS system which can create as many thread local keys as you want<sup id="fnref:dynamictls" role="doc-noteref"><a href="#fn:dynamictls" class="footnote" rel="footnote">30</a></sup>, and it is slower.</li>
</ul>

<p>So while we got a good looking chart, this solution doesn’t exactly dominate the simpler ones. You pay a price along several axes for the lack of contention and you shouldn’t blindly replace the simpler solutions with this one – it needs to be a carefully considered and use-case dependent decision.</p>

<p>Is it over yet? Can I close this browser tab and reclaim all that memory? Almost. Just one level to go.</p>

<h3 id="level-0-vanilla">Level 0: Vanilla</h3>

<p>The last and fastest level is achieved when only vanilla instructions are used (and without contention). By <em>vanilla instructions</em> I mean things like regular loads and stores which don’t imply additional synchronization above what the hardware memory model offers by default<sup id="fnref:noatomic" role="doc-noteref"><a href="#fn:noatomic" class="footnote" rel="footnote">31</a></sup>.</p>

<p>How can we increment a counter atomically while allowing it to be read from any thread? By ensuring there is only one writer for any given physical counter. If we keep a counter <em>per thread</em> and only allow the owning thread to write to it, there is no need for an atomic increment.</p>

<p>The obvious way to keep a per-thread counter is use thread-local storage. Something like this:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
 * Keeps a counter per thread, readers need to sum
 * the counters from all active threads and add the
 * accumulated value from dead threads.
 */</span>
<span class="k">class</span> <span class="nc">tls_counter</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span> <span class="n">counter</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>

    <span class="cm">/* protects all_counters and accumulator */</span>
    <span class="k">static</span> <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">lock</span><span class="p">;</span>
    <span class="cm">/* list of all active counters */</span>
    <span class="k">static</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tls_counter</span> <span class="o">*&gt;</span> <span class="n">all_counters</span><span class="p">;</span>
    <span class="cm">/* accumulated value of counters from dead threads */</span>
    <span class="k">static</span> <span class="kt">uint64_t</span> <span class="n">accumulator</span><span class="p">;</span>
    <span class="cm">/* per-thread tls_counter object */</span>
    <span class="k">static</span> <span class="k">thread_local</span> <span class="n">tls_counter</span> <span class="n">tls</span><span class="p">;</span>

    <span class="cm">/** add ourselves to the counter list */</span>
    <span class="n">tls_counter</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">g</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
        <span class="n">all_counters</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="k">this</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="cm">/**
     * destruction means the thread is going away, so
     * we stash the current value in the accumulator and
     * remove ourselves from the array
     */</span>
    <span class="o">~</span><span class="n">tls_counter</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">g</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
        <span class="n">accumulator</span> <span class="o">+=</span> <span class="n">counter</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
        <span class="n">all_counters</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">remove</span><span class="p">(</span><span class="n">all_counters</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">all_counters</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="k">this</span><span class="p">),</span> <span class="n">all_counters</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
    <span class="p">}</span>

    <span class="kt">void</span> <span class="n">incr</span><span class="p">()</span> <span class="p">{</span>
        <span class="k">auto</span> <span class="n">cur</span> <span class="o">=</span> <span class="n">counter</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
        <span class="n">counter</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">cur</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
    <span class="p">}</span>

<span class="nl">public:</span>

    <span class="k">static</span> <span class="kt">uint64_t</span> <span class="n">read</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">g</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
        <span class="kt">uint64_t</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">h</span> <span class="o">:</span> <span class="n">all_counters</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">h</span><span class="o">-&gt;</span><span class="n">counter</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
            <span class="n">count</span><span class="o">++</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">accumulator</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">static</span> <span class="kt">void</span> <span class="n">increment</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">tls</span><span class="p">.</span><span class="n">incr</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">};</span>
</code></pre></div></div>

<p>The approach is the similar to the per-CPU counter, except that we keep one counter per thread, using <code class="language-plaintext highlighter-rouge">thread_local</code>. Unlike earlier implementations, you don’t create instances of this class: there is only one counter and you increment it by calling the static method <code class="language-plaintext highlighter-rouge">tls_counter::increment()</code>.</p>

<p>Let’s focus a moment on the actual increment inside the thread-local counter instance:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">incr</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">cur</span> <span class="o">=</span> <span class="n">counter</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
    <span class="n">counter</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">cur</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">memory_order_relaxed</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This is just a verbose way of saying “add 1 to this <code class="language-plaintext highlighter-rouge">std::atomic&lt;uint64_t&gt;</code> but it doesn’t have to be atomic”. We don’t need an atomic increment as there is only one writer<sup id="fnref:whyatomic" role="doc-noteref"><a href="#fn:whyatomic" class="footnote" rel="footnote">32</a></sup>. Using the <em>relaxed</em> memory order means that no barriers are inserted<sup id="fnref:barrier" role="doc-noteref"><a href="#fn:barrier" class="footnote" rel="footnote">33</a></sup>. We still need a way to read all the thread-local counters, and the rest of the code deals with that: there is a global vector of pointers to all the active <code class="language-plaintext highlighter-rouge">tls_counter</code> objects, and <code class="language-plaintext highlighter-rouge">read()</code> iterates over this. All access to this vector is protected by a <code class="language-plaintext highlighter-rouge">std::mutex</code>, since it will be accessed concurrently. When threads die, we remove their entry from the array, and add their final value to <code class="language-plaintext highlighter-rouge">tls_counter::accumulator</code> which is added to the sum of active counters in <code class="language-plaintext highlighter-rouge">read()</code>.</p>

<p>Whew.</p>

<p>So how does this <abbr title="Uses thread-local storage for a counter per thread.">tls add</abbr> implementation benchmark?</p>

<div class="tabs" id="tabs-tls">
    <!-- Courtesy of https://codepen.io/Merri/pen/bytea -->
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-tls-1" name="tab-group-tls" checked="" />
      <label class="tab-label" for="tab-tls-1">Skylake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/skl/tls.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/skl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/skl/tls.html">
        <img class="figimg" src="/assets/concurrency-costs/skl/tls.svg" alt="Increment Cost: Thread Local Storage" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-tls-2" name="tab-group-tls" />
      <label class="tab-label" for="tab-tls-2">Ice Lake</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/icl/tls.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/icl/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/icl/tls.html">
        <img class="figimg" src="/assets/concurrency-costs/icl/tls.svg" alt="Increment Cost: Thread Local Storage" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-tls-3" name="tab-group-tls" />
      <label class="tab-label" for="tab-tls-3">Graviton</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g1-16/tls.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g1-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g1-16/tls.html">
        <img class="figimg" src="/assets/concurrency-costs/g1-16/tls.svg" alt="Increment Cost: Thread Local Storage" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
    <div class="tab">
      <input class="tab-radio" type="radio" id="tab-tls-4" name="tab-group-tls" />
      <label class="tab-label" for="tab-tls-4">Graviton 2</label>
      <div class="tab-panel">
        <div class="tab-content">
          
          
          





<div class="svg-fig">
    <div class="svg-fig-links">
        
        
            <a href="/misc/tables/concurrency-costs/g2-16/tls.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/tree/master/results/g2-16/combined.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables/concurrency-costs/g2-16/tls.html">
        <img class="figimg" src="/assets/concurrency-costs/g2-16/tls.svg" alt="Increment Cost: Thread Local Storage" width="648" height="432" />
    </a>
</div>

        </div>
      </div>
    </div>
    
</div>

<p>That’s two nanoseconds per increment, regardless of the number of active cores. This turns out to be exactly as fast as just incrementing a variable in memory with a single instruction like <code class="language-plaintext highlighter-rouge">inc [eax]</code> or <code class="language-plaintext highlighter-rouge">add [eax], 1</code>, so it’s somehow as fast as possible for any solution which ends up incrementing something in memory<sup id="fnref:whitelie" role="doc-noteref"><a href="#fn:whitelie" class="footnote" rel="footnote">34</a></sup>.</p>

<p>Let’s take a look at the number of atomics, total instructions and performance for the three implementations in the last plot, for four threads:</p>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th style="text-align: right">Atomics</th>
      <th style="text-align: right">Instructions</th>
      <th style="text-align: right">Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">~ 110 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses a CAS on an adatively per-CPU counter.">cas multi</abbr></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">~ 12 ns</td>
    </tr>
    <tr>
      <td><abbr title="Uses thread-local storage for a counter per thread.">tls add</abbr></td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">~ 2 ns</td>
    </tr>
  </tbody>
</table>

<p>This is a clear indication that the difference in performance has very little to do with the number of instructions: the ranking by instruction count is exactly the reverse of the ranking by performance! <abbr title="Uses thread-local storage for a counter per thread.">tls add</abbr> has three times the number of instructions, yet is more than <em>fifty times</em> faster (so the <abbr title="Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.">IPC</abbr> varies by a factor of more than 150x).</p>

<p>As we saw at the last 1, this improvement in performance doesn’t come for free:</p>

<ul>
  <li>The total code size is considerably larger than the per-CPU approach, although most of it is related to creation of the initial object on each thread, and not on the hot path.</li>
  <li>We have one object per thread, instead of per CPU. For an application with many threads using the counter, this may mean the creation of many individual counters which use both more memory<sup id="fnref:tlsmem" role="doc-noteref"><a href="#fn:tlsmem" class="footnote" rel="footnote">35</a></sup> and result in a slower <code class="language-plaintext highlighter-rouge">read()</code> function.</li>
  <li>This implementation only supports <em>one</em> counter: the key methods in <code class="language-plaintext highlighter-rouge">tls_counter</code> are static. This boils down to the need for a <code class="language-plaintext highlighter-rouge">thread_local</code> object for the physical counter, which must be static by the rules of C++. A template parameter could be added to allow multiple counters based on dummy types used as tags, but this is still more awkward to use than instances of a class (and some platforms <a href="https://docs.microsoft.com/en-us/windows/win32/procthread/thread-local-storage">have limits</a> on the number of <code class="language-plaintext highlighter-rouge">thread_local</code> variables). This limitation could be removed in the same way as discussed earlier for the <abbr title="Uses a CAS on an adatively per-CPU counter.">cas multi</abbr> <code class="language-plaintext highlighter-rouge">idx</code> variable, but at a cost in performance and complexity.</li>
  <li>A lock was introduced to protect the array of all counters. Although the important increment operation is still lock-free, things like the <code class="language-plaintext highlighter-rouge">read()</code> call, the first counter access on a given thread and thread destruction all compete for the same lock. This could be eased with a read-write lock or a concurrent data structure, but at a cost as always.</li>
</ul>

<h2 id="the-table">The Table</h2>

<style>
.yesno {
    display: inline-block;
    border-radius: 3px;
    min-width: 25px;
    text-align: center;
}
.yes {
    background-color: #070;
    padding: 3px;
}
.no {
    background-color: orangered;
    padding: 3px 5px;
}
</style>

<p>Let’s summarize all the levels in this table.</p>

<p>The <em>~Cost</em> column is a <em>very</em> approximate estimate of the cost of each “occurrence” of the expensive operation associated with the level. It should be taken as a very rough ballpark for current Intel and AMD hardware, but especially the later levels can vary a lot.</p>

<p>The <em>Perf Event</em> column lists a Linux <code class="language-plaintext highlighter-rouge">perf</code> event that you can use to count the number of times the operation associated with this level occurs, i.e., the thing that is slow. For example, in level 1, you count atomic operations using the <code class="language-plaintext highlighter-rouge">mem_inst_retired.lock_loads</code> counter, and if you get three counts per high level operation, you can expect roughly 3 x 10 ns = 30 ns cost. Of course, you don’t necessarily need perf in this case: you can inspect the assembly too.</p>

<p>The <em>Local</em> column records whether the behavior of this level is <em>core local</em>. If yes, it means that operations on different cores complete independently and don’t compete and so the performance scales with the number of cores. If not, there is contention or serialization, so the throughput of the entire system is often limited, regardless of how many cores are involved. For example, only one core at a time performs an atomic operation on a cache line, so the throughput of the whole system is fixed and the throughput per core decreases as more cores become involved.</p>

<p>The <em>Key Characteristic</em> tries to get across the idea of the level in one bit-sized chunk.</p>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Name</th>
      <th style="text-align: right">~Cost (ns)</th>
      <th style="text-align: center">Perf Event</th>
      <th style="text-align: center">Local</th>
      <th>Key Characteristic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Vanilla</td>
      <td style="text-align: right">low</td>
      <td style="text-align: center">depends</td>
      <td style="text-align: center"><strong class="yes yesno">Yes</strong></td>
      <td>No atomic instructions or contended accesses at all</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Uncontended Atomic</td>
      <td style="text-align: right">10</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">mem_inst_retired.</code> <code class="language-plaintext highlighter-rouge">lock_loads</code></td>
      <td style="text-align: center"><strong class="yes yesno">Yes</strong></td>
      <td>Atomic instructions without contention</td>
    </tr>
    <tr>
      <td>2</td>
      <td>True Sharing</td>
      <td style="text-align: right">40 - 400</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">mem_load_l3_hit_retired.</code> <code class="language-plaintext highlighter-rouge">xsnp_hitm</code></td>
      <td style="text-align: center"><strong class="no yesno">No</strong></td>
      <td>Contended atomics or locks</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Syscall</td>
      <td style="text-align: right">1,000</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">raw_syscalls:sys_enter</code></td>
      <td style="text-align: center"><strong class="no yesno">No</strong></td>
      <td>System call</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Context Switch</td>
      <td style="text-align: right">10,000</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">context-switches</code></td>
      <td style="text-align: center"><strong class="no yesno">No</strong></td>
      <td>Forced context switch</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Catastrophe</td>
      <td style="text-align: right">huge</td>
      <td style="text-align: center">depends</td>
      <td style="text-align: center"><strong class="no yesno">No</strong></td>
      <td>Stalls until quantum exhausted, or other sadness</td>
    </tr>
  </tbody>
</table>

<h2 id="so-what">So What?</h2>

<p>What’s the point of all this?</p>

<p>Primarily, I use the hierarchy as a simplification mechanism when thinking about concurrency and performance. As a first order approximation <em>you mostly only need to care about the operations related to the current level</em>. That is, if you are focusing on something which has contended atomic operations (level 2), you don’t need to worry too much about uncontended atomics or instruction counts: just focus on reducing contention. Similarly, if you are at level 1 (uncontended atomics) it is often worth using <em>more</em> instructions to reduce the number of atomics.</p>

<p>This guideline only goes so far: if you have to add 100 instructions to remove one atomic, it is probably not worth it.</p>

<p>Second, when optimizing a concurrent system I always try to consider how I can get to a (numerically) lower level. Can I remove the last atomic? Can I avoid contention? Successfully moving to a lower level can often provide an order-of-magnitude boost to performance, so it should be attempted first, before finer-grained optimizations within the current level. Don’t spend forever optimizing your contended lock, if there’s some way to get rid of the contention entirely.</p>

<p>Of course, this is not always possible, or not possible without tradeoffs you are unwilling to make.</p>

<h3 id="getting-there">Getting There</h3>

<p>Here’s a quick look at some usual and unusual ways of achieving levels lower on the hierarchy.</p>

<h4 id="level-4">Level 4</h4>

<p>You probably don’t want to really be in level 4 but it’s certainly better than level 5. So, if you still have your job and your users haven’t all abandoned you, it’s usually pretty easy to get out of level 5. More than half the battle is just recognizing what’s going on and from there the solution is often clear. Many times, you’ve simply violated some rule like “don’t use pure spinlocks in userspace” or “you built a spinlock by accident” or “so-and-so accidentally held that core lock during IO”. There’s almost never any inherent reason you’d need to stay in level 5 and you can usually find an almost tradeoff-free fix.</p>

<p>A better approach than targeting level 4 is just to skip to level 2, since that’s usually not too difficult.</p>

<h4 id="level-3">Level 3</h4>

<p>Getting to level 3 just means solving the underlying reason for so many context switches. In the example used in this post, it means giving up fairness. Other approaches include not using threads for small work units, using smarter thread pools, not oversubscribing cores, and keeping locked regions short.</p>

<p>You don’t usually really want to be in level 3 though: just skip right to level 2.</p>

<h4 id="level-2">Level 2</h4>

<p>Level 3 isn’t a <em>terrible</em> place to be, but you’ll always have that gnawing in your stomach that you’re leaving a 10x speedup on the table. You just need to get rid of that system call or context switch, bringing you to level 2.</p>

<p>Most library provided concurrency primitives already avoid system calls on the happy path. E.g., pthreads mutex, <code class="language-plaintext highlighter-rouge">std::mutex</code>, Windows <code class="language-plaintext highlighter-rouge">CRITICAL_SECTION</code> will avoid a system call while acquiring and releasing an uncontended lock. There are, however, some notable exceptions: if you are using a <a href="https://docs.microsoft.com/en-us/windows/win32/sync/mutex-objects">Win32 mutex object</a> or <a href="https://man7.org/linux/man-pages/man2/semop.2.html">System V semaphore</a> object, you are paying a system call on every operation. Double check if you can use an in-process alternative in this case.</p>

<p>For more general synchronization purposes which don’t fit the lock-unlock pattern, a condition variable often fits the bill and a quality implementation generally avoids system calls on the fast path. A relatively unknown and higher performance alternative to condition variables, especially suitable for coordinating blocking for otherwise lock-free structures, is an <a href="http://pvk.ca/Blog/2019/01/09/preemption-is-gc-for-memory-reordering/#event-counts-with-x86-tso-and-futexes"><em>event count</em></a>. Paul’s implementation is <a href="https://github.com/concurrencykit/ck/blob/master/include/ck_ec.h">available in concurrency kit</a> and we’ll mention it again at Level 0.</p>

<p>System calls often creep in when home-grown synchronization solutions are used, e.g., using Windows events to build your own read-write lock or striped lock or whatever the flavor of the day is. You can often remove the call in the fast path by making a check in user-space to see if a system call is necessary. For example, rather than unconditionally unblocking any waiters when releasing some exclusive object, <em>check</em> to see if there are waiters<sup id="fnref:tricky" role="doc-noteref"><a href="#fn:tricky" class="footnote" rel="footnote">36</a></sup> in userspace and skip the system call if there are none.</p>

<p>If a lock is generally held for a short period, you can avoid unnecessary system calls and context switches with a hybrid lock that spins for an appropriate<sup id="fnref:spin" role="doc-noteref"><a href="#fn:spin" class="footnote" rel="footnote">37</a></sup> amount of time before blocking. This can trade tens of nanoseconds of spinning for hundreds or thousands of nanoseconds of system calls.</p>

<p>Ensure your use of threads is “right sized” as much as possible. A lot of unnecessary context switches occur when many more threads are running than there are CPUs, and this increases the chance of a lock being held during a context switch (and makes it worse when it does happen: it takes longer for the holding thread to run again as the scheduler probably cycles through all the other runnable threads first).</p>

<h4 id="level-1">Level 1</h4>

<p>A lot of code that does the work to get to level 2 actually ends up in level 1. Recall that the primary difference between level 1 and 2 is the lack of contention in level 1. So if your process naturally or by design has low contention, simply using existing off-the-shelf synchronization like <code class="language-plaintext highlighter-rouge">std::mutex</code> can get you to level 1.</p>

<p>I can’t give a step-by-step recipe for reducing contention, but here’s a laundry list of things to consider:</p>

<ul>
  <li>Keep your critical sections as short as possible. Ensure you do any heavy work that doesn’t directly involve a shared resource outside of the critical section. Sometimes this means making a copy of the shared data to work on it “outside” of the lock, which might increase the total amount of work done, but reduce contention.</li>
  <li>For things like atomic counters, try to batch your updates: e.g., if you update the counter multiple times during some operation, update a local on the stack rather than the global counter and only “upload” the entire value once at the end.</li>
  <li>Consider using structures that use fine-grained locks, striped locks or similar mechanisms that reduce contention by locking only portions of a container.</li>
  <li>Consider per-CPU structures, as in the examples above, or some approximation of them (e.g., hashing the current thread ID into an array of structures). This post used an atomic counter as a simple example, but it applies more generally to any case where the mutations can be done independently and aggregated later.</li>
</ul>

<p>For all of the advice above, when I say <em>consider doing X</em> I really mean <em>consider finding and using an existing off-the shelf component that does X</em>. Writing concurrent structures yourself should be considered a last resort – despite what you think, your use case is probably not all that unique.</p>

<p>Level 1 is where a lot of well written, straightforward and high-performing concurrent code lives. There is nothing wrong with this level – it is a happy place.</p>

<h4 id="level-0">Level 0</h4>

<p>It is not always easy or possible to remove the last atomic access from your fast paths, but if you just can’t live with the extra ~10 ns, here are some options:</p>

<ul>
  <li>The general approach of using thread local storage, as discussed above, can also be extended to structures more complicated than counters.</li>
  <li>You may be able to achieve fewer than one expensive atomic instruction per logical operation by <em>batching:</em> saving up multiple operations and then committing them at all once with a small fixed number of atomic operations. Some containers or concurrent structures may have a batched API which does this for you, but even if not you can sometimes add batching yourself, e.g., by inserting collections of elements rather than a single element<sup id="fnref:hiddenbatch" role="doc-noteref"><a href="#fn:hiddenbatch" class="footnote" rel="footnote">38</a></sup>.</li>
  <li>Many lock-free structures offer atomic-free <em>read</em> paths, notably concurrent containers in garbage collected languages, such as <code class="language-plaintext highlighter-rouge">ConcurrentHashMap</code> in Java. Languages without garbage collection have fewer straightforward options, mostly because safe memory reclamation is a <a href="http://concurrencyfreaks.blogspot.com/2017/08/why-is-memory-reclamation-so-important.html">hard problem</a>, but there are still <a href="http://concurrencykit.org/">some</a> <a href="https://software.intel.com/content/www/us/en/develop/documentation/tbb-documentation/top/intel-threading-building-blocks-developer-guide/containers.html">good</a> <a href="https://github.com/facebook/folly/tree/master/folly/concurrency">options</a> out there.</li>
  <li>I find that <a href="https://liburcu.org/">RCU</a> is especially powerful and fairly general if you are using a garbage collected language, or can satisfy the requirements for an efficient reclamation method in a non-GC language.</li>
  <li>The <a href="https://en.wikipedia.org/wiki/Seqlock">seqlock</a><sup id="fnref:despite" role="doc-noteref"><a href="#fn:despite" class="footnote" rel="footnote">39</a></sup> is an underrated and little known alternative to RCU without reclaim problems, although not as general. Concurrencykit has <a href="http://concurrencykit.org/doc/ck_sequence.html">an implementation</a>. It has an atomic-free read path for readers. Unfortunately, seqlocks don’t integrate cleanly with either the Java<sup id="fnref:stampedlock" role="doc-noteref"><a href="#fn:stampedlock" class="footnote" rel="footnote">40</a></sup> or <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1478r1.html">C++</a> memory models.</li>
  <li>It is also possible in some cases to do a per-CPU rather than a per-thread approach using only vanilla instructions, although the possibility of interruption at any point makes this tricky. <a href="https://www.efficios.com/blog/2019/02/08/linux-restartable-sequences/">Restartable sequences (rseq)</a> can help, and there are other tricks lurking out there.</li>
  <li>Event counts, mentioned earlier, <a href="https://pvk.ca/Blog/2019/01/09/preemption-is-gc-for-memory-reordering/#event-counts-with-x86-tso-and-futexes:~:text=However%2C%20if%20we%20go">can even be level 0</a> in a single writer scenario, as Paul shows.</li>
  <li>This is the last point, but it should be the first: you can probably often redesign your algorithm or application to avoid sharing data in the first place, or to share much less. For example, rather than constantly updating a shared collection with intermediate results, do as much private computation as possible before only merging the final results.</li>
</ul>

<h3 id="summary">Summary</h3>

<p>We looked at the six different levels that make up this concurrency cost hierarchy. The slow half (3, 4 and 5) are all basically performance bugs. You should be able to achieve level 2 or level 1 (if you naturally have low contention) for most designs fairly easily and those are probably what you should target by default. Level 1 in a contended scenario and level 0 are harder to achieve and often come with difficult tradeoffs, but the performance boost can be significant: often one or more orders of magnitude.</p>

<h3 id="thanks">Thanks</h3>

<p>Thanks to Paul Khuong who <a href="https://pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers">showed me something</a> that made me reconsider in what scenarios level 0 is achievable and typo fixes.</p>

<p>Thanks to <a href="https://twitter.com/never_released">@never_released</a> for help on a problem I had bringing up an EC2 bare-metal instance (tip: just wait).</p>

<p>Special thanks to <a href="https://twitter.com/matt_dz">matt_dz</a> and Zach Wenger for helping fix about <em>sixty</em> typos between them.</p>

<p>Thanks to Alexander Monakov, Dave Andersen, Laurent and Kriau for reporting typos, and Aaron Jacobs for suggesting clarifications to the level 0 definition.</p>

<p>Traffic light photo by <a href="https://unsplash.com/@harshaldesai">Harshal Desai</a> on <a href="https://unsplash.com/s/photos/traffic-light">Unsplash</a>.</p>

<h3 id="discussion-and-feedback">Discussion and Feedback</h3>

<p>You can leave a <a href="#comment-section">comment below</a> or discuss on <a href="https://news.ycombinator.com/item?id=23749172">Hacker News</a>, <a href="https://www.reddit.com/r/programming/comments/hma5y1/a_concurrency_cost_hierarchy/">r/programming</a> or <a href="https://www.reddit.com/r/cpp/comments/hmaocb/a_concurrency_cost_hierarchy/">r/cpp</a>.</p>

<p class="info">If you liked this post, check out the <a href="/">homepage</a> for others you might enjoy.</p>

<hr />
<hr />
<p><br /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:realworld" role="doc-endnote">
      <p>Well, this is quite real world: such atomic counters are used widely for a variety of purposes. I throw the <em>if you squint</em> in there because, after all, we are using microbenchmarks which simulate a probably-unrealistic density of increments to this counter, and it is a <em>bit</em> of a stretch to make this one example span all five levels – but I tried! <a href="#fnref:realworld" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:g2cores" role="doc-endnote">
      <p>The Graviton 2 bare metal hardware has 64 cores, but this instance size makes 16 of them available. This means that in principle the results can be affected by the coherency traffic of other tenants on the same hardware, but the relatively stable results seem to indicate it doesn’t affect the results much. <a href="#fnref:g2cores" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:parisc" role="doc-endnote">
      <p>Some hardware supports very limited atomic operations, which may be mostly useful <em>only</em> for locking, although you can <a href="https://parisc.wiki.kernel.org/index.php/FutexImplementation">get tricky</a>. <a href="#fnref:parisc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:post" role="doc-endnote">
      <p>We could actually use either the pre or post-increment version of the operator here. The usual advice is to prefer the pre-increment form <code class="language-plaintext highlighter-rouge">++c</code> as it can be faster as it can return the mutated value, rather than making a copy to return after the mutation. Now this advice rarely applies to primitive values, but atomic increment is actually an interesting case which turns it on its head: the post-increment version is probably better (at least, never slower) since the underlying hardware operation returns the previous value. So it’s <a href="https://godbolt.org/z/p4TDjX">at least one extra operation</a> to calculate the pre-increment value (or much worse, apparently, if icc gets involved). <a href="#fnref:post" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:atomicsup" role="doc-endnote">
      <p>Many ISAs, including POWER and ARM, traditionally only included support for a <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>-like or <a href="https://en.wikipedia.org/wiki/Load-link/store-conditional">LL/SC</a> operation, without specific support for more specific atomic arithmetic operations. The idea, I think, was that you could build any operation you want on top of of these primitives, at the cost of “only” a small retry loop and that’s more RISC-y, right? This seems to be changing as ARMv8.1 got a bunch of atomic operations. <a href="#fnref:atomicsup" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:java" role="doc-endnote">
      <p>From its introduction through Java 7, the <code class="language-plaintext highlighter-rouge">AtomicInteger</code> and related classes in Java implemented all their atomic operations on top of a <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> loop, as <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> was the only primitive implemented as an intrinsic. In Java 8, almost exactly a decade later, these were finally replaced with dedicated atomic RMWs where possible, with <a href="http://ashkrit.blogspot.com/2014/02/atomicinteger-java-7-vs-java-8.html">good results</a>. <a href="#fnref:java" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:l3" role="doc-endnote">
      <p>On my system and most (all?) modern Intel systems this is essentially the L3 cache, as the caching home agent (CHA) lives in or adjacent to the L3 cache. <a href="#fnref:l3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:inter" role="doc-endnote">
      <p>This doesn’t imply that each atomic operation needs to take 70 cycles under contention: a single core could do <em>multiple</em> operations on the cache line after it gains exclusive ownership, so the cost of obtaining the line could be amortized over all of these operations. How much of this occurs is a measure of fairness: a very fair CPU will not let any core monopolize the line for long, but this makes highly concurrent benchmarks like this slower. Recent Intel CPUs seem quite fair in this sense. <a href="#fnref:inter" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notwhat" role="doc-endnote">
      <p>It also might not <a href="https://www.realworldtech.com/forum/?threadid=189711&amp;curpostid=189752">work how you think</a>, depending on details of the OS scheduler. <a href="#fnref:notwhat" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:spectre" role="doc-endnote">
      <p>They used to be cheaper: based on my measurements the cost of system calls has more than doubled, on most Intel hardware, after the Spectre and Meltdown mitigations have been applied. <a href="#fnref:spectre" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hyst" role="doc-endnote">
      <p>An interesting thing about convoys is that they exhibit hysteresis: once you start having a convoy, they become self-sustaining, even if the conditions that started it are removed. Imagine two threads that lock a common lock for 1 nanosecond every 10,000 nanoseconds. Contention is low: the chance of any particular lock acquisition being contended is 0.01%. However, as soon as a contended acquisition occurs, the lock effectively becomes held for the amount of time it takes to do a full context switch (for the losing thread to block, and then to wake up). If that’s longer than 10,000 nanoseconds, the convoy will sustain itself indefinitely, until something happens to break the loop (e.g., one thread deciding to work on something else). A restart also “fixes” it, which is one of many possible explanations for processes that suddenly shoot to 100% CPU (but are still making progress), but can be fixed by a restart. Everything becomes worse with more than two threads, too. <a href="#fnref:hyst" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:huh" role="doc-endnote">
      <p>Actually I find it remarkable that this performs about as well as the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>-based <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>, since the fairness necessarily implies that the lock is acquired in a round-robin order, so the cache line with the lock must at a minimum move around to each acquiring thread. This is a real stress test of the arbitrary coherency mechanisms offered by the CPU. <a href="#fnref:huh" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:noht" role="doc-endnote">
      <p>And no SMT enabled, so there are 4 logical processors from the point of view of the OS. <a href="#fnref:noht" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:once" role="doc-endnote">
      <p>In fact, the <em>once</em> scenario is the most likely, since one would assume with homogeneous threads the scheduler will approximate something like round-robin scheduling. So the thread that is descheduled is most likely the one that is also closest to the head of the lock queue, because it had been spinning the longest. <a href="#fnref:once" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:perf" role="doc-endnote">
      <p>In fact, you can measure this with <code class="language-plaintext highlighter-rouge">perf</code> and see that the total number of context switches is usually within a factor of 2 for both tests, when oversubscribed. <a href="#fnref:perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:csdepend" role="doc-endnote">
      <p>There is another level of complication here: the convoy only gets set up when the fifth thread joins the fun <em>if</em> the thread that gets switched out had expressed interest in the lock before it lost the CPU. That is, after a thread unlocks the lock, there is a period before it gets a new ticket as it tries to obtain the lock again. Before it gets that ticket, it is essentially invisible to the other threads, and if it gets context switched out, the catastrophic convoy won’t be set up (because the new set of four threads will be able to efficiently share the lock among themselves). <a href="#fnref:csdepend" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notexx" role="doc-endnote">
      <p>This won’t always <em>necessarily</em> be the case. You could write a primitive that always makes a system call, putting it at level 3, even if there is no contention, but here I’ve made sure to always have a no-syscall fast path for the no-contention case. <a href="#fnref:notexx" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:atomhow" role="doc-endnote">
      <p>On Intel hardware you can use <a href="https://github.com/travisdowns/concurrency-hierarchy-bench/blob/master/scripts/details.sh">details.sh</a> to collect the atomic instruction count easily, taking advantage of the <code class="language-plaintext highlighter-rouge">mem_inst_retired.lock_loads</code> performance counter. <a href="#fnref:atomhow" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:casworse" role="doc-endnote">
      <p>The <abbr title="Uses a CAS loop to increment a single shared counter.">cas add</abbr> implementation comes off looking slightly worse than the other single-atomic implementations here because the load required to set up the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> value effectively adds to the dependency chain involving the atomic operation, which explains the 5-cycle difference with <abbr title="Uses an atomic increment on a single shared counter.">atomic add</abbr>. This goes away if you can do a <em>blind <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr></em> (e.g., in locks’ acquire paths), but that’s not possible here. <a href="#fnref:casworse" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:twolayer" role="doc-endnote">
      <p>Actually three layers, <a href="https://github.com/gcc-mirror/gcc/blob/4ff685a8705e8ee55fa86e75afb769ffb0975aea/libstdc%2B%2B-v3/include/bits/std_mutex.h#L98">libstdc++</a>, then <a href="https://github.com/gcc-mirror/gcc/blob/4ff685a8705e8ee55fa86e75afb769ffb0975aea/libgcc/gthr-posix.h#L775">libgcc</a> and then finally pthreads. I’ll count the first two as one though because those can all inline into the caller. Based on a rough accounting, probably 75% of the instruction count comes from pthreads, the rest from the other two layers. The pthreads mutexes are more general purpose than what <code class="language-plaintext highlighter-rouge">std::mutex</code> offers (e.g., they support recursion), and the features are configured at runtime on a per-mutex basis, so that explains a lot of the additional work these functions are doing. It’s only due to cost of atomic operations that <code class="language-plaintext highlighter-rouge">std::mutex</code> doesn’t take a significant penalty compared to a more svelte design. <a href="#fnref:twolayer" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:padded" role="doc-endnote">
      <p><em>Padded</em> means that the counters are aligned such that each falls into its own 64 byte cache line, to avoid <a href="https://en.wikipedia.org/wiki/False_sharing"><em>false sharing</em></a>. This means that even though each counter only has 8 bytes of logical payload, it requires 64 bytes of storage. Some people claim that you need to pad out to 128 bytes, not 64, to avoid the effect of the <em>adjacent line prefetcher</em> which fetches the 64-byte that completes an aligned 128-byte pair of lines. However, I have not observed this effect often on modern CPUs. Maybe the prefetcher is conservative and doesn’t trigger unless past behavior indicates the fetches are likely to be used, or the prefetch logic can detect and avoid cases of false sharing (e.g., by noticing when prefetched lines are subsequently invalidated by a snoop). <a href="#fnref:padded" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:notallfailure" role="doc-endnote">
      <p>Actually, not <em>all</em> failure indicates contention: there is a small chance also that a context switch exactly splits the load and the subsequent <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr>, and in this case the <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> would fail when the thread was scheduled again if any thread that ran in the meantime updated the same counter. Treating this as contention doesn’t really cause any serious problems. <a href="#fnref:notallfailure" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:perfsame" role="doc-endnote">
      <p>Not surprising, since there is no contention and the fast path looks the same for either algorithm: a single <abbr title="Compare-and-swap: an atomic operation implemented on x86 and other CPUs.">CAS</abbr> that always succeeds. <a href="#fnref:perfsame" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:eightbyte" role="doc-endnote">
      <p>Here I’m assuming that <code class="language-plaintext highlighter-rouge">sizeof(std::atomic&lt;uint64_t&gt;)</code> is 8, and this is the case on all current mainstream platforms. Also, you may or may not want to pad out the single-counter version to 64 bytes as well, to avoid some <em>potential</em> false sharing with nearby values, but this is different than the multi-counter case where padding is obligatory to avoid guaranteed false sharing. <a href="#fnref:eightbyte" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:read" role="doc-endnote">
      <p>In this limited case, I <em>think</em> <code class="language-plaintext highlighter-rouge">read()</code> provides the same guarantees as the single-counter case. Informally, <code class="language-plaintext highlighter-rouge">read()</code> returns some value that the counter had at some point between the start and end of the <code class="language-plaintext highlighter-rouge">read()</code> call. Formally, there is a <em>linearization point</em> within <code class="language-plaintext highlighter-rouge">read()</code> although this point can only be determined in retrospect by examining the returned value (unlike the single-counter approaches, where the linearization is clear regardless of the value). However, <em>this is only true because the only mutating operation is <code class="language-plaintext highlighter-rouge">increment()</code></em>. If we also offered a <code class="language-plaintext highlighter-rouge">decrement()</code> method, this would no longer be true: you could read values that the logical counter never had based on the sequence of increments and decrements. Specifically, if you execute <code class="language-plaintext highlighter-rouge">increment(); decrement(); increment()</code> and even if you know these operations are strictly ordered (e.g., via locking), a concurrent call to <code class="language-plaintext highlighter-rouge">read()</code> could return <em>2</em>, even though the counter never logically exceeded 1. <a href="#fnref:read" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:sharedidx" role="doc-endnote">
      <p>In particular, if contention is seen on one object, the per-thread index will change to avoid it, which changes the index of all other objects as well, even if they have not seen any contention. This doesn’t seem like much of a problem for this simple implementation (which index we write to doesn’t matter much), but it could make some other optimizations more difficult: e.g., if we size the counter array dynamically, we don’t want to unnecessarily change the <code class="language-plaintext highlighter-rouge">idx</code> for uncontended objects, since it requires a larger counter array, unnecessarily. <a href="#fnref:sharedidx" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:rtcost" role="doc-endnote">
      <p>At least, an extra indirection to access the array which is no longer embedded in the object<sup id="fnref:soa" role="doc-noteref"><a href="#fn:soa" class="footnote" rel="footnote">41</a></sup>, and checks to ensure the array is large enough. Furthermore, we have another decision to make: when to expand the array. How much contention should we suffer before we decide the array is too small? <a href="#fnref:rtcost" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:subtle" role="doc-endnote">
      <p>The intuition is later counter positions only get written when an earlier position failed a compare and swap, which necessarily implies it was written to by some other thread and hence non-zero. There is some subtlety here: this wouldn’t hold if <code class="language-plaintext highlighter-rouge">compare_exchange_weak</code> was used instead of <code class="language-plaintext highlighter-rouge">compare_exchange_strong</code>, and it more obviously wouldn’t apply if we allowed decrements or wanted to change the “probe” strategy. <a href="#fnref:subtle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lesshot" role="doc-endnote">
      <p>I’m not sure if “less hot” means <code class="language-plaintext highlighter-rouge">__attribute__((cold))</code> necessarily, that might be <em>too</em> cold. We mostly just want to separate the first-cas-succeeds case and the rest of the logic so we don’t pay the dynamic code size impact except when the fallback path is taken. <a href="#fnref:lesshot" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dynamictls" role="doc-endnote">
      <p>A sketch of an implementation would be to use something like a single static <code class="language-plaintext highlighter-rouge">thread_local</code> pointer to an array or map, which maps an ID contained in the dynamic TLS key to the object data. Lookup speed is important, which favors an array, but you also need to be able to remove elements, which can favor some type of hash map. All of this is probably at least twice as slow as a plain <code class="language-plaintext highlighter-rouge">thread_local</code> access … or just use <a href="https://github.com/facebook/folly/blob/master/folly/docs/ThreadLocal.md">folly</a> or <a href="https://www.boost.org/doc/libs/1_73_0/doc/html/thread/thread_local_storage.html">boost</a>. <a href="#fnref:dynamictls" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:noatomic" role="doc-endnote">
      <p>On x86, what’s vanilla and what isn’t is fairly cut and dried: regular memory accesses and read-modify-write instructions are <em>vanilla</em> while LOCKed <abbr title="Read-modify-write: an instruction that reads from a memory location, operates on the value, and writes the result back to the same location.">RMW</abbr> instructions, whether explicit like <code class="language-plaintext highlighter-rouge">lock inc</code> or implicit like <a href="https://www.felixcloutier.com/x86/xchg"><code class="language-plaintext highlighter-rouge">xchg [mem], reg</code></a>, are not and are an order of magnitude slower. Out of the fences, <code class="language-plaintext highlighter-rouge">mfence</code> is also a slow non-vanilla instruction, comparable in cost to a LOCKed instruction. On other platforms like ARM or POWER, there may be shades of grey: you still have vanilla accesses on one end, and expensive full barriers like <code class="language-plaintext highlighter-rouge">dmb</code> on ARM or <code class="language-plaintext highlighter-rouge">sync</code> on POWER at the other, but you also have things in the middle with some additional ordering guarantees but still short of sequential consistency. This includes things like <code class="language-plaintext highlighter-rouge">LDAR</code> and <code class="language-plaintext highlighter-rouge">LDAPR</code> on ARM which implement sort of a sliding scale of load ordering and performance. Still, on any given hardware, you might find that instructions generally fall into a “cheap” (vanilla) and “expensive” (non-vanilla) bucket. <a href="#fnref:noatomic" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whyatomic" role="doc-endnote">
      <p>The only reason we even need <code class="language-plaintext highlighter-rouge">std::atomic&lt;uint64_t&gt;</code> at all is because it is <em>undefined behavior</em> to have concurrent access to any variable if at least one access is a write. Since the owning thread is making writes, this would <em>technically</em> be a violation of the standard if there was a concurrent <code class="language-plaintext highlighter-rouge">tls_counter::read()</code> call. Most actual hardware has no problem with concurrent reads and writes like this, but it’s better to stay on the right side of the law. Some hardware could also exhibit <em>tearing</em> of the writes, and <code class="language-plaintext highlighter-rouge">std::atomic</code> guarantees this doesn’t happen. That is, the read and write are still <em>individually</em> atomic. <a href="#fnref:whyatomic" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:barrier" role="doc-endnote">
      <p>If you use the default <code class="language-plaintext highlighter-rouge">std::memory_order_seq_cst</code>, on x86 gcc inserts an <code class="language-plaintext highlighter-rouge">mfence</code> which makes this <em>even slower than an atomic increment</em> since <code class="language-plaintext highlighter-rouge">mfence</code> is generally slower than instructions with a lock prefix (it has slightly stronger barrier semantics). <a href="#fnref:barrier" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:whitelie" role="doc-endnote">
      <p>This is a very small white lie. I’ll explain more elsewhere. <a href="#fnref:whitelie" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tlsmem" role="doc-endnote">
      <p>On the other hand, the TLS approach doesn’t need padding since the counters will generally appear next to other thread-local data, and not subject to false sharing, which means an 8x reduction (from 64 to 8 bytes) in the per-counter size, so if your process has a number of threads roughly equal to the number of cores, you will probably <em>save</em> memory over the per-CPU approach. <a href="#fnref:tlsmem" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tricky" role="doc-endnote">
      <p>It’s easy to introduce a missed wakeup problem if this isn’t done correctly. The usual cause is a race condition between some waiter arriving at a lock-like thing, seeing that it’s locked and then indicating interest, but in the critical region of that check-then-act the owning thread left the lock and didn’t see any waiters. The waiter blocks but there is nobody to unblock them. These bugs often go undetected since the situation resolves itself as soon as another thread arrives, so in a busy system you might not notice the temporarily hung threads. The <code class="language-plaintext highlighter-rouge">futex</code> system call is basically designed to make solving this easy, while the Event stuff in Windows requires a bit more work (usually based on a compare-and-swap). <a href="#fnref:tricky" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:spin" role="doc-endnote">
      <p>An “appropriate” time is probably something like the typical runtime of the locked region. Basically you want to spin in any case where the lock is held by a currently running thread, which will release it soon. As soon as you’ve been spinning for more than the typical hold time of the lock, it becomes much more likely you are simply waiting for a lock held by a thread that is <em>not</em> running (e.g., it was unlucky enough to incur a context switch while it held the lock). In that case, you are better off sleeping. <a href="#fnref:spin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hiddenbatch" role="doc-endnote">
      <p>An interesting design point is a data type that implements batching internally behind an API offering single-element operations. For example, a queue might decide that added elements won’t be immediately consumed (because there are already some elements in the queue), and hold them in a local staging area until several can be added as a batch, or until their absence would be noticed. <a href="#fnref:hiddenbatch" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:despite" role="doc-endnote">
      <p>Despite the current claim in wikipedia that seqlocks are somehow a Linux-specific construct involving the kernel, they work great in userspace only and are not tied to Linux. It is likely they were not invented for use in Linux but <a href="https://twitter.com/davidtgoldblatt/status/1280189008803278848">pre-dated</a> the OS, although maybe the use in Linux was where the name <em>seqlock</em> first appeared? <a href="#fnref:despite" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:stampedlock" role="doc-endnote">
      <p>Java does provide <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/locks/StampedLock.html">StampedLock</a> which offers seqlock functionality. <a href="#fnref:stampedlock" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:soa" role="doc-endnote">
      <p>Of course, we could go even <em>one step further</em> and embed a small array of 1 or 2 elements in the counter object, in the hope that this is enough and only use a dynamically allocated array and suffer the additional indirection if we observe contention. <a href="#fnref:soa" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- travis override -->
  
    <section class="comments" id="comment-section">
  <hr>
  
  <!-- Existing comments -->
  <div class="comments__existing">
    <h2>Comments</h2>
    
    
    <!-- List main comments in reverse date order, newest first. List replies in date order, oldest first. -->
    
    

<article id="comment-12b68530-c816-11ec-a58a-21cf651d99f4" class="js-comment comment" uid="12b68530-c816-11ec-a58a-21cf651d99f4">

  <div class="comment__author">
    Josh Scheid
    <span class="comment__date">•
        <a href="#comment-12b68530-c816-11ec-a58a-21cf651d99f4" title="Permalink to this comment">April 29th, 2022 23:42</a></span>
  </div>

  <div class="comment__body">
    <p>What was interesting and non-interesting (so that it didn’t make this post) about “plain_add”?</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-12b68530-c816-11ec-a58a-21cf651d99f4', 'respond', '12b68530-c816-11ec-a58a-21cf651d99f4')">↪&#xFE0E; Reply to Josh Scheid</a>
    </div>
</article>
  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-bd7e59c0-5a2b-11eb-9e5b-15b668946273" class="js-comment comment" uid="bd7e59c0-5a2b-11eb-9e5b-15b668946273">

  <div class="comment__author">
    Noah Goldstein
    <span class="comment__date">•
        <a href="#comment-bd7e59c0-5a2b-11eb-9e5b-15b668946273" title="Permalink to this comment">January 19th, 2021 07:55</a></span>
  </div>

  <div class="comment__body">
    <p>An interesting thing about restartable sequences is that the critical section only needs to be atomic s.t any interrupt to the processor at any time returning RIP to the abort handler needs to be graceful. On x86_64 for example this means your commit instruction could be an AVX2 store (so 32 byte atomic operation percpu which in itself is pretty cool). But there may be a way using Travis’ investigation of <a href="https://travisdowns.github.io/blog/2019/08/20/interrupts.html">where interrupts happen</a> to build a multi instruction commit if all the instructions retired at the same time.</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-bd7e59c0-5a2b-11eb-9e5b-15b668946273', 'respond', 'bd7e59c0-5a2b-11eb-9e5b-15b668946273')">↪&#xFE0E; Reply to Noah Goldstein</a>
    </div>
</article>
  

<article id="comment-02454660-61c8-11eb-b4e8-69e1fbbf9903" class="js-comment comment admin child" uid="02454660-61c8-11eb-b4e8-69e1fbbf9903">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-02454660-61c8-11eb-b4e8-69e1fbbf9903" title="Permalink to this comment">January 29th, 2021 00:22</a></span>
  </div>

  <div class="comment__body">
    <p>Yeah definitely! It’s a whole new way of thinking about atomic operations, since now you’re looking for instructions which aren’t necessarily atomic in the usual sense, but just do a lot of work in a single instruction. For example, you can use <code class="language-plaintext highlighter-rouge">movsq</code> to copy 64 bits from one place to another “instruction atomically” (in the rseq sense) which isn’t even possible with existing atomic instructions.</p>

<p>I wonder about instructions like <code class="language-plaintext highlighter-rouge">rep movsb</code> which have a <code class="language-plaintext highlighter-rouge">rep</code> prefix. These are “interruptible” in that they may partially execute: updating their to/from and count pointers. I suppose these aren’t supported as “commit” instructions for rseq since the implementation probably expects instructions to fully execute, or not.</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-3cfb4b30-3511-11eb-b90e-3d932abd6adb" class="js-comment comment" uid="3cfb4b30-3511-11eb-b90e-3d932abd6adb">

  <div class="comment__author">
    Chia-Lun Liu
    <span class="comment__date">•
        <a href="#comment-3cfb4b30-3511-11eb-b90e-3d932abd6adb" title="Permalink to this comment">December  3th, 2020 02:42</a></span>
  </div>

  <div class="comment__body">
    <p>Thanks for sharing. This is really nice.</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-3cfb4b30-3511-11eb-b90e-3d932abd6adb', 'respond', '3cfb4b30-3511-11eb-b90e-3d932abd6adb')">↪&#xFE0E; Reply to Chia-Lun Liu</a>
    </div>
</article>
  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-d00c9470-fac8-11ea-ab88-0be8ecc64f23" class="js-comment comment" uid="d00c9470-fac8-11ea-ab88-0be8ecc64f23">

  <div class="comment__author">
    Matthew Fernandez
    <span class="comment__date">•
        <a href="#comment-d00c9470-fac8-11ea-ab88-0be8ecc64f23" title="Permalink to this comment">September 19th, 2020 22:38</a></span>
  </div>

  <div class="comment__body">
    <blockquote>
  <p>I’m not sure if “less hot” means <strong>attribute</strong>((cold)) necessarily, that might be too cold.</p>
</blockquote>

<p>Maybe you could mark the initial CAS branch with <code class="language-plaintext highlighter-rouge">__builtin_expect(..., 1)</code>? Then again, I wouldn’t be surprised if compilers already consider a strong CAS as “likely.”</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-d00c9470-fac8-11ea-ab88-0be8ecc64f23', 'respond', 'd00c9470-fac8-11ea-ab88-0be8ecc64f23')">↪&#xFE0E; Reply to Matthew Fernandez</a>
    </div>
</article>
  

<article id="comment-21c5c9f0-ff6d-11ea-a6a7-5da3c03edee8" class="js-comment comment admin child" uid="21c5c9f0-ff6d-11ea-a6a7-5da3c03edee8">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-21c5c9f0-ff6d-11ea-a6a7-5da3c03edee8" title="Permalink to this comment">September 25th, 2020 20:24</a></span>
  </div>

  <div class="comment__body">
    <p>Indeed, I didn’t get any difference in the code generation with or without a likely hint: in both cases the compiler favored the success case (putting it as the fall through).</p>

<p>What I was suggesting was something along the lines of <a href="https://godbolt.org/z/b8qea9">this</a>. Here, the <code class="language-plaintext highlighter-rouge">inc2</code> version carves off the slow path into its own function (not visible, here) so that the slow path doesn’t add much to the inlined version of the increment function. Here the difference isn’t all that much since the entire slow path (the stuff at labels L2 and L9 in the gcc version) is fairly small, not much bigger than the code to call the slow path (L14 and L15 in the gcc version).</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-ee4b2fa0-cc56-11ea-b83c-554ea087b3be" class="js-comment comment" uid="ee4b2fa0-cc56-11ea-b83c-554ea087b3be">

  <div class="comment__author">
    Tuan Phuc PHAN
    <span class="comment__date">•
        <a href="#comment-ee4b2fa0-cc56-11ea-b83c-554ea087b3be" title="Permalink to this comment">July 22th, 2020 20:07</a></span>
  </div>

  <div class="comment__body">
    <p>Thank you for a great post</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-ee4b2fa0-cc56-11ea-b83c-554ea087b3be', 'respond', 'ee4b2fa0-cc56-11ea-b83c-554ea087b3be')">↪&#xFE0E; Reply to Tuan Phuc PHAN</a>
    </div>
</article>
  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-eec11b60-cc34-11ea-bae1-9d3e76ce4b30" class="js-comment comment" uid="eec11b60-cc34-11ea-bae1-9d3e76ce4b30">

  <div class="comment__author">
    Rene Damm
    <span class="comment__date">•
        <a href="#comment-eec11b60-cc34-11ea-bae1-9d3e76ce4b30" title="Permalink to this comment">July 22th, 2020 16:03</a></span>
  </div>

  <div class="comment__body">
    <p>Great post. Thank you for the detailed writeup.</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-eec11b60-cc34-11ea-bae1-9d3e76ce4b30', 'respond', 'eec11b60-cc34-11ea-bae1-9d3e76ce4b30')">↪&#xFE0E; Reply to Rene Damm</a>
    </div>
</article>
  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-66567800-c0c3-11ea-949b-e10c063dc6bd" class="js-comment comment" uid="66567800-c0c3-11ea-949b-e10c063dc6bd">

  <div class="comment__author">
    Darkmercy
    <span class="comment__date">•
        <a href="#comment-66567800-c0c3-11ea-949b-e10c063dc6bd" title="Permalink to this comment">July  8th, 2020 02:33</a></span>
  </div>

  <div class="comment__body">
    <p>Loads and stores are atomic so in CRDT “level 0” case why do you need synchronization at all?</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-66567800-c0c3-11ea-949b-e10c063dc6bd', 'respond', '66567800-c0c3-11ea-949b-e10c063dc6bd')">↪&#xFE0E; Reply to Darkmercy</a>
    </div>
</article>
  

<article id="comment-3fae9660-c0c8-11ea-949b-e10c063dc6bd" class="js-comment comment admin child" uid="3fae9660-c0c8-11ea-949b-e10c063dc6bd">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-3fae9660-c0c8-11ea-949b-e10c063dc6bd" title="Permalink to this comment">July  8th, 2020 03:08</a></span>
  </div>

  <div class="comment__body">
    <p>Do you mean why is <code class="language-plaintext highlighter-rouge">std::atomic</code> used at all for the TLS based approach? It is because there is still concurrent writers and readers, and at the C++ level this requires <code class="language-plaintext highlighter-rouge">std::atomic</code> even if you “know” reads and writes are atomic. More detail in <a href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html?fojosdfjss=x#fnref:whyatomic">this footnote</a>.</p>

<p>It doesn’t really add any overhead, because we “build our own” non-atomic increment, so the code ends up basically just as nice as if we went the UB route and used plain <code class="language-plaintext highlighter-rouge">uint64_t</code> without <code class="language-plaintext highlighter-rouge">std::atomic</code>.</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-e207f270-c05b-11ea-9c85-e34dac5bcfe2" class="js-comment comment" uid="e207f270-c05b-11ea-9c85-e34dac5bcfe2">

  <div class="comment__author">
    Jason
    <span class="comment__date">•
        <a href="#comment-e207f270-c05b-11ea-9c85-e34dac5bcfe2" title="Permalink to this comment">July  7th, 2020 14:12</a></span>
  </div>

  <div class="comment__body">
    <p>another question, do you think the overhead of ARM CPUs could partially come from virtualization?</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-e207f270-c05b-11ea-9c85-e34dac5bcfe2', 'respond', 'e207f270-c05b-11ea-9c85-e34dac5bcfe2')">↪&#xFE0E; Reply to Jason</a>
    </div>
</article>
  

<article id="comment-c7966a90-c0c7-11ea-949b-e10c063dc6bd" class="js-comment comment admin child" uid="c7966a90-c0c7-11ea-949b-e10c063dc6bd">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-c7966a90-c0c7-11ea-949b-e10c063dc6bd" title="Permalink to this comment">July  8th, 2020 03:04</a></span>
  </div>

  <div class="comment__body">
    <p>I am not seeing any particular overhead, at least for Graviton 2: the Graviton 2 results look fine.</p>

<p>Of course, the numbers for most of the non-scalable ones (level 2 and above) get worse the more CPUs you add, so 16 CPUs looks “worse” than 4, but that’s not fault of the CPU design.</p>

<p>I don’t think virtualization will cause any noticeable overhead here, at least for the majority of the tests. Most of these are “pure CPU” and virtualization generally runs those loads with basically zero overhead. Of course, this would be different if you were getting a fraction of a CPU (e.g., the hypervisor is time-slicing multiple guests onto one physical CPU), but that’s not the case for these EC2 instances.</p>

<p>What is possible is interference with other tenants in the same box, but I didn’t see much evidence of it.</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-36cbbc20-c05b-11ea-9c85-e34dac5bcfe2" class="js-comment comment" uid="36cbbc20-c05b-11ea-9c85-e34dac5bcfe2">

  <div class="comment__author">
    Jason
    <span class="comment__date">•
        <a href="#comment-36cbbc20-c05b-11ea-9c85-e34dac5bcfe2" title="Permalink to this comment">July  7th, 2020 14:07</a></span>
  </div>

  <div class="comment__body">
    <p>This is very helpful for someone working on atomics and lock right now. But one missing part is the benchmark of Intel xeon CPUs, which I believe is critical. I am picking two CPUs and running the same benchmark, if you are interested, I can send the results over.</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-36cbbc20-c05b-11ea-9c85-e34dac5bcfe2', 'respond', '36cbbc20-c05b-11ea-9c85-e34dac5bcfe2')">↪&#xFE0E; Reply to Jason</a>
    </div>
</article>
  

<article id="comment-6c978e70-61c7-11eb-b4e8-69e1fbbf9903" class="js-comment comment admin child" uid="6c978e70-61c7-11eb-b4e8-69e1fbbf9903">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-6c978e70-61c7-11eb-b4e8-69e1fbbf9903" title="Permalink to this comment">January 29th, 2021 00:17</a></span>
  </div>

  <div class="comment__body">
    <p>Sure, you can send them over.</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
  </div>
  

  <!-- New comment form -->
  <div id="respond" class="comment__new">
    <form class="js-form form" method="post" action="https://staticman-travisdownsio.herokuapp.com/v2/entry/travisdowns/travisdowns.github.io/master/comments">
  <input type="hidden" name="options[origin]" value="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html">
  <input type="hidden" name="options[parent]" value="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html">
  <input type="hidden" id="comment-replying-to-uid" name="fields[replying_to_uid]" value="">
  <input type="hidden" name="options[slug]" value="concurrency-costs">
  
  
  <div class="textfield">
    <label for="comment-form-message"><h2>Add Comment</h2>
      <textarea class="textfield__input" name="fields[message]" type="text" id="comment-form-message" placeholder="Your comment (markdown accepted)" required rows="6"></textarea>
    </label>
  </div>

    <div class="textfield narrowfield">
      <label for="comment-form-name">Name
        <input class="textfield__input" name="fields[name]" type="text" id="comment-form-name" placeholder="Your name (required)" required/>
      </label>
    </div>

    <div class="textfield narrowfield">
      <label for="comment-form-email">E-mail
        <input class="textfield__input" name="fields[email]" type="email" id="comment-form-email" placeholder="Your email (optional)"/>
      </label>
    </div>

    <div class="textfield narrowfield hp">
      <label for="hp">
        <input class="textfield__input" name="fields[hp]" id="hp" type="text" placeholder="Leave blank">
      </label>
    </div>

    

    <button type="button" class="button" id="cancel-comment-reply-link" style="display: none">
      Cancel Reply
    </button>
  
    <button class="button" id="comment-form-submit">
      Submit
    </button>

</form>

<article class="modal">
  <div>
    <h3 class="modal-title js-modal-title"></h3>
  </div>
  <div class="mdl-card__supporting-text js-modal-text"></div>
  <div class="mdl-card__actions mdl-card--border">
    <button class="button mdl-button--colored mdl-js-button mdl-js-ripple-effect js-close-modal">Close</button>
  </div>
</article>

<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="display:none" >
  <symbol id="icon-loading" viewBox="149.8 37.8 499.818 525"><path d="M557.8 187.8c13.8 0 24.601-10.8 24.601-24.6S571.6 138.6 557.8 138.6s-24.6 10.8-24.6 24.6c0 13.2 10.8 24.6 24.6 24.6zm61.2 90.6c-16.8 0-30.6 13.8-30.6 30.6s13.8 30.6 30.6 30.6 30.6-13.8 30.6-30.6c.6-16.8-13.2-30.6-30.6-30.6zm-61.2 145.2c-20.399 0-36.6 16.2-36.6 36.601 0 20.399 16.2 36.6 36.6 36.6 20.4 0 36.601-16.2 36.601-36.6C595 439.8 578.2 423.6 557.8 423.6zM409 476.4c-24 0-43.2 19.199-43.2 43.199s19.2 43.2 43.2 43.2 43.2-19.2 43.2-43.2S433 476.4 409 476.4zM260.8 411c-27 0-49.2 22.2-49.2 49.2s22.2 49.2 49.2 49.2 49.2-22.2 49.2-49.2-22.2-49.2-49.2-49.2zm-10.2-102c0-27.6-22.8-50.4-50.4-50.4-27.6 0-50.4 22.8-50.4 50.4 0 27.6 22.8 50.4 50.4 50.4 27.6 0 50.4-22.2 50.4-50.4zm10.2-199.8c-30 0-54 24-54 54s24 54 54 54 54-24 54-54-24.6-54-54-54zM409 37.8c-35.4 0-63.6 28.8-63.6 63.6S374.2 165 409 165s63.6-28.8 63.6-63.6-28.2-63.6-63.6-63.6z"/>
  </symbol>
</svg>



  </div>
</section>

<script src="/assets/main.js"></script>


  
  <!-- end override -->

  <a class="u-url" href="/blog/2020/07/06/concurrency-costs.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Travis Downs</li>
          <li><a class="u-email" href="mailto:travis.downs@gmail.com">travis.downs@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>A blog about low-level software and hardware performance.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/travisdowns" title="travisdowns"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/trav_downs" title="trav_downs"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
