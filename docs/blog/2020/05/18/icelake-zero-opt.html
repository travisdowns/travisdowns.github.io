<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="color-scheme" content="light dark">

<link rel="stylesheet" href="/assets/css/light.css">
<link rel="stylesheet" href="/assets/css/dark.css" media="(prefers-color-scheme: dark)">
<script>
  var DARKMODE = (function() {
    const i = {
    PROP: 'force-color',
    OID: 'override-style',
    BANNER: 'true',
    getOverride: function () {
      try {
        return localStorage.getItem(i.PROP);
      } catch (e) {
        return null;
      }
    },
    get: function () {
      try {
        var o = i.getOverride();
        if (o === 'dark' || (!o && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
          return 'dark';
        }
      } catch (e) {}
      return 'light';
    },
    gaTheme: function () {
      var o = i.getOverride();
      return o ? o + ' force' : i.get() + ' default';
    },
    makeLink: function (c) {
      e = document.createElement('link');
      e.id = i.OID;
      e.rel = 'stylesheet';
      e.href = (c === 'dark' ? '/assets/css/dark.css' : '/assets/css/light.css');
      return e;
    }
  }
  return i;
}());

if (DARKMODE.getOverride()) {
  var dm_color = DARKMODE.get();
  document.write(DARKMODE.makeLink(dm_color).outerHTML +
    '\n<meta name="theme-color" content="#' + (dm_color === 'dark' ? '181818' : 'fdfdfd') + '">');
}
</script>
<meta name="theme-color" content="#fdfdfd" media="not all and (prefers-color-scheme: dark)">
<meta name="theme-color" content="#181818" media="(prefers-color-scheme: dark)">
<script defer src="/assets/dark-mode.js"></script>

<!-- Begin Jekyll SEO tag v2.7.1p -->
<title>Ice Lake Store Elimination | Performance Matters</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Ice Lake Store Elimination" />
<meta name="author" content="Travis Downs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We look at the zero store optimization as it applies to Intel’s newest micro-architecture." />
<meta property="og:description" content="We look at the zero store optimization as it applies to Intel’s newest micro-architecture." />
<link rel="canonical" href="https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html" />
<meta property="og:url" content="https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html" />
<meta property="og:site_name" content="Performance Matters" />
<meta property="og:image" content="https://travisdowns.github.io/assets/intel-zero-opt/twitter-card-post2.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-18T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://travisdowns.github.io/assets/intel-zero-opt/twitter-card-post2.png" />
<meta property="twitter:title" content="Ice Lake Store Elimination" />
<meta name="twitter:site" content="@trav_downs" />
<meta name="twitter:creator" content="@trav_downs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Travis Downs"},"dateModified":"2020-05-18T00:00:00+00:00","datePublished":"2020-05-18T00:00:00+00:00","description":"We look at the zero store optimization as it applies to Intel’s newest micro-architecture.","headline":"Ice Lake Store Elimination","image":"https://travisdowns.github.io/assets/intel-zero-opt/twitter-card-post2.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://travisdowns.github.io/assets/rabbit3.png"},"name":"Travis Downs"},"url":"https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://travisdowns.github.io/feed.xml" title="Performance Matters" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-136594956-1"></script>
<script>
  window['ga-disable-UA-136594956-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('set', { 'custom_map': { 'dimension1': 'theme' } });
  gtag('config', 'UA-136594956-1', { 'theme' : DARKMODE.gaTheme(), 'cookie_flags': 'SameSite=None;Secure' });
</script>

</head>
<body>
  <header id="dm-header" class="dm-header hidden">
    <div class="dm-bar">
      <div class="wrapper">
        <label class="dm-checkbox"><span>Enable Dark Mode: </span><input type="checkbox" id="dm-select"/></label>
        <span onclick="DARKMODE.closeBar()" class='dm-close'>&times;</span>
      </div>
    </div>
    <div class="dm-spacer"></div>
  </header>
<header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Performance Matters</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/settings">Settings</a></div>
      </nav></div>
</header>
<main class="page-content invert-rotate-img" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Ice Lake Store Elimination</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-05-18T00:00:00+00:00" itemprop="datePublished">
        May 18, 2020
      </time><span>
          •
          <span class="tag-link"><a href="/tags/Intel.html">Intel</a></span><span class="tag-link"><a href="/tags/x86.html">x86</a></span><span class="tag-link"><a href="/tags/uarch.html">uarch</a></span><span class="tag-link"><a href="/tags/icelake.html">icelake</a></span>
        </span></p>
    <!-- end override -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    
<!-- boilerplate 
page.assets: /assets/intel-zero-opt
assetpath: /assets/intel-zero-opt
tablepath: /misc/tables//intel-zero-opt
-->

<h2 id="introduction">Introduction</h2>

<p>If you made it down to the <a href="/blog/2020/05/13/intel-zero-opt.html#hardware-survey">hardware survey</a> on the last post, you might have <a href="https://twitter.com/tarlinian/status/1260629853000265728">wondered</a> where Intel’s newest mainstream architecture was. <em>Ice Lake was missing!</em></p>

<p>Well good news: it’s here… and it’s interesting. We’ll jump right into the same analysis we did last time for Skylake client. If you haven’t read the <a href="/blog/2020/05/13/intel-zero-opt.html">first article</a> you’ll probably want to start there, because we’ll refer to concepts introduced there without reexplaining them here.</p>

<p>As usual, you can skip to the <a href="#summary">summary</a> for the bite sized version of the findings.</p>

<h2 id="icl-results">ICL Results</h2>

<h3 id="the-compiler-has-an-opinion">The Compiler Has an Opinion</h3>

<p>Let’s first take a look at the overall performance: facing off <code class="language-plaintext highlighter-rouge">fill0</code> vs <code class="language-plaintext highlighter-rouge">fill1</code> as we’ve been doing for every microarchitecture. Remember, <code class="language-plaintext highlighter-rouge">fill0</code> fills a region with zeros, while <code class="language-plaintext highlighter-rouge">fill1</code> fills a region with the value one (as a 4-byte <code class="language-plaintext highlighter-rouge">int</code>).</p>

<p class="warning">All of these tests run at 3.5 GHz. The max single-core turbo for this chip is at 3.7 GHz, but is difficult to run in a sustained manner at this frequency, because of AVX-512 clocking effects and because other cores occasionally activate. 3.5 GHz is a good compromise that keeps the chip running at the same frequency, while remaining close to the ideal turbo. Disabling turbo is not a good option, because this chip runs at 1.1 GHz without turbo, which would introduce a large distortion when exercising the uncore and RAM.</p>

<center><strong>Figure 7a</strong></center>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig7a" id="fig7a">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig7a.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl512/overall-warm.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig7a.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig7a.svg" alt="Figure 7a" width="648" height="432" />
    </a>
</div>

<p>Actually, I lied. <em>This</em> is the right plot for Ice Lake:</p>

<center><strong>Figure 7b</strong></center>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig7b" id="fig7b">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig7b.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl/overall-warm.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig7b.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig7b.svg" alt="Figure 7b" width="648" height="432" />
    </a>
</div>

<p>Well, which is it?</p>

<p>Those two have a couple of key differences. The first is this weird thing that <strong>Figure 7a</strong> has going on in the right half of the L1 region: there are two obvious and distinct performance levels visible, each with roughly half the samples.</p>

<!-- https://stackoverflow.com/questions/43806515/position-svg-elements-over-an-image -->
<style>
.img-overlay-wrap {
  position: relative;
  display: block; /* <= shrinks container to image size */
}

.img-overlay-wrap svg {
  position: absolute;
  top: 0;
  left: 0;
}
</style>

<div class="img-overlay-wrap">
  <img class="figimg" src="/assets/intel-zero-opt/fig7a.svg" alt="Figure 7a Annotated" />
  <svg viewBox="0 0 90 60">
    <g stroke-width=".5" fill="none" opacity="0.5">
      <ellipse transform="rotate(-25 34 10)" cx="34" cy="10" rx="12" ry="5" stroke="green" />
      <ellipse transform="rotate(-25 34 34.5)" cx="34" cy="34.5" rx="12" ry="5" stroke="red" />
    </g>
  </svg>
</div>

<p>The second thing is that while both of the plots show <em>some</em> of the zero optimization effect in the L3 and RAM regions, the effect is <em>much larger</em> in <strong>Figure 7b</strong>:</p>

<div class="img-overlay-wrap">
  <img class="figimg" src="/assets/intel-zero-opt/fig7b.svg" alt="Figure 7a Annotated" />
  <svg viewBox="0 0 90 60">
    <g stroke-width=".5" fill="none" opacity="0.5">
      <ellipse cx="63" cy="40" rx="10" ry="8" stroke="blue" />
    </g>
  </svg>
</div>

<p>So what’s the difference between these two plots? The top one was compiled with <code class="language-plaintext highlighter-rouge">-march=native</code>, the second with <code class="language-plaintext highlighter-rouge">-march=icelake-client</code>.</p>

<p>Since I’m compiling this <em>on</em> the Ice Lake client system, I would expect these to do the same thing, but for <a href="https://twitter.com/stdlib/status/1261038662751522826">some reason they don’t</a>. The primary difference is that <code class="language-plaintext highlighter-rouge">-march=native</code> <a href="https://godbolt.org/z/gm3vRa">generates</a> 512-bit instructions like so (for the main loop):</p>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">.L4:</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">add</span>         <span class="nb">rax</span><span class="p">,</span> <span class="mi">512</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">448</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">384</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">320</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">256</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">192</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">128</span><span class="p">],</span> <span class="nv">zmm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">-</span><span class="mi">64</span><span class="p">],</span>  <span class="nv">zmm0</span>
    <span class="nf">cmp</span>     <span class="nb">rax</span><span class="p">,</span> <span class="nv">r9</span>
    <span class="nf">jne</span>     <span class="nv">.L4</span>
</code></pre></div></div>

<p>Using <code class="language-plaintext highlighter-rouge">-march=icelake-client</code> uses 256-bit instructions<sup id="fnref:still512" role="doc-noteref"><a href="#fn:still512" class="footnote" rel="footnote">1</a></sup>:</p>

<div class="language-nasm highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">.L4:</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">32</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">64</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">96</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">128</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">160</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">192</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">vmovdqu32</span>   <span class="p">[</span><span class="nb">rax</span><span class="o">+</span><span class="mi">224</span><span class="p">],</span> <span class="nv">ymm0</span>
    <span class="nf">add</span>     <span class="nb">rax</span><span class="p">,</span> <span class="mi">256</span>
    <span class="nf">cmp</span>     <span class="nb">rax</span><span class="p">,</span> <span class="nv">r9</span>
    <span class="nf">jne</span>     <span class="nv">.L4</span>
</code></pre></div></div>

<p>Most compilers use 256-bit instructions by default even for targets that support AVX-512 (reason: <a href="https://reviews.llvm.org/D67259">downclocking</a>, so the <code class="language-plaintext highlighter-rouge">-march=native</code> version is the weird one here. All of the earlier x86 tests used 256-bit instructions.</p>

<p>The observation that <strong>Figure 7a</strong> results from running 512-bit instructions, combined with a peek at the data lets us immediately resolve the mystery of the bi-modal behavior.</p>

<p>Here’s the raw data for the 17 samples at a buffer size of 9864:</p>

<div class="table-wrapper">
<table class="dataframe" style="max-width:500px; min-width: 50%; font-size:80%;">
  <thead>
    <tr>
      <td colspan="2"></td>
      <th colspan="2" halign="left">GB/s</th>
    </tr>
    <tr>
      <th>Size</th>
      <th>Trial</th>
      <th>fill0</th>
      <th>fill1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="17" valign="top">9864</th>
      <th>0</th>
      <td>92.3</td>
      <td>92.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>91.9</td>
      <td>91.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>91.9</td>
      <td>91.9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>92.4</td>
      <td>92.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>92.0</td>
      <td>92.3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>92.1</td>
      <td>92.1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>92.0</td>
      <td>92.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>92.3</td>
      <td>92.1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>92.2</td>
      <td>92.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>92.0</td>
      <td>92.1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>183.3</td>
      <td>93.9</td>
    </tr>
    <tr>
      <th>11</th>
      <td>197.3</td>
      <td>196.9</td>
    </tr>
    <tr>
      <th>12</th>
      <td>197.3</td>
      <td>196.6</td>
    </tr>
    <tr>
      <th>13</th>
      <td>196.6</td>
      <td>197.3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>197.3</td>
      <td>196.6</td>
    </tr>
    <tr>
      <th>15</th>
      <td>196.6</td>
      <td>197.3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>196.6</td>
      <td>196.6</td>
    </tr>
  </tbody>
</table>
</div>

<p>The performance follows a specific pattern with respect to the trials for both <code class="language-plaintext highlighter-rouge">fill0</code> and <code class="language-plaintext highlighter-rouge">fill1</code>: it starts out slow (about 90 GB/s) for the first 9-10 samples then suddenly jumps up the higher performance level (close to 200 GB/s). It turns out this is just <a href="/blog/2020/01/17/avxfreq1.html">voltage and frequency management</a> biting us again. In this case there is no frequency change: the <a href="https://github.com/travisdowns/zero-fill-bench/blob/post2/results/icl512/overall-warm.csv#L546">raw data</a> has a frequency column that shows the trials always run at 3.5 GHz. There is only a voltage change, and while the voltage is changing, the CPU runs with reduced dispatch throughput<sup id="fnref:iclbetter" role="doc-noteref"><a href="#fn:iclbetter" class="footnote" rel="footnote">2</a></sup>.</p>

<p>The reason this effect repeats for every new set of trials (new buffer size value) is that each new set of trials is preceded by a 100 ms spin wait: this spin wait doesn’t run any AVX-512 instructions, so the CPU drops back to the lower voltage level and this process repeats. The effect stops when the benchmark moves into the L2 region, because there it is slow enough that the 10 discarded warmup trials are enough to absorb the time to switch to the higher voltage level.</p>

<p>We can avoid this problem simply by removing the 100 ms warmup (passing <code class="language-plaintext highlighter-rouge">--warmup-ms=0</code> to the benchmark), and for the rest of this post we’ll discuss the no-warmup version (we keep the 10 warmup <em>trials</em> and they should be enough).</p>

<h2 id="elimination-in-ice-lake">Elimination in Ice Lake</h2>

<p>So we’re left with the second effect, which is that the 256-bit store version shows <em>very</em> effective elimination, as opposed to the 512-bit version. For now let’s stop picking favorites between 256 and 512 (push that on your stack, we’ll get back to it), and just focus on the elimination behavior for 256-bit stores.</p>

<p>Here’s the closeup of the L3 region for the 256-bit store version, showing also the L2 eviction type, as discussed in the previous post:</p>

<center><strong>Figure 8</strong></center>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig8" id="fig8">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig8.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl/l2-focus.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig8.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig8.svg" alt="Figure 8" width="648" height="432" />
    </a>
</div>

<p>We finally have the elusive (near) 100% elimination of redundant zero stores! The <code class="language-plaintext highlighter-rouge">fill0</code> case peaks at 96% silent (eliminated<sup id="fnref:stricly" role="doc-noteref"><a href="#fn:stricly" class="footnote" rel="footnote">3</a></sup>) evictions. Typical L3 bandwidth is ~59 GB/s with elimination and ~42 GB/s without, for a better than 40% speedup! So this is a potentially a big deal on Ice Lake.</p>

<p>Like last time, we can also check the uncore tracker performance counters, to see what happens for larger buffers which would normally write back to memory.</p>

<center><strong>Figure 9</strong></center>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig9" id="fig9">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig9.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl/l3-focus.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig9.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig9.svg" alt="Figure 9" width="648" height="432" />
    </a>
</div>

<p class="info"><strong>Note:</strong> the way to interpret the events in this plot is the reverse of the above: more uncore tracker writes means <em>less</em> elimination, while in the earlier chart more silent writebacks means <em>more</em> elimination (since every silent writeback replaces a non-silent one).</p>

<p>As with the L3 case, we see that the store elimination appears 96% effective: the number of uncore to memory writebacks flatlines at 4% for the <code class="language-plaintext highlighter-rouge">fill0</code> case. Compare this to <a href="/assets/intel-zero-opt/fig3.svg"><strong>Figure 3</strong></a>, which is the same benchmark running on Skylake-S, and note that only half the writes to RAM are eliminated.</p>

<p>This chart also includes results for the <code class="language-plaintext highlighter-rouge">alt01</code> benchmark. Recall that this benchmark writes 64 bytes of zeros alternating with 64 bytes of ones. This means that, at best, only half the lines can be eliminated by zero-over-zero elimination. On Skylake-S, only about 50% of eligible (zero) lines were eliminated, but here we again get to 96% elimination! That is, in the <code class="language-plaintext highlighter-rouge">alt01</code> case, 48% of all writes were eliminated, half of which are all-ones and not eligible.</p>

<p>The asymptotic speedup for the all zero case for the RAM region is less than the L3 region, at about 23% but that’s still not exactly something to sneeze at. The speedup for the alternating case is 10%, somewhat less than half the benefit of the all zero case<sup id="fnref:altwrites" role="doc-noteref"><a href="#fn:altwrites" class="footnote" rel="footnote">4</a></sup>. In the L3 region, we also note that the benefit of elimination for <code class="language-plaintext highlighter-rouge">alt01</code> is only about 7%, much smaller than the ~20% benefit you’d expect if you cut the 40% benefit the all-zeros case sees. We saw a similar effect in Skylake-S.</p>

<p>Finally it’s worth noting this little uptick in uncore writes in the <code class="language-plaintext highlighter-rouge">fill0</code> case:</p>

<p><img src="/assets/intel-zero-opt/little-uptick.png" alt="Little Uptick" /></p>

<p>This happens right around the transition from L3 to RAM, and this, the writes flatline down to 0.04 per line, but this uptick is fairly consistently reproducible. So there’s some interesting effect there, probably, perhaps related to the adaptive nature of the L3 caching<sup id="fnref:l3adapt" role="doc-noteref"><a href="#fn:l3adapt" class="footnote" rel="footnote">5</a></sup>.</p>

<h3 id="512-bit-stores">512-bit Stores</h3>

<p>If we rewind time, time to pop the mental stack and return to something we noticed earlier: that 256-bit stores seemed to get superior performance for the L3 region compared to 512-bit ones.</p>

<p>Remember that we ended up with 256-bit and 512-bit versions due to unexpected behavior in the <code class="language-plaintext highlighter-rouge">-march</code> flag. Rather they <em>relying</em> on this weirdness<sup id="fnref:gccfix" role="doc-noteref"><a href="#fn:gccfix" class="footnote" rel="footnote">6</a></sup>, let’s just write slighly lazy<sup id="fnref:lazy" role="doc-noteref"><a href="#fn:lazy" class="footnote" rel="footnote">7</a></sup> <a href="https://github.com/travisdowns/zero-fill-bench/blob/master/algos.cpp#L151">methods</a> that explicitly use 256-bit and 512-bit stores but are otherwise identical. <code class="language-plaintext highlighter-rouge">fill256_0</code> uses 256-bit stores and writes zeros, and I’ll let you pattern match the rest of the names.</p>

<p>Here’s how they perform on my ICL hardware:</p>

<center><strong>Figure 10</strong></center>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig10" id="fig10">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig10.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl/256-512.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig10.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig10.svg" alt="Figure 10" width="648" height="432" />
    </a>
</div>

<p class="warning">This chart shows only the the median of 17 trials. You can look at the raw data for an idea of the trial variance, but it is generally low.</p>

<p>In the L1 region, the 512-bit approach usually wins and there is no apparent difference between writing 0 or 1 (the two halves of the moon mostly line up). Still, 256-bit stores are roughly <em>competitive</em> with 512-bit: they aren’t running at half the throughput. That’s thanks to the second store port on Ice Lake. Without that feature, you’d be limited to 112 GB/s at 3.5 GHz, but here we handily reach ~190 GB/s with 256-bit stores, and ~195 GB/s with 512-bit stores. 512-bit stores probably have a slight advantage just because of fewer total instructions executed (about half of the 256-bit case) and associated second order effects.</p>

<p class="info">Ice Lake has two <em>store ports</em> which lets it execute two stores per cycle, but only a single cache line can be written per cycle. However, if two consecutive stores fall into the <em>same</em> cache line, they will generally both be written in the same cycle. So the maximum sustained throughput is up to two stores per cycle, <em>if</em> they fall in the same line<sup id="fnref:l1port" role="doc-noteref"><a href="#fn:l1port" class="footnote" rel="footnote">8</a></sup>.</p>

<p>In the L2 region, however, the 256-bit approaches seem to pull ahead. This is a bit like the Buffalo Bills winning the Super Bowl: it just isn’t supposed to happen.</p>

<p>Let’s zoom in:</p>

<center><strong>Figure 11</strong></center>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig11" id="fig11">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig11.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl/256-512-l2-l3.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig11.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig11.svg" alt="Figure 11" width="648" height="432" />
    </a>
</div>

<p>The 256-bit benchmarks start roughly tied with their 512 bit cousins, but then steadily pull away as the region approaches the full size of the L2. By the end of the L2 region, they have nearly a ~13% edge. This applies to <em>both</em> <code class="language-plaintext highlighter-rouge">fill256</code> versions – the zeros-writing and ones-writing flavors. So this effect doesn’t seem explicable by store elimination: we already know ones are not eliminated and, also, elimination only starts to play an obvious role when the region is L3-sized.</p>

<p>In the L3, the situation changes: now the 256-bit version really pulls ahead, <em>but only the version that writes zeros</em>. The 256-bit and 512-bit one-fill versions fall down in throughput, nearly to the same level (but the 256-bit version still seems <em>slightly but measurably ahead</em> at ~2% faster). The 256-bit zero fill version is now ahead by roughly 45%!</p>

<p>Let’s concentrate only on the two benchmarks that write zero: <code class="language-plaintext highlighter-rouge">fill256_0</code> and <code class="language-plaintext highlighter-rouge">fill512_0</code>, and turn on the L2 eviction counters (you probably saw that one coming by now):</p>

<div class="svg-fig">
    <div class="svg-fig-links">
        <a href="#fig12" id="fig12">[link<span class="only-large"> to this chart</span>]</a> 
        
            <a href="/misc/tables//intel-zero-opt/fig12.html">[data<span class="only-large"> table</span>]</a> 
        
        <a href="https://github.com/travisdowns/zero-fill-bench/tree/master/results/icl/256-512-l2-l3.csv">[raw<span class="only-large"> data</span>]</a> 
    </div>
    <a href="/misc/tables//intel-zero-opt/fig12.html">
        <img class="figimg" src="/assets/intel-zero-opt/fig12.svg" alt="Figure 12" width="648" height="432" />
    </a>
</div>

<p class="warning">Only the <em>L2 Lines Out Silent</em> event is shown – the balance of the evictions are <em>non-silent</em> as usual.</p>

<p>Despite the fact that I had to leave the right axis legend just kind floating around in the middle of the plot, I hope the story is clear: 256-bit stores get eliminated at the usual 96% rate, but 512-bit stores are hovering at a decidedly Skylake-like ~56%. I can’t be sure, but I expect this difference in store elimination largely explains the performance difference.</p>

<p>I checked also the behavior with prefetching off, but the pattern is very similar, except with both approaches having reduced performance in L3 (you can <a href="/assets/intel-zero-opt/fig12-nopf.svg">see for yourself</a>). It is interesting to note that for zero-over-zero stores, the 256-bit store performance <em>in L3</em> is almost the same as the 512-bit store performance <em>in L2!</em> It buys you almost a whole level in the cache hierarchy, performance-wise (in this benchmark).</p>

<p>Normally I’d take a shot at guessing what’s going on here, but this time I’m not going to do it. I just don’t know<sup id="fnref:lied" role="doc-noteref"><a href="#fn:lied" class="footnote" rel="footnote">9</a></sup>. The whole thing is very puzzling, because everything after the L1 operates on a cache-line basis: we expect the fine-grained pattern of stores made by the core, <em>within a line</em> to basically be invisible to the rest of the caching system which sees only full lines. Yet there is some large effect in the L3 and even in RAM<sup id="fnref:RAM" role="doc-noteref"><a href="#fn:RAM" class="footnote" rel="footnote">10</a></sup> related to whether the core is writing a cache line in two 256-bit chunks or a single 512-bit chunk.</p>

<h2 id="summary">Summary</h2>

<p>We have found that the store elimination optimization originally uncovered on Skylake client is still present in Ice Lake and is roughly twice as effective in our fill benchmarks. Elimination of 96% L2 writebacks (to L3) and L3 writebacks (to RAM) was observed, compared to 50% to 60% on Skylake. We found speedups of up to 45% in the L3 region and speedups of about 25% in RAM, compared to improvements of less than 20% in Skylake.</p>

<p>We find that when zero-filling writes occur to a region sized for the L2 cache or larger, 256-bit writes are often significantly <em>faster</em> than 512-bit writes. The effect is largest for the L2, where 256-bit zero-over-zero writes are up to <em>45% faster</em> than 512-bit writes. We find a similar effect even for non-zeroing writes, but only in the L2.</p>

<h2 id="future">Future</h2>

<p>It is an interesting open question whether the as-yet-unreleased <abbr title="The new 7nm microarchitecture used in Ice Lake CPUs.">Sunny Cove</abbr> server chips will exhibit this same optimization.</p>

<h2 id="advice">Advice</h2>

<p>Unless you are developing only for your own laptop, as of May 2020 Ice Lake is deployed on a microscopic fraction of total hosts you would care about, so the headline advice in the previous post applies: this optimization doesn’t apply to enough hardware for you to target it specifically. This might change in the future as Ice Lake and sequels roll out in force. In that case, the magnitude of the effect might make it worth optimizing for in some cases.</p>

<p>For fine-grained advice, see the <a href="/blog/2020/05/13/intel-zero-opt.html#tuning-advice">list in the previous post</a>.</p>

<h2 id="thanks">Thanks</h2>

<p>Vijay and Zach Wegner for pointing out typos.</p>

<p>Ice Lake photo by <a href="https://unsplash.com/@marcuslofvenberg">Marcus Löfvenberg</a> on Unsplash.</p>

<p>Saagar Jha for helping me track down and fix a WebKit rendering <a href="https://github.com/travisdowns/travisdowns.github.io/issues/102">issue</a>.</p>

<h2 id="discussion-and-feedback">Discussion and Feedback</h2>

<p>If you have something to say, leave a comment below. There are also discussions on <a href="https://twitter.com/trav_downs/status/1262428350511022081">Twitter</a> and <a href="https://news.ycombinator.com/item?id=23225260">Hacker News</a>.</p>

<p>Feedback is also warmly welcomed by <a href="mailto:travis.downs@gmail.com">email</a> or as <a href="https://github.com/travisdowns/travisdowns.github.io/issues">a GitHub issue</a>.</p>

<hr />
<p><br /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:still512" role="doc-endnote">
      <p>It’s actually still using the EVEX-encoded AVX-512 instruction <code class="language-plaintext highlighter-rouge">vmovdqu32</code>, which is somewhat more efficient here because AVX-512 has more compact encoding of offsets that are a multiple of the vector size (as they usually are). <a href="#fnref:still512" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:iclbetter" role="doc-endnote">
      <p>In this case, the throughput is only halved, versus the 1/4 throughput when we looked at dispatch throttling on <abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr>, so based on this very preliminary result it seems like the dispatch throttling might be less severe in Ice Lake (this needs a deeper look: we never used stores to test on <abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr>). <a href="#fnref:iclbetter" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:stricly" role="doc-endnote">
      <p>Strictly speaking, a silent writeback is a <em>sufficient</em>, but not a <em>necessary</em> condition for elimination, so it is a lower bound on the number of eliminated stores. For all I know, 100% of stores are eliminated, but out of those 4% are written back not-silently (but not in a modified state). <a href="#fnref:stricly" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:altwrites" role="doc-endnote">
      <p>One reason could be that writing only alternating lines is somewhat more expensive than writing half the data but contiguously. Of course this is obviously true closer to the core, since you touch half the number of the pages in the contiguous case, need half the number of page walks, prefetching is more effective since you cross half as many 4K boundaries (prefetch stops at 4K boundaries) and so on. Even at the memory interface, alternating line writes might be less efficient because you get less benefit from opening each DRAM page, can’t do longer than 64-byte bursts, etc. In a pathological case, alternating lines could be <em>half</em> the bandwidth if the controller maps alternating lines to alternating channels, since you’ll only be accessing a single channel. We could try to isolate this effect by trying more coarse grained interleaving. <a href="#fnref:altwrites" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:l3adapt" role="doc-endnote">
      <p>The L3 is capable of determining if the current access pattern would be better served by something like an <abbr title="Most recently used - an eviction strategy suitable for data with little temporal locality">MRU</abbr> eviction strategy, for example when a stream of data is being accessed without reuse, it would be better to kick that data out of the cache quickly, rather than evicting other data that may be useful. <a href="#fnref:l3adapt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:gccfix" role="doc-endnote">
      <p>After all, there’s a good chance it will be fixed in a later version of gcc. <a href="#fnref:gccfix" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lazy" role="doc-endnote">
      <p>These are lazy in the sense that I don’t do any scalar head or tail handling: the final iteration just does a full width <abbr title="Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.">SIMD</abbr> store even if there aren’t 64 bytes left: we overwrite the buffer by up to 63 bytes. We account for this when we allocate the buffer by ensuring the allocation is oversized by at least that amount. This doesn’t matter for larger buffers, but it means this version will get a boost for very small buffers versus approaches that do the fill exactly. In any case, we are interested in large buffers here. <a href="#fnref:lazy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:l1port" role="doc-endnote">
      <p>Most likely, the L1 has a single 64 byte wide write port, like <abbr title="Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W">SKX</abbr>, and the commit logic at the head of the store buffer can look ahead one store to see if it is in the same line in order to dequeue two stores in a single cycle. Without this feature, you could <em>execute</em> two stores per cycle, but only commit one, so the long-run store throughput would be limited to one per cycle. <a href="#fnref:l1port" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lied" role="doc-endnote">
      <p>Well I lied. I at least have some ideas. It may be that the CPU power budget is dynamically partitioned between the core and uncore, and with 512-bit stores triggering the AVX-512 power budget, there is less power for the uncore and it runs at a lower frequency (that could be checked). This seems unlikely given that it should not obviously affect the elimination chance. <a href="#fnref:lied" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:RAM" role="doc-endnote">
      <p>We didn’t take a close look at the effect in RAM but it persists, albeit at a lower magnitude. 256-bit zero-over-zero writes are about 10% faster than 512-bit writes of the same type. <a href="#fnref:RAM" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- travis override -->
  
    <section class="comments" id="comment-section">
  <hr>
  
  <!-- Existing comments -->
  <div class="comments__existing">
    <h2>Comments</h2>
    
    
    <!-- List main comments in reverse date order, newest first. List replies in date order, oldest first. -->
    
    

<article id="comment-f26ed410-75fc-11eb-b1b1-b1e90cd6ebd3" class="js-comment comment" uid="f26ed410-75fc-11eb-b1b1-b1e90cd6ebd3">

  <div class="comment__author">
    Noah Goldstein
    <span class="comment__date">•
        <a href="#comment-f26ed410-75fc-11eb-b1b1-b1e90cd6ebd3" title="Permalink to this comment">February 23th, 2021 17:31</a></span>
  </div>

  <div class="comment__body">
    <p>Hi Travis,</p>

<p>Re: <a href="https://stackoverflow.com/questions/66274948/why-am-i-seeing-more-rfo-read-for-ownership-requests-using-rep-movsb-than-with?noredirect=1#comment117262448_66274948">Why am I seeing more RFO (Read For Ownership) requests using REP MOVSB than with vmovdqa</a></p>

<p>Your explination for why the <a href="https://community.intel.com/t5/Software-Tuning-Performance/What-is-the-ItoM-IDI-opcode-in-the-performance-monitoring-events/m-p/1253627#M7797">RFO-ND</a> optimization and 0-over-0 write elimination optimization for mutally exclusive makes sense. What I am still curious about is why <code class="language-plaintext highlighter-rouge">zmm</code> fill0 seem to get worst of both worlds; being unable to make the RFO-ND optimization and having a worse rate of 0-over-0 write elimination than <code class="language-plaintext highlighter-rouge">ymm</code> fill0.</p>

<p>Here is a layout of some observations that I think might be useful for figuring this out:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">rep stosb</code> writing in 64 byte chunks is able to make the RFO-ND optimization but is not able to get the 0-over-0 write elminiation optimization.</li>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa ymm, (reg); vmovdqa ymm, 32(reg)</code> (assumining reg is 64 byte aligned) is unable to make the RFO-ND optimization and gets a high rate of 0-over-0 elimination optimization</li>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa ymm, (reg); vmovdqa ymm, 64(reg); vmovdqa ymm, 32(reg); vmovdqa ymm, 96(reg)</code> (assumining reg is 64 byte aligned) is unable to make the RFO-ND optimization and gets a <em>slightly higher</em> rate of 0-over-0 elimination optimization than case 2.</li>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa zmm, (reg)</code> is not able to make the RFO-ND optimization and has the worse rate of 0-over-0 elminination than both case 2 and case 3 above.</li>
</ol>

<p>Case 4 is strange because it gets the worst of both worlds. Intuitively if it was the RFO-ND that prevents 0-over-0 write elimination (i.e whats happening with <code class="language-plaintext highlighter-rouge">rep stosb</code>) we would either expect to see some RFO-ND requests when using <code class="language-plaintext highlighter-rouge">vmovdqa zmm, (reg)</code> but we don’t. Likewise if we not seeing RFO-ND requests why would its 0-over-0 elimination rate be lower.</p>

<p>Cases 2 and 3 I think are really interesting (reproducible by <a href="https://github.com/travisdowns/zero-fill-bench/blob/master/algos.cpp#L52">changing that order of stores here</a>) and I might be useful in understanding case 4. The only difference between cases 2 and 3 that I can think of is that <a href="https://www.realworldtech.com/forum/?threadid=173441&amp;curpostid=192262">case 2 can write coalesce in the LFB whereas case 3 cannot</a>.</p>

<p>So as a possible explinination for why case 3 gets a higher 0-over-0 write elimination rate than case 2 I was thinking something along the following for relationship between RFO requests and LFB coalescing.</p>

<p>For Case 2:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa ymm, (reg)</code> (write A) graduates in the store buffer and goes to LFB.</li>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa ymm, (reg)</code> (write B) graduates in the store buffer and goes to LFB.</li>
  <li>Write B coalesces with tail of LFB write A</li>
  <li>Write AB makes an RFO request w/ data</li>
  <li>Write AB is 64 bytes so when RFO returns something special happens that might ignore the data returned by RFO request.
    <ul>
      <li>We know its not fully optimizing out the check because case 2 has a relatively high 0-over-0 write elminination rate and case 4 has a non-zero rate.</li>
    </ul>
  </li>
</ul>

<p>For Case 3:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa ymm, (reg)</code> (write A) graduates in the store buffer and goes to LFB.</li>
  <li>Write A makes an RFO request w/ data</li>
  <li><code class="language-plaintext highlighter-rouge">vmovdqa ymm, (reg)</code> (write B) graduates in the store buffer and goes to LFB.</li>
  <li>Write B makes an RFO request w/ data</li>
  <li>Write A is 32 bytes so when RFO returns it checks the data</li>
  <li>Write B is 32 bytes so when RFO returns it checks the data
    <ul>
      <li>Since A and B where not coalesced we will never see the “special” circumstances where the RFO data is ignored.</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>perf stat --all-user -e cpu/event=0x24,umask=0xef,name=l2_rqsts_references/,cpu/event=0x24,umask=0xe2,name=l2_rqsts_all_rfo/,cpu/event=0x24,umask=0xc2,name=l2_rqsts_rfo_hit/,cpu/event=0x24,umask=0x22,name=l2_rqsts_rfo_miss/
</code></pre></div></div>
<p>(The data has the same trend with prefetched events counted)</p>

<table>
  <thead>
    <tr>
      <th>Case</th>
      <th>l2_rqsts_references</th>
      <th>l2_rqsts_all_rfo</th>
      <th>l2_rqsts_rfo_hit</th>
      <th>l2_rqsts_rfo_miss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>4,449,489</td>
      <td>4,325,110</td>
      <td>3,320,352</td>
      <td>1,004,758</td>
    </tr>
    <tr>
      <td>3</td>
      <td>11,832,291</td>
      <td>11,704,843</td>
      <td>7,624,891</td>
      <td>4,079,952</td>
    </tr>
    <tr>
      <td>4</td>
      <td>4,402,628</td>
      <td>4,254,720</td>
      <td>3,263,507</td>
      <td>991,213</td>
    </tr>
  </tbody>
</table>

<p>We see here that case 2 and case 4 have basically the same RFO behavior which would indicate that writes A and B merge before the RFO request is made. What this doesn’t make clear is:</p>
<ul>
  <li>whether the difference in 0-over-0 elimination rate between cases 2 and 3 is because there is a difference in the RFO requests, or whether the difference is related to how the RFO requests are handled upon returning.</li>
  <li>where the difference between cases 2 and 4 emerges (i.e we can’t just say an RFO request for a full cache line write has its data part ignored).</li>
</ul>

<p>The only explination I can think of is if RFO prefetching takes place (<a href="https://stackoverflow.com/questions/61129773/how-do-the-store-buffer-and-line-fill-buffer-interact-with-each-other">mentioned a bit here</a>) its possible you could try and explain the data with the following assumptions:</p>
<ul>
  <li>That RFO prefetching does in fact take place</li>
  <li>That whether the RFO data can be ignored is a function of RFO configuration, not the size of the entry in the LFB.</li>
  <li>Prefetched RFO requests are always configured to use the return data (unknown what’s different about <code class="language-plaintext highlighter-rouge">rep stosb</code> and why this would be case).</li>
  <li>Prefetched RFO requests don’t show up on event counters.</li>
</ul>

<p>There are basically four assumptions here any of which being untrue would blowup the theory, but if they all happened to be the case then we could explain the difference in 0-over-0 write elmination between cases 2 and 4 by saying that case 2 has many more chances to prefetch and the difference between cases 2 and 3 by saying that when case 2 fails both prefetches the coalesced RFO from writes AB will optimize out the data check.</p>

<p>Overall having trouble wrapping my head around all of this and wondering if you have an idea what distinguishes the 4 cases above.</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-f26ed410-75fc-11eb-b1b1-b1e90cd6ebd3', 'respond', 'f26ed410-75fc-11eb-b1b1-b1e90cd6ebd3')">↪&#xFE0E; Reply to Noah Goldstein</a>
    </div>
</article>
  

<article id="comment-e1ddce50-763b-11eb-9ce9-c7a446ce00c0" class="js-comment comment admin child" uid="e1ddce50-763b-11eb-9ce9-c7a446ce00c0">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-e1ddce50-763b-11eb-9ce9-c7a446ce00c0" title="Permalink to this comment">February 24th, 2021 01:01</a></span>
  </div>

  <div class="comment__body">
    <p>Hi Noah,</p>

<p>Thanks for your (comprehensive!) comment.</p>

<p>I think RFO-ND might be a bit of a red herring here: none of the vanilla store types use this protocol, so we can mostly ignore it and treat <code class="language-plaintext highlighter-rouge">rep stosb</code> as its own special thing which doesn’t get 0-over-0 optimization, and which can affect prefetchers in a totally differenet way, etc. The fact that aligned 64-byte <code class="language-plaintext highlighter-rouge">zmm</code> writes don’t use the ND protocol is perhaps a bit surprising, but maybe not <em>that</em> surprising in that back-to-back contiguous, aligned 32-byte writes also did not use it on previous architectures and given the intermediate buffering provided by the LFB it seems like even the narrower stores could have done this, if it were easy enough. It seems like it is not easy enough.</p>

<p>I am suprised by the <code class="language-plaintext highlighter-rouge">perf</code> results for case 3 vs the other two cases. There are more than 2x the number of RFO requests. I would have expected about the same numbers: yes, there is a lack of coalescing in the LFB (so I think performance would suffer) but I would expect ultimately that the second writes to each line would “hit” in the existing LFB or L1 (when they are allowed to proceed) and not generate any additional RFO requests.</p>

<p>What do those numbers look like when normalized against the number of cache lines written? I.e., how many cache lines are written by the test? This helps understand which if any of the tests is close to 1:1 with lines written/RFO.</p>

<p>I am not sure the optimization is related to how the data is returned by the RFO. I believe it may happen much later, when the relevant lines are <em>evicted</em>. For instances, there is the test that initially writes non-zero values to a line, then writes zero values: in this case the optimization would not apply at the time of the RFO (non-zero values), but it would apply at the time of eviction (0s were written after, but w/o any new RFO since the lines are already locally cached in M state): in this case, I still saw the 0-over-0 optimization happen. Also, I saw it happen when the entire line is not overwritten (from my notes, although they aren’t 100% clear on this).</p>

<p>I believe the optimization may be load/occupancy related: e.g., only happen sometimes, when the load between some cache levels is above/below some level, or some other condition is met. In this case, small changes to the test might change things enough to affect that threshold.</p>

<p>Another possibility is more along the lines you mentioned: as part of the coherence flows, the “R” (data) and “O” (grant of exclusive ownership) might arrive at different times, in two different messages. Also, sometimes the outer cache might send a “pull” request to the inner cache for the data (after the inner cache indicates that it wants to write it), while other times the inner cache might send the data without a pull. Perhaps whether the optimization applies depends on the order of the messages or another detail like this and timing changes it.</p>

<p>Your guess that</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
    

<article id="comment-4afb2ef0-73c1-11eb-9aaa-d51dc4f5f74f" class="js-comment comment" uid="4afb2ef0-73c1-11eb-9aaa-d51dc4f5f74f">

  <div class="comment__author">
    Noah Goldstein
    <span class="comment__date">•
        <a href="#comment-4afb2ef0-73c1-11eb-9aaa-d51dc4f5f74f" title="Permalink to this comment">February 20th, 2021 21:19</a></span>
  </div>

  <div class="comment__body">
    <p>Tested and found that <code class="language-plaintext highlighter-rouge">rep stosb</code> does not eliminate 0 over 0 cache line stores.</p>

  </div>


    <div class="comment__meta">
      <a rel="nofollow" class="comment__reply-link" onclick="return addComment.moveForm('comment-4afb2ef0-73c1-11eb-9aaa-d51dc4f5f74f', 'respond', '4afb2ef0-73c1-11eb-9aaa-d51dc4f5f74f')">↪&#xFE0E; Reply to Noah Goldstein</a>
    </div>
</article>
  

<article id="comment-75fafc90-740a-11eb-93fe-079499d9a376" class="js-comment comment admin child" uid="75fafc90-740a-11eb-93fe-079499d9a376">

  <div class="comment__author">
     
    <span class="comment__admin_tag">Author</span>
    Travis Downs
    <span class="comment__date">•
        <a href="#comment-75fafc90-740a-11eb-93fe-079499d9a376" title="Permalink to this comment">February 21th, 2021 06:03</a></span>
  </div>

  <div class="comment__body">
    <p>Hi Noah, thanks for your comment and observation about <code class="language-plaintext highlighter-rouge">rep stosb</code>!</p>

<p>I think your finding makes sense: <code class="language-plaintext highlighter-rouge">rep stosb</code> can use a “RFO-ND” (request ownership <em>without data</em>) protocol for larger region sizes, as opposed to <a href="https://en.wikipedia.org/wiki/MESI_protocol#Read_For_Ownership">vanilla RFO</a> which brings the existing cache line up into the cache hierarchy. This works because the core can guarantee entire lines will be overwritten by the string operation since it knows the total size of the operation: thus the old data is “dead”.</p>

<p>Since the old data isn’t fetched, it can’t be compared against zero, and a zero-over-zero optimization couldn’t happen. In essence, this RFO-ND optimization is the opposite approach to that discussed in this post: RFO-ND avoids the <em>read</em> implied by a store, while this zero-over-zero optimization avoids the <em>write</em>. I think you have to pick one or the other: I don’t see an easy way to do both for data in RAM. The RFO-ND approach has the benefit of applying to any value, not just zero.</p>

<p>An open question is whether the zero-over-zero optimization might apply for <code class="language-plaintext highlighter-rouge">rep stosb</code> over short regions (where RFO-ND isn’t used) or if the data is already cached in L1 or L2 (since then I think the RFO-ND doens’t come into play).</p>

  </div>


</article>


  

  <hr style="border-top: 1px solid #ccc; background: transparent; margin-bottom: 10px;">


    
  </div>
  

  <!-- New comment form -->
  <div id="respond" class="comment__new">
    <form class="js-form form" method="post" action="https://staticman-travisdownsio.herokuapp.com/v2/entry/travisdowns/travisdowns.github.io/master/comments">
  <input type="hidden" name="options[origin]" value="https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html">
  <input type="hidden" name="options[parent]" value="https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html">
  <input type="hidden" id="comment-replying-to-uid" name="fields[replying_to_uid]" value="">
  <input type="hidden" name="options[slug]" value="icelake-zero-opt">
  
  
  <div class="textfield">
    <label for="comment-form-message"><h2>Add Comment</h2>
      <textarea class="textfield__input" name="fields[message]" type="text" id="comment-form-message" placeholder="Your comment (markdown accepted)" required rows="6"></textarea>
    </label>
  </div>

    <div class="textfield narrowfield">
      <label for="comment-form-name">Name
        <input class="textfield__input" name="fields[name]" type="text" id="comment-form-name" placeholder="Your name (required)" required/>
      </label>
    </div>

    <div class="textfield narrowfield">
      <label for="comment-form-email">E-mail
        <input class="textfield__input" name="fields[email]" type="email" id="comment-form-email" placeholder="Your email (optional)"/>
      </label>
    </div>

    <div class="textfield narrowfield hp">
      <label for="hp">
        <input class="textfield__input" name="fields[hp]" id="hp" type="text" placeholder="Leave blank">
      </label>
    </div>

    

    <button type="button" class="button" id="cancel-comment-reply-link" style="display: none">
      Cancel Reply
    </button>
  
    <button class="button" id="comment-form-submit">
      Submit
    </button>

</form>

<article class="modal">
  <div>
    <h3 class="modal-title js-modal-title"></h3>
  </div>
  <div class="mdl-card__supporting-text js-modal-text"></div>
  <div class="mdl-card__actions mdl-card--border">
    <button class="button mdl-button--colored mdl-js-button mdl-js-ripple-effect js-close-modal">Close</button>
  </div>
</article>

<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="display:none" >
  <symbol id="icon-loading" viewBox="149.8 37.8 499.818 525"><path d="M557.8 187.8c13.8 0 24.601-10.8 24.601-24.6S571.6 138.6 557.8 138.6s-24.6 10.8-24.6 24.6c0 13.2 10.8 24.6 24.6 24.6zm61.2 90.6c-16.8 0-30.6 13.8-30.6 30.6s13.8 30.6 30.6 30.6 30.6-13.8 30.6-30.6c.6-16.8-13.2-30.6-30.6-30.6zm-61.2 145.2c-20.399 0-36.6 16.2-36.6 36.601 0 20.399 16.2 36.6 36.6 36.6 20.4 0 36.601-16.2 36.601-36.6C595 439.8 578.2 423.6 557.8 423.6zM409 476.4c-24 0-43.2 19.199-43.2 43.199s19.2 43.2 43.2 43.2 43.2-19.2 43.2-43.2S433 476.4 409 476.4zM260.8 411c-27 0-49.2 22.2-49.2 49.2s22.2 49.2 49.2 49.2 49.2-22.2 49.2-49.2-22.2-49.2-49.2-49.2zm-10.2-102c0-27.6-22.8-50.4-50.4-50.4-27.6 0-50.4 22.8-50.4 50.4 0 27.6 22.8 50.4 50.4 50.4 27.6 0 50.4-22.2 50.4-50.4zm10.2-199.8c-30 0-54 24-54 54s24 54 54 54 54-24 54-54-24.6-54-54-54zM409 37.8c-35.4 0-63.6 28.8-63.6 63.6S374.2 165 409 165s63.6-28.8 63.6-63.6-28.2-63.6-63.6-63.6z"/>
  </symbol>
</svg>



  </div>
</section>

<script src="/assets/main.js"></script>


  
  <!-- end override -->

  <a class="u-url" href="/blog/2020/05/18/icelake-zero-opt.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Travis Downs</li>
          <li><a class="u-email" href="mailto:travis.downs@gmail.com">travis.downs@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>A blog about low-level software and hardware performance.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/travisdowns" title="travisdowns"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/trav_downs" title="trav_downs"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
